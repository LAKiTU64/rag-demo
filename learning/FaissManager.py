# FaissManager.py

import os
import shutil
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List

import torch

from langchain_community.document_loaders import (
    Docx2txtLoader,
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ä¸¥æ ¼ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼ˆä¸å…è®¸è”ç½‘ä¸‹è½½ï¼‰
# è¯·å°†æœ¬åœ°æ¨¡å‹æ”¾åœ¨ EMBEDDING_MODEL æŒ‡å®šçš„è·¯å¾„ï¼Œæˆ–ä¿®æ”¹ EMBEDDING_MODEL å€¼

# --- Config ---
EMBEDDING_MODEL = "./.models/BAAI/bge-small-zh-v1.5"
INDEX_PATH = "./faiss_index"
CHUNK_SIZE = 300  # å¦‚æœå›ç­”æ€»æ˜¯â€œæ–­ç« å–ä¹‰â€ï¼Œéœ€è¦æŠŠè¿™ä¸ªå€¼è°ƒå¤§ï¼›å¦‚æœå‘ç° LLM æ€»æ˜¯æ‰¾ä¸åˆ°é‡ç‚¹ï¼Œå¯èƒ½éœ€è¦è°ƒå°ã€‚
CHUNK_OVERLAP = 30  # å¦‚æœåˆ‡åˆ†åçš„å¥å­ç»å¸¸å‡ºç°â€œå‰å› åæœâ€ä¸è¿è´¯ï¼Œéœ€è¦è°ƒå°è¿™ä¸ªå€¼ã€‚
DEFAULT_SEARCH_K = 3
SIMILARITY_THRESHOLD = 0.6  # å¦‚æœæœç´¢ç»“æœæ€»æ˜¯â€œä¸ç›¸å…³â€ï¼Œéœ€è¦è°ƒå°è¿™ä¸ªå€¼ï¼›å¦‚æœæ€»æ˜¯â€œé‡å¤â€æˆ–â€œå®Œå…¨ä¸å¯¹â€ï¼Œéœ€è¦è°ƒå¤§è¿™ä¸ªå€¼ã€‚
SYSTEM_DOC_ID = "system"  # é»˜è®¤åˆå§‹åŒ–çš„æ–‡æ¡£ID
BEIJING_TZ = timezone(timedelta(hours=8))  # å®šä¹‰ä¸œå…«åŒºæ—¶åŒº


class VectorKBManager:
    """
    å‘é‡çŸ¥è¯†åº“ç®¡ç†ç±»ï¼šæ”¯æŒåŸºäºæ–‡æ¡£ç»´åº¦çš„å¢ã€åˆ ã€æŸ¥ã€‚
    åˆ é™¤ç­–ç•¥ï¼šé‡‡ç”¨â€œè½¯åˆ é™¤æ ‡è®° + ç¡¬åˆ é™¤é‡æ„â€çš„æ–¹æ¡ˆã€‚
    """

    def __init__(self, index_path="./faiss_index") -> None:
        self.index_path = index_path
        # åˆå§‹åŒ– Embeddingï¼šä¸¥æ ¼ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆä¸å…è®¸è”ç½‘ä¸‹è½½ï¼‰
        if not os.path.exists(EMBEDDING_MODEL):
            raise FileNotFoundError(
                f"æœ¬åœ°åµŒå…¥æ¨¡å‹æœªæ‰¾åˆ°: {EMBEDDING_MODEL}. è¯·å°†æ¨¡å‹æ”¾ç½®åœ¨è¯¥è·¯å¾„æˆ–ä¿®æ”¹ EMBEDDING_MODEL é…ç½®ã€‚"
            )
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model_kwargs = {"device": device, "local_files_only": True}
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs=model_kwargs,
            encode_kwargs={"normalize_embeddings": True},
        )
        self.vectorstore = None

        # ã€é€»è¾‘ä¼˜åŒ–ã€‘è®°å½•æ¯ä¸ªæ–‡æ¡£çš„â€œæœ‰æ•ˆèµ·å§‹æ—¶é—´â€
        # é”®ï¼šdoc_id, å€¼ï¼šdatetime å¯¹è±¡ã€‚æ£€ç´¢æ—¶ä»…åŒ¹é… add_time >= è¯¥æ—¶é—´çš„åˆ‡ç‰‡ã€‚
        self.doc_valid_from: Dict[str, datetime] = {}

        # è½¯åˆ é™¤åå•ï¼šå­˜å‚¨åœ¨å†…å­˜ä¸­çš„æ–‡ä»¶åé›†åˆã€‚å³ä½¿å‘é‡è¿˜åœ¨ç´¢å¼•é‡Œï¼Œåªè¦åœ¨è¿™é‡Œé¢çš„æ–‡ä»¶ï¼Œæœç´¢æ—¶éƒ½ä¼šè¢«è¿‡æ»¤æ‰ã€‚
        # æ³¨æ„ï¼šåœ¨æ–°çš„æ—¶é—´æˆ³é€»è¾‘ä¸‹ï¼Œæ­¤é›†åˆä¸»è¦ç”¨äºæ ‡è¯†å“ªäº›æ–‡æ¡£å¤„äºå®Œå…¨å±è”½çŠ¶æ€ã€‚
        self.soft_deleted_sources = set()

        self._load_or_create()

    def _load_or_create(self, is_reset: bool = False) -> None:
        """
        åˆå§‹åŒ–åŠ è½½ã€‚å¦‚æœæœ¬åœ°æœ‰ç´¢å¼•åˆ™è¯»å–ï¼Œå¦åˆ™åˆ›å»ºä¸€ä¸ªç©ºåº“ã€‚
        """

        if os.path.exists(self.index_path) and not is_reset:
            # åŠ è½½æœ¬åœ° FAISS ç´¢å¼•
            self.vectorstore = FAISS.load_local(
                self.index_path, self.embeddings, allow_dangerous_deserialization=True
            )
            # åŠ è½½åæ‰«æä¸€éå…¨åº“ï¼ŒåŒæ­¥æ–‡æ¡£çš„æœ‰æ•ˆæ—¶é—´æˆ³ï¼ˆé»˜è®¤ä¸ºå„ä¸ªæ–‡æ¡£ç°å­˜çš„æœ€æ—©æ—¶é—´ï¼‰
            self._sync_valid_times()
            print(f"ğŸ“¦ å·²ä»æœ¬åœ°åŠ è½½ç´¢å¼•: {self.index_path}")
        else:
            # FAISS ä¸å…è®¸å®Œå…¨ç©ºçš„åº“å­˜åœ¨ï¼Œæ‰€ä»¥åˆå§‹åŒ–ä¸€ä¸ªç³»ç»Ÿçº§åˆ«çš„å ä½æ–‡æ¡£
            # åŒæ—¶è®°å½•åº“çš„åˆå§‹åˆ›å»ºæ—¶é—´
            create_time = datetime.now(BEIJING_TZ)
            initial_doc = [
                Document(
                    page_content="init_system_placeholder",
                    metadata={"doc_id": SYSTEM_DOC_ID, "add_time": create_time},
                )
            ]
            self.vectorstore = FAISS.from_documents(initial_doc, self.embeddings)
            print(
                f"ğŸ†• å·²åˆå§‹åŒ–å…¨æ–°çš„å‘é‡åº“ (åˆ›å»ºæ—¶é—´: {create_time.strftime('%Y-%m-%d %H:%M:%S')})"
            )

    def _sync_valid_times(self) -> None:
        """å†…éƒ¨æ–¹æ³•ï¼šæ‰«æåº“ä¸­æ‰€æœ‰å…ƒæ•°æ®ï¼Œåˆå§‹åŒ–æœ‰æ•ˆæ—¶é—´æ˜ å°„"""
        all_docs = self.vectorstore.docstore._dict.values()
        for d in all_docs:
            did = d.metadata.get("doc_id")
            atime = d.metadata.get("add_time")
            if did and did != SYSTEM_DOC_ID:
                # åˆå§‹åŠ è½½æ—¶ï¼Œé»˜è®¤æœ‰æ•ˆèµ·å§‹æ—¶é—´ä¸ºè¯¥æ–‡æ¡£çš„æœ€æ—©åˆ‡ç‰‡æ—¶é—´
                if did not in self.doc_valid_from or atime < self.doc_valid_from[did]:
                    self.doc_valid_from[did] = atime

    def _get_loader(
        self, file_path: str
    ) -> TextLoader | UnstructuredMarkdownLoader | Docx2txtLoader | PyPDFLoader:
        """
        æ ¹æ®æ–‡ä»¶åç¼€è¿”å›å¯¹åº”çš„ LangChain åŠ è½½å™¨
        """

        ext = file_path.split(".")[-1].lower()
        if ext == "txt":
            return TextLoader(file_path, encoding="utf-8")
        elif ext == "md":
            # Markdown å»ºè®®ä½¿ç”¨éç»“æ„åŒ–åŠ è½½å™¨ï¼Œèƒ½æ›´å¥½å¤„ç†æ ‡é¢˜é€»è¾‘
            return UnstructuredMarkdownLoader(file_path)
        elif ext == "docx":
            return Docx2txtLoader(file_path)
        elif ext == "pdf":
            return PyPDFLoader(file_path)
        else:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")

    def add_document(self, file_path: str) -> None:
        """
        å¢åŠ æ–‡æ¡£ï¼šå°†æ–‡ä»¶è¯»å–ã€åˆ†å‰²å¹¶å­˜å…¥å‘é‡åº“ã€‚
        è¿™ä¸ªå‡½æ•°æœ‰é—®é¢˜ï¼Œå¦‚æœä¸€å®šè¦ä½¿ç”¨FAISSï¼Œéœ€è¦ä¼˜åŒ–é€»è¾‘ã€‚
        :param file_path: æœ¬åœ°æ–‡æ¡£è·¯å¾„
        """

        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

        file_name = os.path.basename(file_path)
        add_time = datetime.now(BEIJING_TZ)

        # é€»è¾‘ä¿æŠ¤ï¼šå¦‚æœä¹‹å‰è½¯åˆ é™¤äº†ï¼Œç°åœ¨æ¢å¤
        if file_name in self.soft_deleted_sources:
            self.soft_deleted_sources.remove(file_name)

        try:
            # 1. è‡ªåŠ¨é€‰æ‹©åŠ è½½å™¨å¹¶è§£æ
            loader = self._get_loader(file_path)
            docs = loader.load()

            # 2. æ–‡æœ¬åˆ‡åˆ†
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True
            )
            splits = text_splitter.split_documents(docs)

            # 3. ç»Ÿä¸€æ³¨å…¥å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬æ–‡æ¡£IDå’Œæ·»åŠ æ—¶é—´
            for split in splits:
                split.metadata["doc_id"] = file_name
                split.metadata["add_time"] = add_time

            # 4. å…¥åº“ (è¿½åŠ æ¨¡å¼ï¼ŒO(1) æ•ˆç‡)
            self.vectorstore.add_documents(documents=splits)
            self.vectorstore.save_local(self.index_path)

            # æ›´æ–°è¯¥æ–‡æ¡£çš„æœ‰æ•ˆèµ·å§‹æ—¶é—´ä¸ºå½“å‰æ·»åŠ æ—¶é—´ï¼Œæ—§ç‰ˆæœ¬çš„åˆ‡ç‰‡å°†è‡ªåŠ¨åœ¨æ£€ç´¢æ—¶è¢«é€»è¾‘å±è”½
            self.doc_valid_from[file_name] = add_time

            print(
                f"âœ… æ–‡æ¡£ '{file_name}' å·²å…¥åº“ï¼Œæ—§ç‰ˆå·²é€»è¾‘å±è”½ã€‚æ—¶é—´: {add_time.strftime('%Y-%m-%d %H:%M:%S')}"
            )

        except Exception as e:
            print(f"âŒ è§£ææ–‡ä»¶ {file_name} å‡ºé”™: {e}")

    def soft_delete(self, file_name: str) -> None:
        """
        è½¯åˆ é™¤ï¼šä»…åœ¨å‘é‡åº“ä¸­è®°å½•è¯¥æ–‡ä»¶å·²â€œå¤±æ•ˆâ€ï¼Œæœç´¢æ—¶ä¼šè‡ªåŠ¨è·³è¿‡ã€‚
        """
        # é€šè¿‡å°†æœ‰æ•ˆèµ·å§‹æ—¶é—´è®¾ä¸ºâ€œç°åœ¨â€ï¼Œé€»è¾‘ä¸Šå±è”½æ‰ä¹‹å‰å…¥åº“çš„æ‰€æœ‰åŒååˆ‡ç‰‡
        self.doc_valid_from[file_name] = datetime.now(BEIJING_TZ)
        self.soft_deleted_sources.add(file_name)
        print(f"ğŸŸ¡ å·²è½¯åˆ é™¤ï¼ˆæ ‡è®°å±è”½ï¼‰: {file_name}ï¼Œç‰©ç†æ•°æ®ä»ä¿ç•™ï¼ŒæŸ¥è¯¢å·²ä¸å¯è§ã€‚")

    def hard_delete(self) -> None:
        """
        ç¡¬åˆ é™¤ï¼šè€—æ—¶æ“ä½œï¼Œå»ºè®®å®šæœŸæ‰§è¡Œã€‚
        åŸç†ï¼šä» docstore ä¸­æå–æ‰€æœ‰æœªè¢«è½¯åˆ çš„æ–‡æ¡£ï¼Œå½»åº•ä¸¢å¼ƒå·²åˆ é™¤æ•°æ®å¹¶é‡æ„ç´¢å¼•ã€‚
        """
        if (
            not self.soft_deleted_sources
            and len(list(self.vectorstore.docstore._dict.values())) > 1
        ):
            # å¦‚æœæ²¡æœ‰è½¯åˆ æ ‡è®°ï¼Œå¯ä»¥è€ƒè™‘è·³è¿‡ï¼Œé™¤éæ˜¯ä¸ºäº†æ¸…ç†å†å²ç‰ˆæœ¬
            print("ğŸ’¡ æš‚æ— æ˜ç¡®çš„è½¯åˆ é™¤æ ‡è®°éœ€è¦ç‰©ç†æ¸…ç†ã€‚")

        all_docs = list(self.vectorstore.docstore._dict.values())

        # è¿‡æ»¤é€»è¾‘ï¼šä»…ä¿ç•™ 1. ç³»ç»Ÿæ–‡æ¡£ 2. æ²¡è¢«è½¯åˆ ä¸”æ—¶é—´æˆ³ç¬¦åˆæœ€æ–°æœ‰æ•ˆæ—¶é—´çš„åˆ‡ç‰‡
        remaining_docs = [
            doc
            for doc in all_docs
            if doc.metadata.get("doc_id") == SYSTEM_DOC_ID
            or (
                doc.metadata.get("doc_id") not in self.soft_deleted_sources
                and doc.metadata.get("doc_id") in self.doc_valid_from
                and doc.metadata.get("add_time")
                >= self.doc_valid_from[doc.metadata.get("doc_id")]
            )
        ]

        if remaining_docs:
            # å½»åº•é‡å»º FAISS ç´¢å¼•ï¼ˆé‡Šæ”¾ç‰©ç†ç©ºé—´ï¼‰
            self.vectorstore = FAISS.from_documents(remaining_docs, self.embeddings)
            self.vectorstore.save_local(self.index_path)
        else:
            # å¦‚æœæ–‡æ¡£è¢«åˆ å…‰äº†ï¼Œåˆ™é‡ç½®åº“
            if os.path.exists(self.index_path):
                shutil.rmtree(self.index_path)
            self._load_or_create()

        # æ¸…ç©ºè½¯åˆ é™¤åå•ï¼Œå› ä¸ºæ•°æ®å·²ç»ä»ç‰©ç†ä¸ŠæŠ¹é™¤äº†
        self.soft_deleted_sources.clear()
        print("ğŸ”¥ ç¡¬åˆ é™¤å®Œæˆï¼šç´¢å¼•å·²é‡æ„ï¼Œç‰©ç†ç©ºé—´å·²é‡Šæ”¾ï¼Œä»…ä¿ç•™æœ€æ–°ç‰ˆæœ¬ã€‚")

    def search(
        self,
        query: str,
        k: int = DEFAULT_SEARCH_K,
        t: float = SIMILARITY_THRESHOLD,
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥è¯¢ï¼šåœ¨ç›¸ä¼¼åº¦æœç´¢çš„åŸºç¡€ä¸Šå¢åŠ å®æ—¶è¿‡æ»¤é€»è¾‘ã€‚
        è¿”å›åŒ…å«å†…å®¹ã€æ¥æºIDå’Œæ·»åŠ æ—¶é—´çš„å­—å…¸åˆ—è¡¨ã€‚
        """

        # æ—¶é—´æˆ³é€»è¾‘è¿‡æ»¤å‡½æ•°
        def time_filter(meta):
            did = meta.get("doc_id")
            atime = meta.get("add_time")
            if did == SYSTEM_DOC_ID:
                return False  # æ°¸è¿œä¸è¿”å›å ä½æ–‡æ¡£
            if did in self.soft_deleted_sources:
                return False
            # æ ¸å¿ƒï¼šåˆ‡ç‰‡æ—¶é—´å¿…é¡» >= è¯¥æ–‡æ¡£è¦æ±‚çš„æœ‰æ•ˆèµ·å§‹æ—¶é—´
            if did in self.doc_valid_from:
                return atime >= self.doc_valid_from[did]
            return True

        # ä½¿ç”¨ç›¸ä¼¼åº¦åˆ†å€¼æœç´¢å¹¶åº”ç”¨è¿‡æ»¤å™¨
        docs_and_scores = self.vectorstore.similarity_search_with_score(
            query, k=k, filter=time_filter
        )

        formatted_results = []
        for doc, score in docs_and_scores:
            if score < t:
                add_time = doc.metadata.get("add_time")
                time_str = (
                    add_time.strftime("%Y-%m-%d %H:%M:%S")
                    if isinstance(add_time, datetime)
                    else "Unknown"
                )
                formatted_results.append(
                    {
                        "content": doc.page_content,
                        "doc_id": doc.metadata.get("doc_id"),
                        "add_time": time_str,
                        "score": round(float(score), 4),
                    }
                )
        return formatted_results

    def get_overview(self) -> Dict[str, Any]:
        """
        æ¦‚è§ˆï¼šæ˜¾ç¤ºå½“å‰å‘é‡åº“çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬æ–‡æ¡£åˆ—è¡¨å’Œæ›´æ–°ç»Ÿè®¡ã€‚
        """
        all_docs = list(self.vectorstore.docstore._dict.values())
        system_doc = next(
            (d for d in all_docs if d.metadata.get("doc_id") == SYSTEM_DOC_ID), None
        )
        create_time_raw = system_doc.metadata.get("add_time") if system_doc else None
        create_time_str = (
            create_time_raw.strftime("%Y-%m-%d %H:%M:%S")
            if isinstance(create_time_raw, datetime)
            else "Unknown"
        )

        doc_stats = {}
        for doc in all_docs:
            did = doc.metadata.get("doc_id")
            atime = doc.metadata.get("add_time")
            if did and did != SYSTEM_DOC_ID:
                if did not in doc_stats or (
                    isinstance(atime, datetime) and atime > doc_stats[did]
                ):
                    doc_stats[did] = atime

        sorted_docs = sorted(doc_stats.items(), key=lambda x: x[1], reverse=True)
        latest_update = (
            sorted_docs[0][1].strftime("%Y-%m-%d %H:%M:%S")
            if sorted_docs
            else create_time_str
        )

        print("\n" + "=" * 25 + " å‘é‡åº“å®æ—¶æ¦‚è§ˆ " + "=" * 25)
        print(
            f"ğŸ“ è·¯å¾„: {self.index_path} | ğŸ“… åˆ›å»º: {create_time_str} | ğŸ•’ æ›´æ–°: {latest_update}"
        )
        print(f"ğŸ“Š è§„æ¨¡: {len(all_docs)-1} åˆ‡ç‰‡ | {len(doc_stats)} æ–‡æ¡£")
        for name, time in sorted_docs:
            status = (
                "[æ­£å¸¸]"
                if (
                    name not in self.soft_deleted_sources
                    and time >= self.doc_valid_from.get(name, time)
                )
                else "[å·²å±è”½/è¿‡æœŸ]"
            )
            print(
                f"  - {name:<20} | æœ€ç»ˆç‰ˆæœ¬: {time.strftime('%Y-%m-%d %H:%M:%S')} | {status}"
            )
        print("=" * 66 + "\n")
        return {"documents": sorted_docs}

    def reset_index(self) -> None:
        """
        ä¸€é”®åˆå§‹åŒ–/é‡ç½®å‘é‡åº“ï¼š
        å½»åº•åˆ é™¤ç£ç›˜ä¸Šçš„ç´¢å¼•æ–‡ä»¶å¹¶æ¸…ç©ºå†…å­˜çŠ¶æ€ï¼Œæ¢å¤åˆ°åˆå§‹ç©ºåº“çŠ¶æ€ã€‚
        """

        # 1. ç‰©ç†åˆ é™¤æœ¬åœ°ç´¢å¼•ç›®å½•
        if os.path.exists(self.index_path):
            try:
                shutil.rmtree(self.index_path)
                print(f"ğŸ§¹ å·²ç‰©ç†åˆ é™¤æœ¬åœ°ç´¢å¼•ç›®å½•: {self.index_path}")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤ç´¢å¼•ç›®å½•å¤±è´¥: {e}")

        # 2. æ¸…ç©ºå†…å­˜ä¸­çš„è½¯åˆ é™¤è®°å½•
        self.soft_deleted_sources.clear()
        self.doc_valid_from.clear()

        # 3. è°ƒç”¨åˆå§‹åŒ–æ–¹æ³•é‡æ–°åˆ›å»ºç©ºåº“
        self._load_or_create(is_reset=True)
        print("âœ¨ å‘é‡åº“å·²å®Œæˆä¸€é”®é‡ç½®ã€‚")
