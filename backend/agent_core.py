#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Agentæ ¸å¿ƒæ¨¡å— - é›†æˆNSyså’ŒNCUæ€§èƒ½åˆ†æ + Agentic-RAG
"""

import re
import os
import asyncio
import json
from pathlib import Path
import sys
from typing import Dict, List, Optional, Tuple


# å¯¼å…¥åˆ†æå·¥å…·
sys.path.insert(0, str(Path(__file__).parent))

from utils.nsys_to_ncu_analyzer import create_sglang_analysis_workflow
from offline_llm import get_offline_qwen_client
from knowledge_bases.vector_kb_manager import VectorKBManager

OFFLINE_QWEN_PATH = Path(os.getenv("QWEN_LOCAL_MODEL_PATH", "./.models/Qwen/Qwen3-4B"))


class AIAgent:
    """AI Agentæ ¸å¿ƒç±» - è‡ªåŠ¨åŒ–æ€§èƒ½åˆ†æï¼ˆæ”¯æŒ Agentic-RAGï¼‰"""

    def __init__(self, config: Dict):
        self.config = config

        # sglang å’Œæ¨¡å‹è·¯å¾„
        self.sglang_path = Path(config.get("sglang_path"))
        self.models_path = Path(config.get("models_path"))
        self.model_mappings = config.get("model_mappings")

        # è¾“å‡ºç›®å½•
        self.results_dir = Path(config.get("output", {}).get("results_dir"))
        self.results_dir.mkdir(exist_ok=True)

        # æœ¬åœ° LLM å®¢æˆ·ç«¯ï¼ˆç”¨äº Agentic å†³ç­–ï¼‰
        self.offline_qwen_path = Path(config.get("offline_qwen_path"))
        self.llm_client = get_offline_qwen_client(self.offline_qwen_path)

        # åˆ†æå·¥å…·é…ç½®
        self.profiling_config = config.get("profiling_tools")
        self.analysis_defaults = config.get("analysis_defaults")

        # ç¼“å­˜
        self.last_analysis_dir: Optional[str] = None
        self.last_analysis_dirs: List[str] = []
        self.last_analysis_reports: List[str] = []
        self.last_analysis_table: Optional[str] = None

        # å‘é‡çŸ¥è¯†åº“ç›¸å…³
        self.kb = VectorKBManager()
        kb_config = config.get("vector_store")
        self.persist_directory = kb_config.get("persist_directory")
        self.embedding_model = kb_config.get("embedding_model")
        self.chunk_size = kb_config.get("chunk_size")
        self.chunk_overlap = kb_config.get("chunk_overlap")
        self.default_search_k = kb_config.get("default_search_k")
        self.similarity_threshold = kb_config.get("similarity_threshold")

    async def process_message(self, message: str) -> str:
        """Agentic-RAG ä¸»æµç¨‹ï¼šæ£€ç´¢ â†’ æ¨ç† â†’ æ‰§è¡Œæˆ–å›ç­”"""

        # Step 1: RAG æ£€ç´¢
        retrieved_contexts = self.kb.search(query=message, k=3)
        rag_context = ""
        if retrieved_contexts:
            rag_snippets = [
                f"ã€{res['doc_id']}ã€‘{res['content'][:300]}"
                for res in retrieved_contexts
            ]
            rag_context = "\n".join(rag_snippets)

        # Step 2: æ„é€ å†³ç­– Prompt
        rag_prompt = f"""ä½ æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰ä¸å¤§æ¨¡å‹æ€§èƒ½åˆ†æä¸“å®¶ã€‚
è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯åˆ¤æ–­ç”¨æˆ·æ„å›¾ï¼Œå¹¶å†³å®šæ˜¯å¦éœ€è¦å¯åŠ¨ NSys/NCU æ€§èƒ½åˆ†ææµç¨‹ã€‚

### ç”¨æˆ·åŸå§‹è¯·æ±‚
{message}

### ç›¸å…³çŸ¥è¯†åº“ç‰‡æ®µï¼ˆå¦‚æœ‰ï¼‰
{rag_context if rag_context else "æ— ç›¸å…³å†å²æ–‡æ¡£"}

### ä½ çš„ä»»åŠ¡
1. å¦‚æœçŸ¥è¯†åº“å·²åŒ…å«è¶³å¤Ÿç­”æ¡ˆï¼ˆä¾‹å¦‚å¸¸è§é—®é¢˜ã€å·²çŸ¥ç“¶é¢ˆã€ä¼˜åŒ–å»ºè®®ï¼‰ï¼Œè¯·ç›´æ¥å›ç­”ã€‚
2. å¦‚æœè¯·æ±‚æ¶‰åŠå¯¹å…·ä½“æ¨¡å‹çš„æ€§èƒ½åˆ†æï¼ˆå¦‚â€œåˆ†æ qwen-7bâ€ã€â€œæµ‹è¯• batch_size=4â€ï¼‰ï¼Œåˆ™å¿…é¡»è¿”å›ä¸¥æ ¼ JSON æ ¼å¼ï¼š
   {{"action": "run_analysis", "model": "æ¨¡å‹å", "analysis_type": "ç±»å‹", "params": {{"batch_size": [...], "input_len": [...], "output_len": [...]}}}}
3. å¦‚æœä¿¡æ¯ä¸è¶³æˆ–æ¨¡å‹æœªçŸ¥ï¼Œè¯·è¿”å›ï¼š
   {{"action": "clarify", "message": "è¯·è¯´æ˜..."}}

åªè¾“å‡º JSON æˆ–ç›´æ¥å›ç­”ï¼Œä¸è¦è§£é‡Šã€‚"""

        # Step 3: è°ƒç”¨ LLM å†³ç­–
        try:
            decision_output = self.llm_client.generate(rag_prompt, max_tokens=512)
            decision_text = decision_output.strip()

            # å°è¯•è§£æ JSON
            if decision_text.startswith("{") and decision_text.endswith("}"):
                decision = json.loads(decision_text)

                if decision.get("action") == "run_analysis":
                    model_name = decision["model"]
                    analysis_type = decision["analysis_type"]
                    params = decision.get("params", {})
                    return await self._execute_analysis_flow(
                        model_name, analysis_type, params
                    )

                elif decision.get("action") == "clarify":
                    return f"ğŸ’¡ {decision['message']}"

            else:
                # LLM ç›´æ¥å›ç­”ï¼ˆçŸ¥è¯†åº“å‘½ä¸­ï¼‰
                return decision_text

        except Exception as e:
            print(f"âš ï¸ LLM å†³ç­–å¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™å¼•æ“: {e}")
            return await self._fallback_rule_based_process(message)

    async def _fallback_rule_based_process(self, message: str) -> str:
        """åŸè§„åˆ™å¼•æ“é€»è¾‘ï¼ˆå…¼å®¹ fallbackï¼‰"""
        model_name = self._extract_model_name(message)
        analysis_type = self._extract_analysis_type(message)
        params = self._extract_parameters(message)

        if not params.get("batch_size"):
            params["batch_size"] = self.analysis_defaults.get("batch_size", [1])
        if not params.get("input_len"):
            params["input_len"] = self.analysis_defaults.get("input_len", [128])
        if not params.get("output_len"):
            params["output_len"] = self.analysis_defaults.get("output_len", [1])

        if model_name:
            model_path = self._resolve_model_path(model_name)
            if not model_path:
                return f"âŒ æœªæ‰¾åˆ°æ¨¡å‹ '{model_name}'ï¼Œå¯ç”¨æ¨¡å‹: {', '.join(self.model_mappings.keys())}"
            return await self._run_analysis(
                model_path=model_path, analysis_type=analysis_type, params=params
            )
        else:
            return f"ğŸ’¡ è¯·æŒ‡å®šæ¨¡å‹ï¼Œå¦‚â€œåˆ†æ qwen-7bâ€ã€‚å¯ç”¨æ¨¡å‹: {', '.join(self.model_mappings.keys())}"

    async def _execute_analysis_flow(
        self, model_name: str, analysis_type: str, params: Dict
    ) -> str:
        model_path = self._resolve_model_path(model_name)
        if not model_path:
            return f"âŒ æ¨¡å‹è·¯å¾„è§£æå¤±è´¥: {model_name}"
        return await self._run_analysis(
            model_path=model_path, analysis_type=analysis_type, params=params
        )

    async def _run_analysis(
        self, model_path: str, analysis_type: str, params: Dict
    ) -> str:
        results = []
        self.last_analysis_table = None
        self.last_analysis_reports = []
        self.last_analysis_dirs = []
        self.last_analysis_dir = None

        batch_sizes = params.get("batch_size", [1])
        input_lens = params.get("input_len", [128])
        output_lens = params.get("output_len", [1])

        batch_size = batch_sizes[0] if isinstance(batch_sizes, list) else batch_sizes
        input_len = input_lens[0] if isinstance(input_lens, list) else input_lens
        output_len = output_lens[0] if isinstance(output_lens, list) else output_lens

        # ========== æ–°å¢ï¼šæµ‹è¯•æ¨¡å¼ - è·³è¿‡çœŸå®åˆ†æ ==========
        if os.getenv("AGENT_TEST_MODE", "0") == "1":
            # æ¨¡æ‹Ÿä¸€ä¸ªæ€§èƒ½æŠ¥å‘Šæ–‡æœ¬
            mock_report = """
ä¸€ã€æ€»ä½“ç»Ÿè®¡
- æ€»kernelsæ•°é‡: 42
- æ€»kernelæ‰§è¡Œæ—¶é—´: 125.6 ms

äºŒã€çƒ­ç‚¹Kernelsï¼ˆæŒ‰æ—¶é—´é™åºï¼‰
1. flash_attn_fwd_kernel
   - æ‰§è¡Œæ—¶é—´: 45.2 ms
   - æ—¶é—´å æ¯”: 36.0%
2. rms_norm_kernel
   - æ‰§è¡Œæ—¶é—´: 28.7 ms
   - æ—¶é—´å æ¯”: 22.8%
3. fused_mlp_kernel
   - æ‰§è¡Œæ—¶é—´: 18.3 ms
   - æ—¶é—´å æ¯”: 14.6%
"""
            report_path = self.results_dir / "mock_integrated_performance_report.md"
            report_path.write_text(mock_report.strip(), encoding="utf-8")

            # æ„é€ æ¨¡æ‹Ÿçš„ run_records
            run_records = [("0", self.results_dir)]

            # ç»§ç»­èµ°åç»­çš„è¡¨æ ¼ç”Ÿæˆå’Œè¿”å›é€»è¾‘
        else:
            # ========== åŸæœ‰çœŸå®åˆ†æé€»è¾‘ï¼ˆä»…åœ¨éæµ‹è¯•æ¨¡å¼ä¸‹æ‰§è¡Œï¼‰==========
            try:
                analysis_workflow = create_sglang_analysis_workflow()
                workflow_output = await asyncio.get_event_loop().run_in_executor(
                    None,
                    analysis_workflow,
                    str(model_path),
                    batch_size,
                    input_len,
                    output_len,
                )

                run_records: List[Tuple[str, Path]] = []
                if isinstance(workflow_output, list):
                    for idx, item in enumerate(workflow_output):
                        gpu_label: str
                        output_path: Optional[str] = None
                        if isinstance(item, dict):
                            gpu_label = str(item.get("gpu", idx))
                            output_path = item.get("dir") or item.get("path")
                        else:
                            gpu_label = str(idx)
                            output_path = str(item)
                        if output_path:
                            run_records.append((gpu_label, Path(output_path)))
                elif workflow_output:
                    run_records.append(("0", Path(str(workflow_output))))

                if not run_records:
                    results.append("âš ï¸ **åˆ†æå·²å®Œæˆï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºç›®å½•**")
                    return "\n".join(results)

            except Exception as e:
                import traceback

                error_detail = traceback.format_exc()
                results.append(f"""
âŒ **åˆ†ææ‰§è¡Œå¤±è´¥**

é”™è¯¯ä¿¡æ¯: {str(e)}

è¯¦ç»†é”™è¯¯:
{error_detail}

ğŸ’¡ **å¸¸è§é—®é¢˜è§£å†³**:
1. ç¡®ä¿å·²å®‰è£… nsys å’Œ ncu å·¥å…·
2. ç¡®ä¿ SGlang å·²æ­£ç¡®å®‰è£…
3. ç¡®ä¿æ¨¡å‹æ–‡ä»¶è·¯å¾„æ­£ç¡®
4. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜
""")
                return "\n".join(results)

        # ========== ä»¥ä¸‹ä¸ºå…¬å…±åå¤„ç†é€»è¾‘ï¼ˆæµ‹è¯•/çœŸå®æ¨¡å¼å…±ç”¨ï¼‰==========
        self.last_analysis_dirs = [str(path) for _, path in run_records]

        report_infos = []
        missing_reports = []
        for idx, (gpu_label, output_dir) in enumerate(run_records):
            report_path = output_dir / "integrated_performance_report.md"
            if report_path.exists():
                report_text = report_path.read_text(encoding="utf-8")
                report_infos.append(
                    {
                        "gpu": gpu_label,
                        "dir": output_dir,
                        "report": report_path,
                        "text": report_text,
                        "index": idx,
                    }
                )
            else:
                missing_reports.append(output_dir)

        if not report_infos:
            dir_lines = "\n".join(f"  â€¢ {path}" for _, path in run_records)
            results.append(f"""
âš ï¸ **åˆ†æå·²å®Œæˆï¼Œä½†æœªç”ŸæˆæŠ¥å‘Šæ–‡ä»¶**

ğŸ“ ç»“æœç›®å½•:
{dir_lines}
ğŸ’¡ è¯·æ£€æŸ¥ç›®å½•ä¸­çš„å…¶ä»–è¾“å‡ºæ–‡ä»¶
""")
            return "\n".join(results)

        primary_info = report_infos[0]
        self.last_analysis_dir = str(primary_info["dir"])
        self.last_analysis_reports = [str(info["report"]) for info in report_infos]
        summary = self._extract_report_summary(primary_info["text"])

        try:
            loop = asyncio.get_event_loop()
            if len(report_infos) > 1:
                table_markdown = self._generate_multi_gpu_table(
                    [info["text"] for info in report_infos],
                    [info["gpu"] for info in report_infos],
                )
            else:
                table_markdown = await loop.run_in_executor(
                    None, self._generate_report_table, primary_info["text"]
                )
        except Exception as table_exc:
            table_markdown = f"âš ï¸ è¡¨æ ¼ç”Ÿæˆå¤±è´¥: {table_exc}"

        self.last_analysis_table = table_markdown

        dir_lines = "\n".join(
            f"  â€¢ {self._format_gpu_label(info['gpu'], info['index'])}: {info['dir']}"
            for info in report_infos
        )

        missing_lines = ""
        if missing_reports:
            missing_lines = "\n".join(f"  â€¢ {path}" for path in missing_reports)
            missing_lines = f"\nâš ï¸ æœªæ‰¾åˆ°ä»¥ä¸‹ç›®å½•çš„æŠ¥å‘Šæ–‡ä»¶:\n{missing_lines}\n"

        results.append(f"""
âœ… **åˆ†æå®Œæˆ!**

ğŸ“ **ç»“æœç›®å½•**:
{dir_lines}
ğŸ“„ **æŠ¥å‘Šæ–‡ä»¶**: {primary_info["report"]}
{missing_lines}
{summary}

ğŸ“Œ **çƒ­ç‚¹Kernelè¡¨æ ¼é¢„è§ˆ**:
{table_markdown}

ğŸ” **è¯¦ç»†æŠ¥å‘Š**: è¯·æŸ¥çœ‹ {primary_info["report"]}
ğŸ“Š **å¯è§†åŒ–å›¾è¡¨**: è¯·æŸ¥çœ‹å¯¹åº”ç»“æœç›®å½•ä¸­çš„å›¾ç‰‡æ–‡ä»¶
""")

        return "\n".join(results)

    @staticmethod
    def _generate_report_table(report_text: str) -> str:
        client = get_offline_qwen_client(OFFLINE_QWEN_PATH)
        return client.report_to_table(report_text)

    def _generate_multi_gpu_table(
        self, report_texts: List[str], gpu_labels: List[str]
    ) -> str:
        if not report_texts:
            return "âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„æŠ¥å‘Šå†…å®¹"

        parsed_entries = [
            self._parse_kernel_entries_from_report(text) for text in report_texts
        ]
        if not parsed_entries or not parsed_entries[0]:
            return "âš ï¸ æœªèƒ½è§£æå¤šGPUè¡¨æ ¼æ•°æ®"

        label_cells = [
            self._format_gpu_label(lbl, idx) for idx, lbl in enumerate(gpu_labels)
        ]
        header_cells = ["Kernel"]
        for lbl in label_cells:
            header_cells.extend([f"{lbl} Duration(ms)", f"{lbl} Ratio(%)"])

        header = "| " + " | ".join(header_cells) + " |"
        divider = "| " + " | ".join(["---"] * len(header_cells)) + " |"

        max_len = max(len(entries) for entries in parsed_entries)
        rows = []
        for idx in range(max_len):
            name_candidates = []
            for entries in parsed_entries:
                if idx < len(entries) and entries[idx]["name"]:
                    name_candidates.append(entries[idx]["name"])
            base_name = name_candidates[0] if name_candidates else f"Kernel {idx + 1}"
            alt_names = {nm for nm in name_candidates if nm != base_name}
            if alt_names:
                merged_name = base_name + " / " + " / ".join(sorted(alt_names))
            else:
                merged_name = base_name

            row_cells = [merged_name]
            for entries in parsed_entries:
                if idx < len(entries):
                    row_cells.append(entries[idx]["duration"])
                    row_cells.append(entries[idx]["ratio"])
                else:
                    row_cells.extend(["", ""])
            rows.append("| " + " | ".join(row_cells) + " |")

        return "\n".join([header, divider, *rows])

    def _parse_kernel_entries_from_report(
        self, report_text: str
    ) -> List[Dict[str, str]]:
        entries: List[Dict[str, str]] = []
        lines = report_text.splitlines()
        idx = 0
        total_lines = len(lines)
        while idx < total_lines:
            raw_line = lines[idx]
            if raw_line.strip().startswith("äºŒã€"):
                break
            match = re.match(r"^\s*\d+\.\s+(.*)$", raw_line)
            if match:
                name = match.group(1).strip()
                duration = ""
                ratio = ""
                idx += 1
                while idx < total_lines:
                    line = lines[idx].strip()
                    if line.startswith("- æ‰§è¡Œæ—¶é—´"):
                        dur_match = re.search(r"([0-9.]+)\s*ms", line)
                        if dur_match:
                            duration = dur_match.group(1)
                    elif line.startswith("- æ—¶é—´å æ¯”"):
                        ratio_match = re.search(r"([0-9.]+)\s*%", line)
                        if ratio_match:
                            ratio = ratio_match.group(1)
                    elif re.match(r"^\s*\d+\.", lines[idx]) or line.startswith("äºŒã€"):
                        break
                    idx += 1
                entries.append({"name": name, "duration": duration, "ratio": ratio})
            else:
                idx += 1
        return entries

    @staticmethod
    def _format_gpu_label(label: str, index: int) -> str:
        if not label:
            return f"GPU{index}"
        normalized = label.strip()
        if not normalized:
            return f"GPU{index}"
        if normalized.lower().startswith("gpu"):
            return normalized.upper()
        return f"GPU{normalized}"

    def _extract_report_summary(self, report_content: str) -> str:
        lines = report_content.split("\n")
        summary_lines = []

        for i, line in enumerate(lines):
            if "æ€»kernelsæ•°é‡" in line or "æ€»kernelæ‰§è¡Œæ—¶é—´" in line:
                summary_lines.append(line)
            elif "ğŸ”¥ è¯†åˆ«çš„çƒ­ç‚¹Kernels" in line:
                summary_lines.append("\n**ğŸ”¥ çƒ­ç‚¹Kernels (Top 3):**")
                for j in range(i + 1, min(i + 10, len(lines))):
                    if lines[j].strip() and lines[j].startswith(("1.", "2.", "3.")):
                        summary_lines.append(lines[j][:100])
                break

        if summary_lines:
            return "\n".join(summary_lines)
        else:
            return "**ğŸ“Š åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ–‡ä»¶**"

    def _resolve_model_path(self, model_name: str) -> Optional[str]:
        if model_name in self.model_mappings:
            mapped_path = self.model_mappings[model_name]
            if Path(mapped_path).is_absolute():
                return mapped_path
            full_path = self.models_path / mapped_path
            return str(full_path)

        if Path(model_name).exists():
            return model_name

        potential_path = self.models_path / model_name
        if potential_path.exists():
            return str(potential_path)

        return None

    def _extract_model_name(self, prompt: str) -> Optional[str]:
        for model_name in self.model_mappings.keys():
            if model_name.lower() in prompt.lower():
                return model_name

        patterns = [
            r"llama[^/\s]*-?\d*[^/\s]*-?\d+[bB]?",
            r"qwen[^/\s]*-?\d*[^/\s]*-?\d+[bB]?",
            r"chatglm[^/\s]*-?\d+[bB]?",
            r"baichuan[^/\s]*-?\d+[bB]?",
            r"vicuna[^/\s]*-?\d+[bB]?",
            r"mistral[^/\s]*-?\d+[bB]?",
            r"mixtral[^/\s]*-?\d+[bB]?",
        ]

        for pattern in patterns:
            match = re.search(pattern, prompt, re.IGNORECASE)
            if match:
                return match.group(0)

        return None

    def _extract_analysis_type(self, prompt: str) -> str:
        prompt_lower = prompt.lower()
        if (
            "ncu" in prompt_lower
            or "kernel" in prompt_lower
            or "æ·±åº¦" in prompt_lower
            or "nsight compute" in prompt_lower
        ):
            return "ncu (æ·±åº¦kernelåˆ†æ)"
        elif (
            "nsys" in prompt_lower
            or "å…¨å±€" in prompt_lower
            or "nsight systems" in prompt_lower
        ):
            return "nsys (å…¨å±€æ€§èƒ½åˆ†æ)"
        elif "é›†æˆ" in prompt_lower or "ç»¼åˆ" in prompt_lower or "å®Œæ•´" in prompt_lower:
            return "auto (é›†æˆåˆ†æ: nsys + ncu)"
        else:
            return "auto (é›†æˆåˆ†æ: nsys + ncu)"

    def _extract_parameters(self, prompt: str) -> Dict:
        params = {}

        batch_match = re.search(
            r"batch[-_\s]*size?[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)", prompt, re.IGNORECASE
        )
        if batch_match:
            batch_sizes = [
                int(x.strip())
                for x in re.split(r"[,ï¼Œ\s]+", batch_match.group(1))
                if x.strip()
            ]
            params["batch_size"] = batch_sizes

        input_match = re.search(
            r"input[-_\s]*len[gth]*[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)",
            prompt,
            re.IGNORECASE,
        )
        if input_match:
            input_lens = [
                int(x.strip())
                for x in re.split(r"[,ï¼Œ\s]+", input_match.group(1))
                if x.strip()
            ]
            params["input_len"] = input_lens

        output_match = re.search(
            r"output[-_\s]*len[gth]*[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)",
            prompt,
            re.IGNORECASE,
        )
        if output_match:
            output_lens = [
                int(x.strip())
                for x in re.split(r"[,ï¼Œ\s]+", output_match.group(1))
                if x.strip()
            ]
            params["output_len"] = output_lens

        return params

    def get_available_models(self) -> List[str]:
        return list(self.model_mappings.keys())

    def get_analysis_status(self) -> Dict:
        return {
            "available_models": self.get_available_models(),
            "results_directory": str(self.results_dir),
            "nsys_enabled": self.profiling_config.get("nsys", {}).get("enabled", True),
            "ncu_enabled": self.profiling_config.get("ncu", {}).get("enabled", True),
        }


# ==================== ç®€å•æµ‹è¯•ç”¨ä¾‹ ====================
if __name__ == "__main__":
    from pathlib import Path

    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£ç›®å½•
    test_doc_dir = Path("./documents")
    test_doc_dir.mkdir(exist_ok=True)
    test_file = test_doc_dir / "optim_tips.md"
    if not test_file.exists():
        test_file.write_text(
            "# Qwen ä¼˜åŒ–å»ºè®®\n"
            "å½“ batch_size > 8 æ—¶ï¼ŒL2 ç¼“å­˜å‘½ä¸­ç‡æ˜¾è‘—ä¸‹é™ã€‚\n"
            "å»ºè®® input_len æ§åˆ¶åœ¨ 512 ä»¥å†…ä»¥é¿å…æ˜¾å­˜æº¢å‡ºã€‚\n"
            "çƒ­ç‚¹ kernel: flash_attn_fwd, rms_norm_kernel\n"
            "å¯¹äº qwen-1.8bï¼Œæ¨è batch_size=1~4ã€‚"
        )

    # åˆå§‹åŒ– KB å¹¶åŠ è½½
    kb = VectorKBManager()
    kb.add_document(str(test_file))

    # æ¨¡æ‹Ÿé…ç½®
    mock_config = {
        "sglang_path": "./SGlang",
        "models_path": "./models",
        "model_mappings": {
            "qwen-1.8b": "Qwen1.5-1.8B",
            "llama-3-8b": "Meta-Llama-3-8B",
        },
        "output": {"results_dir": "./test_results"},
        "analysis_defaults": {
            "batch_size": [1],
            "input_len": [128],
            "output_len": [32],
        },
    }

    agent = AIAgent(mock_config)

    async def run_tests():
        print("ğŸ” æµ‹è¯• 1: çŸ¥è¯†åº“é—®ç­”ï¼ˆåº”ç›´æ¥å›ç­”ï¼‰")
        resp1 = await agent.process_message("Qwen å¤§ batch æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ")
        print(resp1)
        print("\n" + "=" * 60 + "\n")

        print("ğŸš€ æµ‹è¯• 2: å¯åŠ¨æ€§èƒ½åˆ†æï¼ˆåº”è§¦å‘åˆ†ææµç¨‹ï¼‰")
        resp2 = await agent.process_message(
            "åˆ†æ qwen-1.8bï¼Œbatch_size=4, input_len=256"
        )
        print(resp2)

    asyncio.run(run_tests())
