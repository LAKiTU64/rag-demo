#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""å¤šèŠ‚ç‚¹æ€§èƒ½åˆ†æç»“æœåˆå¹¶å·¥å…·

ç”¨é€”:
  å½“ä½ åœ¨å¤šä¸ªæœºå™¨ / GPU èŠ‚ç‚¹ä¸Šåˆ†åˆ«è¿è¡Œäº† nsys_to_ncu_analyzer.py (æˆ– SGlang é›†æˆåˆ†æå·¥ä½œæµ)
  ä¼šåœ¨å„è‡ªçš„è¾“å‡ºç›®å½•ç”Ÿæˆ:
    - comprehensive_analysis.json  ç»¼åˆæ±‡æ€» (ä¸å«çƒ­ç‚¹ kernel è¯¦ç»†åˆ—è¡¨)
    - hot_kernels.json            çƒ­ç‚¹ kernel åˆ—è¡¨ (step2 äº§ç‰©)
    - integrated_performance_report.md  å•èŠ‚ç‚¹ Markdown æŠ¥å‘Š

æœ¬è„šæœ¬æ¥å—å¤šä¸ªåˆ†æè¾“å‡ºç›®å½•, åˆå¹¶ä¸ºç»Ÿä¸€çš„:
    - multi_node_comprehensive_analysis.json
    - multi_node_integrated_report.md
å¹¶å¯é€‰æ‹©å†™å…¥çŸ¥è¯†åº“ (FAISS æˆ– TF-IDF fallback) ç”¨äºåç»­ RAG æ£€ç´¢ã€‚

åˆå¹¶ç­–ç•¥(ç®€åŒ–å¯å‘å¼):
 1. kernel_analysis: é€å­—æ®µæ±‚å’Œ (æ—¶é—´ã€æ•°é‡), é‡æ–°è®¡ç®—å¹³å‡å€¼; unique_kernels åˆå¹¶å»é‡æ±‚æ•°;
 2. memory_analysis: total_transfers/total_data_mb/avg_bandwidth ä»¥åŠ æƒ (æ•°æ®é‡) æˆ–æ±‚å¹³å‡;
 3. timeline_analysis: æ‰§è¡Œè·¨åº¦å– min(first) ä¸ max(last) é‡æ–°è®¡ç®—;
 4. çƒ­ç‚¹ kernels: åˆå¹¶æ‰€æœ‰ hot_kernels.json, åŒå kernel çš„ total_time_ms/count ç´¯åŠ , avg_time_ms é‡æ–°è®¡ç®—, ä¿ç•™ max_time_ms æœ€å¤§å€¼;
 5. ncu_detailed_analysis: æŒ‰é”®åˆå¹¶, æ•°å€¼å­—æ®µæ±‚å¹³å‡, ç“¶é¢ˆåˆ—è¡¨å»é‡;
 6. focus_analysis: åŒä¸Š;

CLI:
  python multi_node_aggregator.py --output merged_report_dir dir1 dir2 dir3 --ingest --kb-path knowledge_store 

ç¯å¢ƒå˜é‡:
  DEFAULT_KB_PATH   è¦†ç›– --kb-path é»˜è®¤å€¼

"""
from __future__ import annotations
import json, os, sys
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime

try:
    from backend.knowledge_bases.kb_ingest import ingest_json_to_faiss
except Exception:
    ingest_json_to_faiss = None  # type: ignore

def _safe_load_json(p: Path) -> Any:
    try:
        return json.loads(p.read_text(encoding='utf-8'))
    except Exception:
        return None

def _merge_kernel_analysis(items: List[Dict[str, Any]]) -> Dict[str, Any]:
    if not items:
        return {}
    total_kernels = sum(i.get('total_kernels', 0) for i in items)
    total_kernel_time = sum(i.get('total_kernel_time', 0.0) for i in items)
    # unique_kernels åˆå¹¶: éœ€è¦æ¯ä¸ªæ¡ç›®å¯èƒ½æ²¡æœ‰åˆ—è¡¨, ä¿ç•™æœ€å¤§/æˆ–æ±‚å’Œ (ç²—ç•¥)
    unique_sets = []
    for i in items:
        # æ— æ³•ç›´æ¥æ‹¿åˆ—è¡¨, comprehensive_analysis é‡Œæ²¡æœ‰å®é™…åç§°é›†åˆ; ç”¨ unique_kernels å­—æ®µè¿‘ä¼¼ä¼°è®¡
        count = i.get('unique_kernels')
        if isinstance(count, int):
            unique_sets.append(count)
    # é€‰æ‹©æœ€å¤§ä½œä¸ºæ•´ä½“ unique è¿‘ä¼¼ (é¿å…é‡å¤åŠ æ€»è¿‡å¤§)
    merged_unique = max(unique_sets) if unique_sets else 0
    avg_kernel_time = (total_kernel_time / total_kernels) if total_kernels else 0.0
    # top_kernels/kernel_distribution å­—æ®µåœ¨ä¸åŒèŠ‚ç‚¹æ„ä¹‰æ··åˆ, æ­¤å¤„ä¸æ‹¼æ¥åŸé•¿å­—ç¬¦ä¸², ä»…ä¿ç•™ç¬¬ä¸€ä¸ª
    first = items[0]
    return {
        'total_kernels': total_kernels,
        'unique_kernels': merged_unique,
        'total_kernel_time': total_kernel_time,
        'avg_kernel_time': avg_kernel_time,
    }

def _merge_memory(items: List[Dict[str, Any]]) -> Dict[str, Any]:
    if not items:
        return {}
    total_transfers = sum(i.get('total_transfers', 0) for i in items)
    total_data_mb = sum(i.get('total_data_mb', 0.0) for i in items)
    # å¹³å‡å¸¦å®½: ç®€å•åŠ æƒå¹³å‡ (æŒ‰ data_mb)
    bw_parts = []
    for i in items:
        bw = i.get('avg_bandwidth'); data = i.get('total_data_mb', 0.0)
        if isinstance(bw, (int, float)) and data > 0:
            bw_parts.append((bw, data))
    weighted_bw = sum(bw*d for bw, d in bw_parts)/sum(d for _, d in bw_parts) if bw_parts else 0.0
    return {
        'total_transfers': total_transfers,
        'total_data_mb': total_data_mb,
        'avg_bandwidth': weighted_bw,
    }

def _merge_timeline(items: List[Dict[str, Any]]) -> Dict[str, Any]:
    if not items:
        return {}
    first_event = min((i.get('first_event_time') for i in items if isinstance(i.get('first_event_time'), (int, float))), default=None)
    last_event = max((i.get('last_event_time') for i in items if isinstance(i.get('last_event_time'), (int, float))), default=None)
    total_events = sum(i.get('total_events', 0) for i in items)
    span = (last_event - first_event) if (first_event is not None and last_event is not None) else None
    return {
        'total_events': total_events,
        'execution_span': span,
        'first_event_time': first_event,
        'last_event_time': last_event,
    }

def _merge_hot_kernels(lists: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    merged: Dict[str, Dict[str, Any]] = {}
    for lst in lists:
        for k in lst:
            name = str(k.get('name'))
            if name not in merged:
                merged[name] = {
                    'name': name,
                    'total_time_ms': k.get('total_time_ms', 0.0),
                    'count': k.get('count', 0),
                    'max_time_ms': k.get('max_time_ms', k.get('avg_time_ms', 0.0)),
                }
            else:
                merged[name]['total_time_ms'] += k.get('total_time_ms', 0.0)
                merged[name]['count'] += k.get('count', 0)
                merged[name]['max_time_ms'] = max(merged[name]['max_time_ms'], k.get('max_time_ms', k.get('avg_time_ms', 0.0)))
    # è®¡ç®— avg_time_ms
    for v in merged.values():
        v['avg_time_ms'] = v['total_time_ms'] / v['count'] if v['count'] else 0.0
    # æ’åºæŒ‰ total_time_ms
    return sorted(merged.values(), key=lambda x: x['total_time_ms'], reverse=True)

def _merge_ncu_detailed(items: List[Dict[str, Any]]) -> Dict[str, Any]:
    # items æ˜¯å¤šä¸ª comprehensive_results['ncu_detailed_analysis'] dict
    merged: Dict[str, Dict[str, Any]] = {}
    for detail in items:
        for kname, data in detail.items():
            tgt = merged.setdefault(kname, {
                'kernels_analyzed': 0,
                'bottlenecks_found': 0,
                'gpu_utilization': {},
                'memory_analysis': {},
                'bottleneck_summary': []
            })
            tgt['kernels_analyzed'] += data.get('kernels_analyzed', 0)
            tgt['bottlenecks_found'] += data.get('bottlenecks_found', 0)
            # åˆå¹¶ gpu_utilization æ•°å€¼å¹³å‡
            gu = data.get('gpu_utilization', {})
            for key, val in gu.items():
                if isinstance(val, (int, float)):
                    lst = tgt['gpu_utilization'].setdefault(key, [])
                    lst.append(val)
            mem = data.get('memory_analysis', {})
            for key, val in mem.items():
                if isinstance(val, dict):
                    # åªæ”¯æŒä¸€å±‚ metrics dict flatten
                    for mkey, mval in val.items():
                        if isinstance(mval, (int, float)):
                            lst = tgt['memory_analysis'].setdefault(mkey, [])
                            lst.append(mval)
            # ç“¶é¢ˆå»é‡æŒ‰ description
            for b in data.get('bottleneck_summary', []):
                if b not in tgt['bottleneck_summary']:
                    tgt['bottleneck_summary'].append(b)
    # å¹³å‡åŒ–æ•°å€¼åˆ—è¡¨
    for kname, data in merged.items():
        for key, lst in list(data['gpu_utilization'].items()):
            if isinstance(lst, list) and lst:
                data['gpu_utilization'][key] = sum(lst)/len(lst)
        for key, lst in list(data['memory_analysis'].items()):
            if isinstance(lst, list) and lst:
                data['memory_analysis'][key] = sum(lst)/len(lst)
    return merged

def _merge_focus(items: List[Dict[str, Any]]) -> Dict[str, Any]:
    # focus metrics structure similar to detailed but simpler
    merged: Dict[str, Dict[str, Any]] = {}
    for focus in items:
        for kname, data in focus.items():
            tgt = merged.setdefault(kname, {
                'kernels_analyzed': 0,
                'gpu_utilization': {},
                'memory_analysis': {},
                'bottleneck_summary': []
            })
            tgt['kernels_analyzed'] += data.get('kernels_analyzed', 0)
            gu = data.get('gpu_utilization', {})
            for key, val in gu.items():
                if isinstance(val, (int, float)):
                    lst = tgt['gpu_utilization'].setdefault(key, [])
                    lst.append(val)
            mem_bw = data.get('memory_analysis', {}).get('bandwidth_stats', {})
            for key, val in mem_bw.items():
                if isinstance(val, (int, float)):
                    lst = tgt['memory_analysis'].setdefault(key, [])
                    lst.append(val)
            for b in data.get('bottleneck_summary', []):
                if b not in tgt['bottleneck_summary']:
                    tgt['bottleneck_summary'].append(b)
    # average
    for kname, data in merged.items():
        for key, lst in list(data['gpu_utilization'].items()):
            data['gpu_utilization'][key] = sum(lst)/len(lst) if lst else None
        for key, lst in list(data['memory_analysis'].items()):
            data['memory_analysis'][key] = sum(lst)/len(lst) if lst else None
    return merged

def merge_analysis_dirs(dirs: List[str]) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
    comps = []
    hot_lists = []
    for d in dirs:
        p = Path(d)
        comp = _safe_load_json(p / 'comprehensive_analysis.json')
        if comp:
            comps.append(comp)
        hot = _safe_load_json(p / 'hot_kernels.json')
        if isinstance(hot, list):
            hot_lists.append(hot)
    if not comps:
        raise RuntimeError('æœªæ‰¾åˆ°ä»»ä½• comprehensive_analysis.json æ–‡ä»¶')
    # Merge overview sections
    kernel_items = [c.get('nsys_overview', {}).get('kernel_analysis', {}) for c in comps]
    mem_items = [c.get('nsys_overview', {}).get('memory_analysis', {}) for c in comps]
    time_items = [c.get('nsys_overview', {}).get('timeline_analysis', {}) for c in comps]
    merged_hot = _merge_hot_kernels(hot_lists)
    merged_detail = _merge_ncu_detailed([c.get('ncu_detailed_analysis', {}) for c in comps])
    merged_focus = _merge_focus([c.get('ncu_focus_analysis', {}) for c in comps])

    merged = {
        'timestamp': datetime.utcnow().isoformat(),
        'nodes_count': len(comps),
        'source_dirs': dirs,
        'nsys_overview': {
            'kernel_analysis': _merge_kernel_analysis(kernel_items),
            'memory_analysis': _merge_memory(mem_items),
            'timeline_analysis': _merge_timeline(time_items),
        },
        'hot_kernels_count': len(merged_hot),
        'hot_kernels_merged': merged_hot[:50],  # limit for readability
        'ncu_detailed_analysis': merged_detail,
        'ncu_focus_analysis': merged_focus
    }
    return merged, merged_hot

def write_reports(merged: Dict[str, Any], all_hot: List[Dict[str, Any]], out_dir: Path) -> Path:
    out_dir.mkdir(parents=True, exist_ok=True)
    json_path = out_dir / 'multi_node_comprehensive_analysis.json'
    json_path.write_text(json.dumps(merged, ensure_ascii=False, indent=2), encoding='utf-8')
    md_path = out_dir / 'multi_node_integrated_report.md'
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write('# å¤šèŠ‚ç‚¹é›†æˆæ€§èƒ½åˆ†ææŠ¥å‘Š\n\n')
        f.write(f'- ç”Ÿæˆæ—¶é—´: {datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")}\n')
        f.write(f'- èŠ‚ç‚¹æ•°é‡: {merged.get("nodes_count")}\n')
        f.write('## æ±‡æ€» Kernel æ¦‚è§ˆ\n')
        ka = merged['nsys_overview']['kernel_analysis']
        f.write(f'- æ€» kernels æ•°: {ka.get("total_kernels",0)}\n')
        f.write(f'- æ€»æ‰§è¡Œæ—¶é—´(ms): {ka.get("total_kernel_time",0):.2f}\n')
        f.write(f'- å¹³å‡å• kernel æ—¶é—´(ms): {ka.get("avg_kernel_time",0):.4f}\n')
        f.write('## åˆå¹¶çƒ­ç‚¹ Kernels (å‰20)\n')
        for hk in all_hot[:20]:
            f.write(f'- {hk["name"]}: total={hk["total_time_ms"]:.2f}ms avg={hk["avg_time_ms"]:.3f}ms count={hk["count"]}\n')
        f.write('\n## ç„¦ç‚¹ NCU èšåˆ (è‹¥å­˜åœ¨)\n')
        for kname, data in (merged.get('ncu_focus_analysis', {}) or {}).items():
            gu = data.get('gpu_utilization', {})
            mem = data.get('memory_analysis', {})
            f.write(f'### {kname}\n')
            if gu:
                f.write(f'- å¹³å‡SMæ•ˆç‡: {gu.get("average_sm_efficiency","N/A")}\n')
                f.write(f'- Occupancy: {gu.get("achieved_occupancy","N/A")}\n')
            if mem:
                f.write(f'- å¹³å‡å¸¦å®½: {mem.get("average_bandwidth","N/A")}\n')
            bsum = data.get('bottleneck_summary', [])
            if bsum:
                f.write('- ç“¶é¢ˆ: ' + ', '.join(b.get('description','') for b in bsum) + '\n')
        f.write('\n## ä¼˜åŒ–å»ºè®®æ¦‚è¦\n')
        f.write('- ä¼˜å…ˆå…³æ³¨ç´¯è®¡æ—¶é—´æœ€é«˜çš„å‰10ä¸ªåˆå¹¶çƒ­ç‚¹ kernel\n')
        f.write('- å¯¹ SM æ•ˆç‡åä½çš„ç„¦ç‚¹å†…æ ¸è¿›è¡Œç®—å­èåˆæˆ–è®¿å­˜ä¼˜åŒ–\n')
        f.write('- é’ˆå¯¹å¹³å‡å¸¦å®½ä½ä¸”å ç”¨æ—¶é—´é•¿çš„å†…æ ¸åˆ†æè®¿å­˜æ¨¡å¼ (ä¾‹å¦‚ coalescing / cache åˆ©ç”¨)\n')
    return md_path

def main(argv: List[str]):
    import argparse
    ap = argparse.ArgumentParser(description='å¤šèŠ‚ç‚¹ç»¼åˆåˆ†æç»“æœåˆå¹¶å·¥å…·')
    ap.add_argument('dirs', nargs='+', help='å„èŠ‚ç‚¹åˆ†æè¾“å‡ºç›®å½• (åŒ…å« comprehensive_analysis.json)')
    ap.add_argument('--output', default='merged_multi_node', help='åˆå¹¶ç»“æœè¾“å‡ºç›®å½•')
    ap.add_argument('--ingest', action='store_true', help='å°†åˆå¹¶ JSON å†™å…¥çŸ¥è¯†åº“å‘é‡ç´¢å¼•')
    ap.add_argument('--kb-path', default=os.getenv('DEFAULT_KB_PATH','knowledge_store'), help='çŸ¥è¯†åº“ç›®å½•')
    args = ap.parse_args(argv)
    merged, all_hot = merge_analysis_dirs(args.dirs)
    out_dir = Path(args.output)
    md_path = write_reports(merged, all_hot, out_dir)
    print(f'âœ… åˆå¹¶æŠ¥å‘Šç”Ÿæˆ: {md_path}')
    if args.ingest:
        if ingest_json_to_faiss is None:
            print('âš ï¸ æœªåŠ è½½çŸ¥è¯†åº“æ‘„å–æ¨¡å—ï¼Œè·³è¿‡ ingest')
        else:
            try:
                ingest_json_to_faiss(json.dumps(merged, ensure_ascii=False), index_dir=Path(args.kb_path))
                print(f'ğŸ“¥ å·²æ‘„å–åˆå¹¶æŠ¥å‘Š JSON åˆ°çŸ¥è¯†åº“: {args.kb_path}')
            except Exception as e:
                print(f'âš ï¸ æ‘„å–å¤±è´¥: {e}')

if __name__ == '__main__':
    main(sys.argv[1:])
