#!/usr/bin/env python3
"""
NVIDIA Nsight Compute (ncu) è¾“å‡ºæ–‡ä»¶è‡ªåŠ¨åŒ–è§£æå·¥å…·

æ”¯æŒè§£æå¤šç§ ncu è¾“å‡ºæ ¼å¼ï¼š
- NCU Report æ–‡ä»¶ (.ncu-rep)
- CSV å¯¼å‡ºæ–‡ä»¶
- JSON å¯¼å‡ºæ–‡ä»¶
- è‡ªåŠ¨è°ƒç”¨ ncu å¯¼å‡ºå·¥å…·

ä¸“æ³¨äºCUDA kernelçº§åˆ«çš„è¯¦ç»†æ€§èƒ½åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- GPU åˆ©ç”¨ç‡æŒ‡æ ‡
- å†…å­˜å¸¦å®½åˆ†æ  
- Warp æ‰§è¡Œæ•ˆç‡
- æŒ‡ä»¤ååé‡
- å ç”¨ç‡åˆ†æ

ä½œè€…: AIåŠ©æ‰‹
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import json
import csv
from io import StringIO
import subprocess
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple, Any
from dataclasses import dataclass
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

INTEGRATED_MD = Path("/workspace/Agent/AI_Agent_Complete/sglang_analysis_b8_i512_o64/integrated_performance_report.md")

@dataclass
class KernelMetrics:
    """CUDA Kernel æ€§èƒ½æŒ‡æ ‡"""
    name: str
    grid_size: Optional[Tuple[int, int, int]] = None
    block_size: Optional[Tuple[int, int, int]] = None
    
    # GPU åˆ©ç”¨ç‡æŒ‡æ ‡
    sm_efficiency: Optional[float] = None  # SMæ•ˆç‡ (%)
    achieved_occupancy: Optional[float] = None  # å®ç°å ç”¨ç‡ (%)
    theoretical_occupancy: Optional[float] = None  # ç†è®ºå ç”¨ç‡ (%)
    
    # å†…å­˜æ€§èƒ½æŒ‡æ ‡
    dram_bandwidth: Optional[float] = None  # DRAMå¸¦å®½ (GB/s)
    l2_hit_rate: Optional[float] = None  # L2ç¼“å­˜å‘½ä¸­ç‡ (%)
    l1_hit_rate: Optional[float] = None  # L1ç¼“å­˜å‘½ä¸­ç‡ (%)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    tensor_active: Optional[float] = None  # Tensor Coreæ´»è·ƒåº¦ (%)
    fp32_pipe_utilization: Optional[float] = None  # FP32ç®¡é“åˆ©ç”¨ç‡ (%)
    fp16_pipe_utilization: Optional[float] = None  # FP16ç®¡é“åˆ©ç”¨ç‡ (%)
    int_pipe_utilization: Optional[float] = None  # INTç®¡é“åˆ©ç”¨ç‡ (%)
    
    # Warp æ‰§è¡ŒæŒ‡æ ‡
    warp_execution_efficiency: Optional[float] = None  # Warpæ‰§è¡Œæ•ˆç‡ (%)
    warp_stall_long_scoreboard: Optional[float] = None  # é•¿è®°åˆ†æ¿åœé¡¿ (%)
    warp_stall_memory_throttle: Optional[float] = None  # å†…å­˜é™æµåœé¡¿ (%)
    warp_stall_memory_dependency: Optional[float] = None  # å†…å­˜ä¾èµ–åœé¡¿ (%)
    
    # å…¶ä»–æŒ‡æ ‡
    duration: Optional[float] = None  # æ‰§è¡Œæ—¶é—´ (ms)
    registers_per_thread: Optional[int] = None
    shared_memory_per_block: Optional[int] = None
    
@dataclass 
class BottleneckInfo:
    """æ€§èƒ½ç“¶é¢ˆä¿¡æ¯"""
    type: str  # ç“¶é¢ˆç±»å‹: memory, compute, latency
    severity: str  # ä¸¥é‡ç¨‹åº¦: low, medium, high, critical
    description: str
    metrics: Dict[str, float]
    recommendations: List[str]

class NCUParser:
    """NCU è¾“å‡ºæ–‡ä»¶è§£æå™¨

    ç»Ÿä¸€è¾“å‡ºè·¯å¾„ç­–ç•¥:
        æ‰€æœ‰å¯¼å‡ºçš„ CSV / JSON å‡å†™åˆ° /workspace/Agent/AI_Agent_Complete ä¸‹ï¼Œ
        ä¾¿äº Agent èšåˆè¯»å–ã€‚
    """
    DEFAULT_BASE_DIR = Path("/workspace/Agent/AI_Agent_Complete")

    def __init__(self, input_file: str):
        self.input_file = Path(input_file)
        # æ ‡å‡†åŒ–ä¸ºç»å¯¹è·¯å¾„ä¸‹çš„æ–‡ä»¶ï¼ˆå¦‚æœç»™çš„æ˜¯ç›¸å¯¹è·¯å¾„ï¼‰
        if not self.input_file.is_absolute():
            self.input_file = Path.cwd() / self.input_file
        self.kernels: List[KernelMetrics] = []
        self.raw_data: Dict = {}
        self.metadata: Dict = {}
        
        if not self.input_file.exists():
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
    
    def parse(self) -> None:
        """è§£æè¾“å…¥æ–‡ä»¶"""
        suffix = self.input_file.suffix.lower()
        
        if suffix == '.ncu-rep':
            self._parse_ncu_rep()
        elif suffix == '.csv':
            self._parse_csv()
        elif suffix == '.json':
            self._parse_json()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
    
    def _parse_ncu_rep(self) -> None:
        """è§£æ .ncu-rep æ–‡ä»¶ï¼ˆéœ€è¦å…ˆå¯¼å‡ºä¸ºCSVï¼‰"""
        print("ğŸ“‹ æ£€æµ‹åˆ° .ncu-rep æ–‡ä»¶ï¼Œæ­£åœ¨å¯¼å‡ºä¸ºCSVæ ¼å¼...")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        # å¼ºåˆ¶è¾“å‡ºåˆ°ç»Ÿä¸€ç›®å½•
        csv_file = self.DEFAULT_BASE_DIR / (self.input_file.stem + '.csv')
        
        # è°ƒç”¨ ncu å¯¼å‡ºå‘½ä»¤
        cmd = [
            'ncu', '--csv',
            '--log-file', str(csv_file),
            '--import', str(self.input_file)
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(f"âœ… å¯¼å‡ºæˆåŠŸ: {csv_file}")
            
            # è§£æå¯¼å‡ºçš„CSVæ–‡ä»¶
            self._parse_csv(csv_file)
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ ncuå¯¼å‡ºå¤±è´¥: {e.stderr}")
            print("å°è¯•ä½¿ç”¨æ›¿ä»£å¯¼å‡ºæ–¹å¼...")
            # å°è¯•JSONæ ¼å¼å¯¼å‡º
            self._parse_ncu_rep_json()
            
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ° ncu å‘½ä»¤")
            print("è¯·å®‰è£… NVIDIA Nsight Compute å¹¶ç¡®ä¿ ncu åœ¨PATHä¸­")
            raise
    
    def _parse_ncu_rep_json(self) -> None:
        """è§£æ .ncu-rep æ–‡ä»¶å¯¼å‡ºä¸ºJSON"""
        json_file = self.DEFAULT_BASE_DIR / (self.input_file.stem + '.json')
        
        cmd = [
            'ncu', '--json',
            '--log-file', str(json_file), 
            '--import', str(self.input_file)
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(f"âœ… JSONå¯¼å‡ºæˆåŠŸ: {json_file}")
            self._parse_json(json_file)
        except subprocess.CalledProcessError as e:
            print(f"âŒ JSONå¯¼å‡ºä¹Ÿå¤±è´¥: {e.stderr}")
            raise
    
    def _parse_csv(self, csv_file: Optional[Path] = None) -> None:
        """è§£æ CSV æ–‡ä»¶"""
        target_file = csv_file or self.input_file
        print(f"ğŸ“Š æ­£åœ¨è§£æCSVæ–‡ä»¶: {target_file}")
        path_obj = Path(target_file)
        if not path_obj.exists():
            print("âš ï¸ CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return
        if path_obj.stat().st_size == 0:
            print("âš ï¸ CSVä¸ºç©ºï¼Œå°è¯• JSON å›é€€...")
            self._fallback_to_json()
            return
        try:
            # é¢„æ¸…æ´—ï¼šè¿‡æ»¤æ³¨é‡Šå’Œç©ºè¡Œ
            with open(path_obj, 'r', encoding='utf-8', errors='ignore') as f:
                lines = [l for l in f.readlines() if l.strip() and not l.startswith('#')]
            if not lines:
                print("âš ï¸ æœ‰æ•ˆå†…å®¹ä¸ºç©ºï¼Œå°è¯• JSON å›é€€...")
                self._fallback_to_json()
                return
            temp_clean = path_obj.parent / (path_obj.stem + '.csv')
            temp_clean.write_text('\n'.join(lines), encoding='utf-8')
            df = pd.read_csv(temp_clean)
            # cleaned_csv_text = '\n'.join(lines)
            # df = pd.read_csv(StringIO(cleaned_csv_text))
            print(f"ğŸ” å‘ç° {len(df)} è¡Œæ•°æ®ï¼Œåˆ—: {list(df.columns)}")
            if df.empty:
                print("âš ï¸ DataFrameä¸ºç©ºï¼Œå°è¯• JSON å›é€€...")
                self._fallback_to_json()
                return
            if 'Kernel Name' in df.columns or 'KernelName' in df.columns:
                self._parse_csv_kernels(df)
            else:
                self._parse_csv_generic(df)
        except Exception as e:
            print(f"âŒ CSVè§£æå¤±è´¥: {e} -> å°è¯• JSON å›é€€")
            self._fallback_to_json()

    def _fallback_to_json(self) -> None:
        """å½“ CSV è§£æå¤±è´¥æˆ–ä¸ºç©ºæ—¶å›é€€è‡³ JSON å¯¼å‡ºè§£æ"""
        try:
            self._parse_ncu_rep_json()
        except Exception as e:
            print(f"âš ï¸ JSON å›é€€ä¹Ÿå¤±è´¥: {e}")
    
    def _parse_csv_kernels(self, df: pd.DataFrame) -> None:
        """è§£æåŒ…å«kernelä¿¡æ¯çš„CSV"""
        kernel_name_col = 'Kernel Name' if 'Kernel Name' in df.columns else 'KernelName'

        # æ–°å¢ï¼šæ”¯æŒâ€œé•¿è¡¨â€ç»“æ„ï¼ˆæ¯è¡Œ=ä¸€ä¸ªæŒ‡æ ‡ï¼‰
        if {'Section Name', 'Metric Name', 'Metric Value'}.issubset(df.columns):
            # è§„èŒƒåŒ– Metric Value ä¸ºæ•°å€¼
            df['Metric Value'] = df['Metric Value'].astype(str).str.replace(',', '', regex=False)
            df['Metric Value'] = pd.to_numeric(df['Metric Value'], errors='coerce')

            for kname, kdf in df.groupby(kernel_name_col):
                metrics = KernelMetrics(name=str(kname))

                def get_metric(section: str, name: str):
                    sel = kdf[(kdf['Section Name'] == section) & (kdf['Metric Name'] == name)]['Metric Value']
                    return None if sel.empty else float(sel.mean())

                # ç½‘æ ¼/å—
                try:
                    bsz = str(kdf['Block Size'].dropna().iloc[0])
                    gsz = str(kdf['Grid Size'].dropna().iloc[0])
                    def parse_xyz(s):
                        s = s.strip().strip('()')
                        x, y, z = [int(float(v.strip())) for v in s.split(',')]
                        return (x, y, z)
                    metrics.block_size = parse_xyz(bsz)
                    metrics.grid_size  = parse_xyz(gsz)
                except Exception:
                    pass

                # å¸¸è§æ˜ å°„
                # SM Busyï¼ˆæˆ–ç”¨ GPU SOL Throughput çš„ Compute(SM) æŒ‡æ ‡ä½œä¸ºè¿‘ä¼¼ï¼‰
                sm_busy = get_metric('Compute Workload Analysis', 'SM Busy')
                if sm_busy is None:
                    sm_busy = get_metric('GPU Speed Of Light Throughput', 'Compute (SM) Throughput')
                metrics.sm_efficiency = sm_busy

                # Occupancy
                metrics.achieved_occupancy    = get_metric('Occupancy', 'Achieved Occupancy')
                metrics.theoretical_occupancy = get_metric('Occupancy', 'Theoretical Occupancy')

                # Memory
                # ä¸åŒç‰ˆæœ¬åˆ—åå¯èƒ½ä¸º "Memory Throughput" æˆ– "DRAM Throughput"
                m_bw = get_metric('Memory Workload Analysis', 'Memory Throughput')
                if m_bw is None:
                    m_bw = get_metric('Memory Workload Analysis', 'DRAM Throughput')
                metrics.dram_bandwidth = m_bw
                metrics.l2_hit_rate    = get_metric('Memory Workload Analysis', 'L2 Hit Rate')
                # L1/TEX å‘½ä¸­ç‡ï¼ˆæœ‰çš„ç‰ˆæœ¬å« L1/TEX Hit Rateï¼‰
                l1_rate = get_metric('Memory Workload Analysis', 'L1/TEX Hit Rate')
                if l1_rate is None:
                    l1_rate = get_metric('Memory Workload Analysis', 'L1 Hit Rate')
                metrics.l1_hit_rate = l1_rate

                # Durationï¼ˆå•ä½å¯èƒ½æ˜¯ usï¼Œä¿å®ˆä¸æ¢ç®—ï¼›å¦‚éœ€ ms å¯é™¤ä»¥ 1000ï¼‰
                dur = get_metric('GPU Speed Of Light Throughput', 'Duration')
                if dur is not None:
                    metrics.duration = dur  # å¦‚éœ€ ms: dur/1000.0

                # Launch Statistics
                regs = get_metric('Launch Statistics', 'Registers Per Thread')
                if regs is not None:
                    metrics.registers_per_thread = int(regs)

                shm_dyn_kb = get_metric('Launch Statistics', 'Dynamic Shared Memory Per Block')
                shm_sta_b  = get_metric('Launch Statistics', 'Static Shared Memory Per Block')
                if shm_dyn_kb is not None or shm_sta_b is not None:
                    dyn_b = (shm_dyn_kb or 0) * 1024.0
                    sta_b = (shm_sta_b or 0)
                    metrics.shared_memory_per_block = int(dyn_b + sta_b)

                self.kernels.append(metrics)

            print(f"ğŸ”¥ è§£æåˆ° {len(self.kernels)} ä¸ªkernelï¼ˆé•¿è¡¨ï¼‰")
            return

        # åŸæœ‰â€œå®½è¡¨â€è§£æï¼ˆä¿ç•™ï¼‰
        for kernel_name in df[kernel_name_col].unique():
            kernel_data = df[df[kernel_name_col] == kernel_name].iloc[0]
            metrics = KernelMetrics(name=kernel_name)
            column_mapping = {
                'SM Efficiency': 'sm_efficiency',
                'Achieved Occupancy': 'achieved_occupancy',
                'Theoretical Occupancy': 'theoretical_occupancy',
                'DRAM Bandwidth': 'dram_bandwidth',
                'L2 Hit Rate': 'l2_hit_rate',
                'L1 Hit Rate': 'l1_hit_rate',
                'Tensor Active': 'tensor_active',
                'FP32 Pipeline Utilization': 'fp32_pipe_utilization',
                'Warp Execution Efficiency': 'warp_execution_efficiency',
                'Duration': 'duration',
                'Registers Per Thread': 'registers_per_thread',
                'Grid Size': 'grid_size',
                'Block Size': 'block_size'
            }
            for col_name, attr_name in column_mapping.items():
                if col_name in df.columns:
                    value = kernel_data[col_name]
                    if pd.notna(value):
                        setattr(metrics, attr_name, value)
            self.kernels.append(metrics)
        print(f"ğŸ”¥ è§£æåˆ° {len(self.kernels)} ä¸ªkernels")
    
    def _parse_csv_generic(self, df: pd.DataFrame) -> None:
        """è§£æé€šç”¨CSVæ ¼å¼"""
        # å‡è®¾æ¯è¡Œæ˜¯ä¸€ä¸ªkernelçš„æ•°æ®
        for _, row in df.iterrows():
            metrics = KernelMetrics(name=f"Kernel_{len(self.kernels)}")
            
            # å°è¯•ä»åˆ—åæ¨æ–­æŒ‡æ ‡
            for col_name, value in row.items():
                if pd.isna(value):
                    continue
                    
                col_lower = col_name.lower()
                if 'sm' in col_lower and 'efficiency' in col_lower:
                    metrics.sm_efficiency = float(value)
                elif 'occupancy' in col_lower:
                    if 'achieved' in col_lower:
                        metrics.achieved_occupancy = float(value)
                    elif 'theoretical' in col_lower:
                        metrics.theoretical_occupancy = float(value)
                elif 'bandwidth' in col_lower:
                    metrics.dram_bandwidth = float(value)
                elif 'duration' in col_lower:
                    metrics.duration = float(value)
            
            self.kernels.append(metrics)
        
        print(f"ğŸ”¥ è§£æåˆ° {len(self.kernels)} ä¸ªkernels")
    
    def _parse_json(self, json_file: Optional[Path] = None) -> None:
        """è§£æ JSON æ–‡ä»¶"""
        target_file = json_file or self.input_file
        print(f"ğŸ“‹ æ­£åœ¨è§£æJSONæ–‡ä»¶: {target_file}")
        
        with open(target_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.raw_data = data
        
        # æ ¹æ®JSONç»“æ„è§£æ
        if isinstance(data, list):
            self._parse_json_list(data)
        elif isinstance(data, dict):
            self._parse_json_dict(data)
    
    def _parse_json_list(self, data: List) -> None:
        """è§£æJSONåˆ—è¡¨æ ¼å¼"""
        for item in data:
            if isinstance(item, dict):
                metrics = self._extract_metrics_from_dict(item)
                if metrics:
                    self.kernels.append(metrics)
    
    def _parse_json_dict(self, data: Dict) -> None:
        """è§£æJSONå­—å…¸æ ¼å¼"""
        if 'kernels' in data:
            self._parse_json_list(data['kernels'])
        elif 'reports' in data:
            for report in data['reports']:
                if 'kernels' in report:
                    self._parse_json_list(report['kernels'])
    
    def _extract_metrics_from_dict(self, data: Dict) -> Optional[KernelMetrics]:
        """ä»å­—å…¸ä¸­æå–æ€§èƒ½æŒ‡æ ‡"""
        if 'name' not in data and 'kernel' not in data:
            return None
        
        name = data.get('name', data.get('kernel', f"Kernel_{len(self.kernels)}"))
        metrics = KernelMetrics(name=name)
        
        # æ˜ å°„JSONå­—æ®µåˆ°æŒ‡æ ‡
        field_mapping = {
            'smEfficiency': 'sm_efficiency',
            'achievedOccupancy': 'achieved_occupancy',
            'theoreticalOccupancy': 'theoretical_occupancy', 
            'dramBandwidth': 'dram_bandwidth',
            'l2HitRate': 'l2_hit_rate',
            'l1HitRate': 'l1_hit_rate',
            'tensorActive': 'tensor_active',
            'warpExecutionEfficiency': 'warp_execution_efficiency',
            'duration': 'duration',
            'registersPerThread': 'registers_per_thread'
        }
        
        for json_field, attr_name in field_mapping.items():
            if json_field in data:
                setattr(metrics, attr_name, data[json_field])
        
        return metrics

class NCUAnalyzer:
    """NCU æ•°æ®åˆ†æå™¨"""
    
    def __init__(self, parser: NCUParser):
        self.parser = parser
        self.stats = {}
        self.bottlenecks: List[BottleneckInfo] = []
    
    def analyze(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ” å¼€å§‹NCUæ€§èƒ½åˆ†æ...")
        
        self.stats = {
            'gpu_utilization': self._analyze_gpu_utilization(),
            'memory_analysis': self._analyze_memory_performance(), 
            'compute_analysis': self._analyze_compute_performance(),
            'warp_analysis': self._analyze_warp_efficiency(),
            'occupancy_analysis': self._analyze_occupancy(),
            'bottleneck_analysis': self._identify_bottlenecks()
        }
        
        return self.stats
    
    def _analyze_gpu_utilization(self) -> Dict:
        """åˆ†æGPUåˆ©ç”¨ç‡"""
        if not self.parser.kernels:
            return {}
        
        sm_efficiencies = [k.sm_efficiency for k in self.parser.kernels if k.sm_efficiency is not None]
        
        if not sm_efficiencies:
            return {'message': 'æ— SMæ•ˆç‡æ•°æ®'}
        
        return {
            'average_sm_efficiency': sum(sm_efficiencies) / len(sm_efficiencies),
            'max_sm_efficiency': max(sm_efficiencies),
            'min_sm_efficiency': min(sm_efficiencies),
            'kernels_below_50_percent': len([x for x in sm_efficiencies if x < 50]),
            'total_kernels': len(sm_efficiencies)
        }
    
    def _analyze_memory_performance(self) -> Dict:
        """åˆ†æå†…å­˜æ€§èƒ½"""
        bandwidth_data = [k.dram_bandwidth for k in self.parser.kernels if k.dram_bandwidth is not None]
        l2_hit_rates = [k.l2_hit_rate for k in self.parser.kernels if k.l2_hit_rate is not None]
        l1_hit_rates = [k.l1_hit_rate for k in self.parser.kernels if k.l1_hit_rate is not None]
        
        analysis = {}
        
        if bandwidth_data:
            analysis['bandwidth_stats'] = {
                'average_bandwidth': sum(bandwidth_data) / len(bandwidth_data),
                'max_bandwidth': max(bandwidth_data),
                'min_bandwidth': min(bandwidth_data)
            }
        
        if l2_hit_rates:
            analysis['l2_cache_stats'] = {
                'average_l2_hit_rate': sum(l2_hit_rates) / len(l2_hit_rates),
                'kernels_low_l2_hit_rate': len([x for x in l2_hit_rates if x < 50])
            }
        
        if l1_hit_rates:
            analysis['l1_cache_stats'] = {
                'average_l1_hit_rate': sum(l1_hit_rates) / len(l1_hit_rates),
                'kernels_low_l1_hit_rate': len([x for x in l1_hit_rates if x < 50])
            }
        
        return analysis
    
    def _analyze_compute_performance(self) -> Dict:
        """åˆ†æè®¡ç®—æ€§èƒ½"""
        tensor_active = [k.tensor_active for k in self.parser.kernels if k.tensor_active is not None]
        fp32_util = [k.fp32_pipe_utilization for k in self.parser.kernels if k.fp32_pipe_utilization is not None]
        
        analysis = {}
        
        if tensor_active:
            analysis['tensor_core_usage'] = {
                'average_tensor_active': sum(tensor_active) / len(tensor_active),
                'kernels_using_tensor': len([x for x in tensor_active if x > 0])
            }
        
        if fp32_util:
            analysis['fp32_pipeline'] = {
                'average_fp32_utilization': sum(fp32_util) / len(fp32_util),
                'max_fp32_utilization': max(fp32_util)
            }
        
        return analysis
    
    def _analyze_warp_efficiency(self) -> Dict:
        """åˆ†æWarpæ‰§è¡Œæ•ˆç‡"""
        warp_eff = [k.warp_execution_efficiency for k in self.parser.kernels if k.warp_execution_efficiency is not None]
        
        if not warp_eff:
            return {'message': 'æ— Warpæ•ˆç‡æ•°æ®'}
        
        return {
            'average_warp_efficiency': sum(warp_eff) / len(warp_eff),
            'min_warp_efficiency': min(warp_eff),
            'kernels_low_warp_efficiency': len([x for x in warp_eff if x < 70])
        }
    
    def _analyze_occupancy(self) -> Dict:
        """åˆ†æå ç”¨ç‡"""
        achieved_occ = [k.achieved_occupancy for k in self.parser.kernels if k.achieved_occupancy is not None]
        theoretical_occ = [k.theoretical_occupancy for k in self.parser.kernels if k.theoretical_occupancy is not None]
        
        analysis = {}
        
        if achieved_occ:
            analysis['achieved_occupancy'] = {
                'average': sum(achieved_occ) / len(achieved_occ),
                'min': min(achieved_occ),
                'max': max(achieved_occ)
            }
        
        if theoretical_occ:
            analysis['theoretical_occupancy'] = {
                'average': sum(theoretical_occ) / len(theoretical_occ),
                'min': min(theoretical_occ),
                'max': max(theoretical_occ)
            }
        
        if achieved_occ and theoretical_occ and len(achieved_occ) == len(theoretical_occ):
            occupancy_ratios = [a/t if t > 0 else 0 for a, t in zip(achieved_occ, theoretical_occ)]
            analysis['occupancy_efficiency'] = {
                'average_ratio': sum(occupancy_ratios) / len(occupancy_ratios),
                'kernels_low_efficiency': len([x for x in occupancy_ratios if x < 0.8])
            }
        
        return analysis
    
    def _identify_bottlenecks(self) -> Dict:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        self.bottlenecks.clear()
        
        for kernel in self.parser.kernels:
            kernel_bottlenecks = []
            
            # æ£€æŸ¥SMæ•ˆç‡
            if kernel.sm_efficiency is not None and kernel.sm_efficiency < 30:
                kernel_bottlenecks.append(BottleneckInfo(
                    type="compute",
                    severity="high" if kernel.sm_efficiency < 15 else "medium",
                    description=f"SMæ•ˆç‡è¿‡ä½ ({kernel.sm_efficiency:.1f}%)",
                    metrics={"sm_efficiency": kernel.sm_efficiency},
                    recommendations=["æ£€æŸ¥kernelç®—æ³•å¤æ‚åº¦", "è€ƒè™‘å¢åŠ å·¥ä½œè´Ÿè½½"]
                ))
            
            # æ£€æŸ¥å†…å­˜å¸¦å®½
            if kernel.dram_bandwidth is not None and kernel.dram_bandwidth < 100:
                kernel_bottlenecks.append(BottleneckInfo(
                    type="memory",
                    severity="medium",
                    description=f"å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ä½ ({kernel.dram_bandwidth:.1f} GB/s)",
                    metrics={"dram_bandwidth": kernel.dram_bandwidth},
                    recommendations=["ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼", "è€ƒè™‘åˆå¹¶è®¿é—®"]
                ))
            
            # æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
            if kernel.l2_hit_rate is not None and kernel.l2_hit_rate < 70:
                kernel_bottlenecks.append(BottleneckInfo(
                    type="memory", 
                    severity="medium",
                    description=f"L2ç¼“å­˜å‘½ä¸­ç‡ä½ ({kernel.l2_hit_rate:.1f}%)",
                    metrics={"l2_hit_rate": kernel.l2_hit_rate},
                    recommendations=["æ”¹å–„æ•°æ®å±€éƒ¨æ€§", "å‡å°‘ä¸è§„åˆ™å†…å­˜è®¿é—®"]
                ))
            
            # æ£€æŸ¥å ç”¨ç‡
            if (kernel.achieved_occupancy is not None and 
                kernel.theoretical_occupancy is not None and
                kernel.theoretical_occupancy > 0):
                
                occupancy_ratio = kernel.achieved_occupancy / kernel.theoretical_occupancy
                if occupancy_ratio < 0.7:
                    kernel_bottlenecks.append(BottleneckInfo(
                        type="latency",
                        severity="medium",
                        description=f"å ç”¨ç‡æ•ˆç‡ä½ ({occupancy_ratio*100:.1f}%)",
                        metrics={
                            "achieved_occupancy": kernel.achieved_occupancy,
                            "theoretical_occupancy": kernel.theoretical_occupancy
                        },
                        recommendations=["æ£€æŸ¥èµ„æºé™åˆ¶", "ä¼˜åŒ–å¯„å­˜å™¨ä½¿ç”¨", "ä¼˜åŒ–å…±äº«å†…å­˜ä½¿ç”¨"]
                    ))
            
            self.bottlenecks.extend(kernel_bottlenecks)
        
        # åˆ†æç“¶é¢ˆç»Ÿè®¡
        bottleneck_stats = {
            'total_bottlenecks': len(self.bottlenecks),
            'bottleneck_types': {},
            'severity_distribution': {},
            'top_issues': []
        }
        
        for bottleneck in self.bottlenecks:
            # ç»Ÿè®¡ç±»å‹
            if bottleneck.type not in bottleneck_stats['bottleneck_types']:
                bottleneck_stats['bottleneck_types'][bottleneck.type] = 0
            bottleneck_stats['bottleneck_types'][bottleneck.type] += 1
            
            # ç»Ÿè®¡ä¸¥é‡ç¨‹åº¦
            if bottleneck.severity not in bottleneck_stats['severity_distribution']:
                bottleneck_stats['severity_distribution'][bottleneck.severity] = 0
            bottleneck_stats['severity_distribution'][bottleneck.severity] += 1
        
        # è·å–ä¸»è¦é—®é¢˜
        bottleneck_stats['top_issues'] = [
            {
                'description': b.description,
                'type': b.type,
                'severity': b.severity,
                'recommendations': b.recommendations[:2]  # åªæ˜¾ç¤ºå‰2ä¸ªå»ºè®®
            }
            for b in sorted(self.bottlenecks, 
                          key=lambda x: {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}.get(x.severity, 0),
                          reverse=True)[:5]
        ]
        
        return bottleneck_stats

class NCUVisualizer:
    """NCU æ•°æ®å¯è§†åŒ–"""
    
    def __init__(self, parser: NCUParser, analyzer: NCUAnalyzer):
        self.parser = parser
        self.analyzer = analyzer
        self.output_dir = Path("ncu_analysis_output")
        self.output_dir.mkdir(exist_ok=True)
    
    def create_visualizations(self) -> None:
        """åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“Š ç”ŸæˆNCUå¯è§†åŒ–å›¾è¡¨...")
        
        if self.parser.kernels:
            self._plot_gpu_utilization()
            self._plot_memory_performance()
            self._plot_occupancy_analysis()
            self._plot_bottleneck_analysis()
            self._plot_kernel_comparison()
        
        print(f"ğŸ“ å›¾è¡¨å·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def _plot_gpu_utilization(self) -> None:
        """ç»˜åˆ¶GPUåˆ©ç”¨ç‡åˆ†æ"""
        sm_efficiencies = [(k.name, k.sm_efficiency) for k in self.parser.kernels 
                          if k.sm_efficiency is not None]
        
        if not sm_efficiencies:
            return
        
        names, efficiencies = zip(*sm_efficiencies)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # SMæ•ˆç‡æŸ±çŠ¶å›¾
        colors = ['red' if eff < 30 else 'orange' if eff < 60 else 'green' 
                 for eff in efficiencies]
        
        ax1.bar(range(len(names)), efficiencies, color=colors)
        ax1.set_xlabel('Kernel ç´¢å¼•')
        ax1.set_ylabel('SM æ•ˆç‡ (%)')
        ax1.set_title('å„Kernel SMæ•ˆç‡')
        ax1.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='50%åŸºçº¿')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ•ˆç‡åˆ†å¸ƒç›´æ–¹å›¾
        ax2.hist(efficiencies, bins=20, alpha=0.7, color='skyblue')
        ax2.set_xlabel('SM æ•ˆç‡ (%)')
        ax2.set_ylabel('Kernel æ•°é‡')
        ax2.set_title('SMæ•ˆç‡åˆ†å¸ƒ')
        ax2.axvline(x=sum(efficiencies)/len(efficiencies), color='red', 
                   linestyle='--', label=f'å¹³å‡å€¼: {sum(efficiencies)/len(efficiencies):.1f}%')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'gpu_utilization.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_memory_performance(self) -> None:
        """ç»˜åˆ¶å†…å­˜æ€§èƒ½åˆ†æ"""
        # æ”¶é›†å†…å­˜ç›¸å…³æ•°æ®
        bandwidth_data = [(k.name, k.dram_bandwidth) for k in self.parser.kernels 
                         if k.dram_bandwidth is not None]
        l2_hit_rates = [(k.name, k.l2_hit_rate) for k in self.parser.kernels 
                       if k.l2_hit_rate is not None]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # DRAMå¸¦å®½
        if bandwidth_data:
            names, bandwidths = zip(*bandwidth_data)
            axes[0, 0].bar(range(len(names)), bandwidths, color='lightcoral')
            axes[0, 0].set_title('DRAM å¸¦å®½åˆ©ç”¨ç‡')
            axes[0, 0].set_ylabel('å¸¦å®½ (GB/s)')
            axes[0, 0].grid(True, alpha=0.3)
        
        # L2å‘½ä¸­ç‡
        if l2_hit_rates:
            names, rates = zip(*l2_hit_rates)
            colors = ['red' if rate < 50 else 'orange' if rate < 80 else 'green' 
                     for rate in rates]
            axes[0, 1].bar(range(len(names)), rates, color=colors)
            axes[0, 1].set_title('L2 ç¼“å­˜å‘½ä¸­ç‡')
            axes[0, 1].set_ylabel('å‘½ä¸­ç‡ (%)')
            axes[0, 1].grid(True, alpha=0.3)
        
        # å¸¦å®½åˆ†å¸ƒ
        if bandwidth_data:
            _, bandwidths = zip(*bandwidth_data)
            axes[1, 0].hist(bandwidths, bins=15, alpha=0.7, color='lightgreen')
            axes[1, 0].set_title('å¸¦å®½åˆ†å¸ƒ')
            axes[1, 0].set_xlabel('å¸¦å®½ (GB/s)')
            axes[1, 0].set_ylabel('é¢‘æ¬¡')
            axes[1, 0].grid(True, alpha=0.3)
        
        # ç¼“å­˜å‘½ä¸­ç‡åˆ†å¸ƒ
        if l2_hit_rates:
            _, rates = zip(*l2_hit_rates)
            axes[1, 1].hist(rates, bins=15, alpha=0.7, color='lightyellow')
            axes[1, 1].set_title('L2å‘½ä¸­ç‡åˆ†å¸ƒ')
            axes[1, 1].set_xlabel('å‘½ä¸­ç‡ (%)')
            axes[1, 1].set_ylabel('é¢‘æ¬¡')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'memory_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_occupancy_analysis(self) -> None:
        """ç»˜åˆ¶å ç”¨ç‡åˆ†æ"""
        occupancy_data = []
        for k in self.parser.kernels:
            if k.achieved_occupancy is not None and k.theoretical_occupancy is not None:
                occupancy_data.append({
                    'name': k.name,
                    'achieved': k.achieved_occupancy,
                    'theoretical': k.theoretical_occupancy,
                    'ratio': k.achieved_occupancy / k.theoretical_occupancy if k.theoretical_occupancy > 0 else 0
                })
        
        if not occupancy_data:
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å ç”¨ç‡å¯¹æ¯”
        names = [d['name'][:20] + '...' if len(d['name']) > 20 else d['name'] for d in occupancy_data]
        achieved = [d['achieved'] for d in occupancy_data]
        theoretical = [d['theoretical'] for d in occupancy_data]
        
        x = range(len(names))
        width = 0.35
        
        ax1.bar([i - width/2 for i in x], achieved, width, label='å®é™…å ç”¨ç‡', color='lightblue')
        ax1.bar([i + width/2 for i in x], theoretical, width, label='ç†è®ºå ç”¨ç‡', color='lightcoral')
        
        ax1.set_xlabel('Kernel')
        ax1.set_ylabel('å ç”¨ç‡ (%)')
        ax1.set_title('å ç”¨ç‡å¯¹æ¯”')
        ax1.set_xticks(x)
        ax1.set_xticklabels(names, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å ç”¨ç‡æ•ˆç‡åˆ†å¸ƒ
        ratios = [d['ratio'] * 100 for d in occupancy_data]
        ax2.hist(ratios, bins=15, alpha=0.7, color='lightgreen')
        ax2.set_xlabel('å ç”¨ç‡æ•ˆç‡ (%)')
        ax2.set_ylabel('Kernel æ•°é‡')
        ax2.set_title('å ç”¨ç‡æ•ˆç‡åˆ†å¸ƒ')
        ax2.axvline(x=80, color='red', linestyle='--', alpha=0.7, label='80%åŸºçº¿')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'occupancy_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_bottleneck_analysis(self) -> None:
        """ç»˜åˆ¶ç“¶é¢ˆåˆ†æ"""
        if not self.analyzer.bottlenecks:
            return
        
        # ç»Ÿè®¡ç“¶é¢ˆç±»å‹
        bottleneck_types = {}
        severity_counts = {}
        
        for b in self.analyzer.bottlenecks:
            bottleneck_types[b.type] = bottleneck_types.get(b.type, 0) + 1
            severity_counts[b.severity] = severity_counts.get(b.severity, 0) + 1
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ç“¶é¢ˆç±»å‹åˆ†å¸ƒ
        types = list(bottleneck_types.keys())
        counts = list(bottleneck_types.values())
        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
        
        ax1.pie(counts, labels=types, autopct='%1.1f%%', colors=colors[:len(types)])
        ax1.set_title('æ€§èƒ½ç“¶é¢ˆç±»å‹åˆ†å¸ƒ')
        
        # ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        severities = list(severity_counts.keys())
        severity_colors = {
            'critical': '#ff4444',
            'high': '#ff8844',
            'medium': '#ffcc44', 
            'low': '#88cc88'
        }
        bar_colors = [severity_colors.get(s, '#cccccc') for s in severities]
        
        ax2.bar(severities, [severity_counts[s] for s in severities], color=bar_colors)
        ax2.set_title('ç“¶é¢ˆä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ')
        ax2.set_ylabel('æ•°é‡')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'bottleneck_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_kernel_comparison(self) -> None:
        """ç»˜åˆ¶kernelæ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾"""
        if len(self.parser.kernels) < 2:
            return
        
        # é€‰æ‹©å‰å‡ ä¸ªæœ‰å®Œæ•´æ•°æ®çš„kernel
        complete_kernels = []
        for k in self.parser.kernels:
            if (k.sm_efficiency is not None and 
                k.achieved_occupancy is not None and
                k.dram_bandwidth is not None):
                complete_kernels.append(k)
                if len(complete_kernels) >= 5:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    break
        
        if len(complete_kernels) < 2:
            return
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        metrics = ['SMæ•ˆç‡', 'å ç”¨ç‡', 'DRAMå¸¦å®½', 'L2å‘½ä¸­ç‡', 'Warpæ•ˆç‡']
        
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
        
        angles = [n / float(len(metrics)) * 2 * 3.14159 for n in range(len(metrics))]
        angles += angles[:1]
        
        colors = plt.cm.Set3(range(len(complete_kernels)))
        
        for i, kernel in enumerate(complete_kernels):
            values = [
                kernel.sm_efficiency or 0,
                kernel.achieved_occupancy or 0,
                min(kernel.dram_bandwidth or 0, 100) if kernel.dram_bandwidth else 0,  # å½’ä¸€åŒ–åˆ°100
                kernel.l2_hit_rate or 0,
                kernel.warp_execution_efficiency or 0
            ]
            values += values[:1]
            
            ax.plot(angles, values, 'o-', linewidth=2, label=kernel.name[:20], color=colors[i])
            ax.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_ylim(0, 100)
        ax.set_title('Kernel æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾')
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'kernel_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

class NCUReporter:
    """NCU åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, parser: NCUParser, analyzer: NCUAnalyzer):
        self.parser = parser
        self.analyzer = analyzer
        self.output_dir = Path("ncu_analysis_output")
        self.output_dir.mkdir(exist_ok=True)
    
    def generate_report(self) -> None:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“„ ç”ŸæˆNCUåˆ†ææŠ¥å‘Š...")
        
        report_path = self.output_dir / "ncu_analysis_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_header())
            f.write(self._generate_summary())
            f.write(self._generate_gpu_utilization_report())
            f.write(self._generate_memory_report())
            f.write(self._generate_occupancy_report())
            f.write(self._generate_bottleneck_report())
            f.write(self._generate_recommendations())
        
        print(f"ğŸ“‹ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # åŒæ—¶ç”ŸæˆJSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_path = self.output_dir / "ncu_analysis_data.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.analyzer.stats, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“Š è¯¦ç»†æ•°æ®å·²ä¿å­˜: {json_path}")
        
        # å†™å…¥é›†æˆæŠ¥å‘Šï¼ˆæ–°å¢ï¼‰
        try:
            report_text = report_path.read_text(encoding='utf-8')
            self._update_integrated_report(report_text)
        except Exception as ie:
            print(f"âš ï¸ é›†æˆæŠ¥å‘Šæ›´æ–°å¤±è´¥: {ie}")
    
    def _generate_header(self) -> str:
        """ç”ŸæˆæŠ¥å‘Šå¤´éƒ¨"""
        return f"""
{'='*80}
NVIDIA Nsight Compute (NCU) æ€§èƒ½åˆ†ææŠ¥å‘Š
{'='*80}
åˆ†ææ–‡ä»¶: {self.parser.input_file}
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
åˆ†ækernelæ•°é‡: {len(self.parser.kernels)}
{'='*80}

"""
    
    def _fmt_pct(self, v) -> str:
        try:
            return f"{float(v):.1f}%"
        except Exception:
            return "N/A"

    def _fmt_num(self, v) -> str:
        try:
            return f"{float(v):.1f}"
        except Exception:
            return "N/A"

    def _generate_summary(self) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        gpu_stats = self.analyzer.stats.get('gpu_utilization', {})
        avg = self._fmt_pct(gpu_stats.get('average_sm_efficiency', None))
        low = gpu_stats.get('kernels_below_50_percent', 'N/A')
        return f"""
ğŸ“Š æ€§èƒ½æ‘˜è¦
{'-'*40}
â€¢ åˆ†ækernelæ•°é‡: {len(self.parser.kernels)}
â€¢ å¹³å‡SMæ•ˆç‡: {avg} (å¦‚æœæœ‰æ•°æ®)
â€¢ æ•ˆç‡ä½äº50%çš„kernelæ•°: {low}
â€¢ è¯†åˆ«çš„æ€§èƒ½ç“¶é¢ˆ: {len(self.analyzer.bottlenecks)}

"""

    def _generate_gpu_utilization_report(self) -> str:
        """ç”ŸæˆGPUåˆ©ç”¨ç‡æŠ¥å‘Š"""
        stats = self.analyzer.stats.get('gpu_utilization', {})
        if 'message' in stats:
            return f"""
ğŸ”¥ GPU åˆ©ç”¨ç‡åˆ†æ
{'-'*40}
{stats['message']}

"""
        return f"""
ğŸ”¥ GPU åˆ©ç”¨ç‡åˆ†æ
{'-'*40}
â€¢ å¹³å‡SMæ•ˆç‡: {self._fmt_pct(stats.get('average_sm_efficiency'))}
â€¢ æœ€é«˜SMæ•ˆç‡: {self._fmt_pct(stats.get('max_sm_efficiency'))}
â€¢ æœ€ä½SMæ•ˆç‡: {self._fmt_pct(stats.get('min_sm_efficiency'))}
â€¢ æ•ˆç‡ä½äº50%çš„kernel: {stats.get('kernels_below_50_percent', 0)} / {stats.get('total_kernels', 0)}

"""
    
    def _generate_memory_report(self) -> str:
        """ç”Ÿæˆå†…å­˜æ€§èƒ½æŠ¥å‘Š"""
        stats = self.analyzer.stats.get('memory_analysis', {})
        
        result = f"""
ğŸ’¾ å†…å­˜æ€§èƒ½åˆ†æ
{'-'*40}
"""
        
        if 'bandwidth_stats' in stats:
            bandwidth = stats['bandwidth_stats']
            result += f"â€¢ å¹³å‡DRAMå¸¦å®½: {bandwidth.get('average_bandwidth', 0):.1f} GB/s\n"
            result += f"â€¢ æœ€å¤§DRAMå¸¦å®½: {bandwidth.get('max_bandwidth', 0):.1f} GB/s\n"
        
        if 'l2_cache_stats' in stats:
            l2_stats = stats['l2_cache_stats']
            result += f"â€¢ å¹³å‡L2å‘½ä¸­ç‡: {l2_stats.get('average_l2_hit_rate', 0):.1f}%\n"
            result += f"â€¢ L2å‘½ä¸­ç‡ä½çš„kernel: {l2_stats.get('kernels_low_l2_hit_rate', 0)}\n"
        
        if 'l1_cache_stats' in stats:
            l1_stats = stats['l1_cache_stats']
            result += f"â€¢ å¹³å‡L1å‘½ä¸­ç‡: {l1_stats.get('average_l1_hit_rate', 0):.1f}%\n"
        
        return result + "\n"
    
    def _generate_occupancy_report(self) -> str:
        """ç”Ÿæˆå ç”¨ç‡æŠ¥å‘Š"""
        stats = self.analyzer.stats.get('occupancy_analysis', {})
        
        result = f"""
ğŸ¯ å ç”¨ç‡åˆ†æ
{'-'*40}
"""
        
        if 'achieved_occupancy' in stats:
            achieved = stats['achieved_occupancy']
            result += f"â€¢ å¹³å‡å®é™…å ç”¨ç‡: {achieved.get('average', 0):.1f}%\n"
            result += f"â€¢ å ç”¨ç‡èŒƒå›´: {achieved.get('min', 0):.1f}% - {achieved.get('max', 0):.1f}%\n"
        
        if 'occupancy_efficiency' in stats:
            efficiency = stats['occupancy_efficiency']
            result += f"â€¢ å¹³å‡å ç”¨ç‡æ•ˆç‡: {efficiency.get('average_ratio', 0)*100:.1f}%\n"
            result += f"â€¢ æ•ˆç‡ä½äº80%çš„kernel: {efficiency.get('kernels_low_efficiency', 0)}\n"
        
        return result + "\n"
    
    def _generate_bottleneck_report(self) -> str:
        """ç”Ÿæˆç“¶é¢ˆåˆ†ææŠ¥å‘Š"""
        stats = self.analyzer.stats.get('bottleneck_analysis', {})
        
        result = f"""
ğŸš« æ€§èƒ½ç“¶é¢ˆåˆ†æ
{'-'*40}
â€¢ æ€»ç“¶é¢ˆæ•°é‡: {stats.get('total_bottlenecks', 0)}
"""
        
        # ç“¶é¢ˆç±»å‹åˆ†å¸ƒ
        if 'bottleneck_types' in stats:
            result += "â€¢ ç“¶é¢ˆç±»å‹åˆ†å¸ƒ:\n"
            for btype, count in stats['bottleneck_types'].items():
                result += f"  - {btype}: {count}\n"
        
        # ä¸»è¦é—®é¢˜
        if 'top_issues' in stats and stats['top_issues']:
            result += "\nä¸»è¦æ€§èƒ½é—®é¢˜:\n"
            for i, issue in enumerate(stats['top_issues'][:3], 1):
                result += f"{i}. {issue['description']} ({issue['severity']})\n"
                for rec in issue['recommendations'][:2]:
                    result += f"   å»ºè®®: {rec}\n"
        
        return result + "\n"
    
    def _generate_recommendations(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        gpu_stats = self.analyzer.stats.get('gpu_utilization', {})
        if gpu_stats.get('kernels_below_50_percent', 0) > 0:
            recommendations.append("â€¢ æœ‰kernelçš„SMæ•ˆç‡ä½äº50%ï¼Œè€ƒè™‘å¢åŠ å·¥ä½œè´Ÿè½½æˆ–ä¼˜åŒ–ç®—æ³•")
        
        memory_stats = self.analyzer.stats.get('memory_analysis', {})
        if 'l2_cache_stats' in memory_stats:
            l2_stats = memory_stats['l2_cache_stats']
            if l2_stats.get('kernels_low_l2_hit_rate', 0) > 0:
                recommendations.append("â€¢ æ£€æµ‹åˆ°L2ç¼“å­˜å‘½ä¸­ç‡ä½çš„kernelï¼Œä¼˜åŒ–æ•°æ®è®¿é—®æ¨¡å¼")
        
        occupancy_stats = self.analyzer.stats.get('occupancy_analysis', {})
        if 'occupancy_efficiency' in occupancy_stats:
            if occupancy_stats['occupancy_efficiency'].get('kernels_low_efficiency', 0) > 0:
                recommendations.append("â€¢ æœ‰kernelå ç”¨ç‡æ•ˆç‡ä½ï¼Œæ£€æŸ¥èµ„æºé™åˆ¶(å¯„å­˜å™¨/å…±äº«å†…å­˜)")
        
        # é»˜è®¤å»ºè®®
        if not recommendations:
            recommendations = [
                "â€¢ ç›‘æ§kernelæ€§èƒ½æŒ‡æ ‡ï¼Œè¯†åˆ«ä¼˜åŒ–æœºä¼š",
                "â€¢ è€ƒè™‘ä½¿ç”¨Tensor CoreåŠ é€Ÿé€‚åˆçš„å·¥ä½œè´Ÿè½½", 
                "â€¢ ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼ä»¥æé«˜å¸¦å®½åˆ©ç”¨ç‡",
                "â€¢ å¹³è¡¡å ç”¨ç‡å’Œæ¯ä¸ªçº¿ç¨‹çš„èµ„æºä½¿ç”¨"
            ]
        
        return f"""
ğŸ’¡ ä¼˜åŒ–å»ºè®®
{'-'*40}
{chr(10).join(recommendations)}

{'='*80}
"""
    def _update_integrated_report(self, ncu_text: str):
        """æŠŠNCUåˆ†æç»“æœæ’å…¥é›†æˆæŠ¥å‘Šï¼Œç”¨æ ‡è®°åŒ…è£¹ä¾¿äºåç»­è¦†ç›–"""
        start_tag = "<!-- NCU_REPORT_START -->"
        end_tag   = "<!-- NCU_REPORT_END -->"
        block = f"{start_tag}\n\n{ncu_text}\n{end_tag}\n"

        if INTEGRATED_MD.exists():
            content = INTEGRATED_MD.read_text(encoding='utf-8')
            if start_tag in content and end_tag in content:
                # æ›¿æ¢æ—§å—
                import re
                content = re.sub(f"{start_tag}.*?{end_tag}", block, content, flags=re.DOTALL)
            else:
                # è¿½åŠ åˆ°æœ«å°¾
                content += ("\n\n" + block)
        else:
            # åˆæ¬¡åˆ›å»º
            header = "# é›†æˆæ€§èƒ½åˆ†ææŠ¥å‘Š\n\n"
            content = header + block

        INTEGRATED_MD.write_text(content, encoding='utf-8')
        print(f"ğŸ§· å·²æ›´æ–°é›†æˆæŠ¥å‘Š: {INTEGRATED_MD}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='NVIDIA Nsight Compute (NCU) è¾“å‡ºæ–‡ä»¶è‡ªåŠ¨åŒ–è§£æå·¥å…·')
    parser.add_argument('input_file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„ (.ncu-rep, .csv, .json)')
    parser.add_argument('--no-viz', action='store_true', help='ä¸ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--no-report', action='store_true', help='ä¸ç”Ÿæˆåˆ†ææŠ¥å‘Š')
    parser.add_argument('--output-dir', default='ncu_analysis_output', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    try:
        # è§£ææ–‡ä»¶
        print(f"ğŸš€ å¼€å§‹è§£æNCUæ–‡ä»¶: {args.input_file}")
        ncu_parser = NCUParser(args.input_file)
        ncu_parser.parse()
        
        # åˆ†ææ•°æ®
        analyzer = NCUAnalyzer(ncu_parser)
        analyzer.analyze()
        
        # ç”Ÿæˆå¯è§†åŒ–
        if not args.no_viz:
            visualizer = NCUVisualizer(ncu_parser, analyzer)
            visualizer.output_dir = Path(args.output_dir)
            visualizer.output_dir.mkdir(exist_ok=True)
            visualizer.create_visualizations()
        
        # ç”ŸæˆæŠ¥å‘Š
        if not args.no_report:
            reporter = NCUReporter(ncu_parser, analyzer)
            reporter.output_dir = Path(args.output_dir)
            reporter.output_dir.mkdir(exist_ok=True)
            reporter.generate_report()
        
        print(f"\nâœ… NCUåˆ†æå®Œæˆ! ç»“æœä¿å­˜åœ¨: {args.output_dir}")
        print(f"ğŸ“Š è§£æäº† {len(ncu_parser.kernels)} ä¸ªkernels")
        print(f"ğŸš« è¯†åˆ«äº† {len(analyzer.bottlenecks)} ä¸ªæ€§èƒ½ç“¶é¢ˆ")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
