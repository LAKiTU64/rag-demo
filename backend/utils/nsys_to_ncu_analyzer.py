#!/usr/bin/env python3
"""
NVIDIA æ€§èƒ½åˆ†æé›†æˆå·¥å…·
å…ˆç”¨ nsys è¯†åˆ«çƒ­ç‚¹kernelsï¼Œå†ç”¨ ncu æ·±åº¦åˆ†æ

å·¥ä½œæµç¨‹ï¼š
1. nsys profile -> è·å–å…¨å±€æ€§èƒ½overview  
2. æå–çƒ­ç‚¹kernelåç§°
3. ncu profile -> é’ˆå¯¹çƒ­ç‚¹kernelsæ·±åº¦åˆ†æ
4. ç»¼åˆåˆ†ææŠ¥å‘Š

ä½œè€…: xjw
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import json
import subprocess
import argparse
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime, timezone

# å¯¼å…¥æˆ‘ä»¬çš„åˆ†æå·¥å…·
sys.path.append(str(Path(__file__).parent))
from nsys_parser import NsysParser, NsysAnalyzer
from ncu_parser import NCUParser, NCUAnalyzer, NCUVisualizer, NCUReporter

# å¼•å…¥é«˜é˜¶æŠ¥å‘Šä¸çŸ¥è¯†åº“æ‘„å–æ¨¡å—ï¼ˆå¯é€‰ï¼‰
try:
    from backend.advanced_report import generate_advanced_report
except Exception:
    generate_advanced_report = None  # type: ignore
try:
    from backend.knowledge_bases.kb_ingest import ingest_json_to_faiss, flatten_json
except Exception:
    ingest_json_to_faiss = None  # type: ignore
    flatten_json = None  # type: ignore

class NSysToNCUAnalyzer:
    """é›†æˆ nsys å’Œ ncu çš„åˆ†æå·¥å…·

    ç»Ÿä¸€è¾“å‡ºç›®å½•:
        é»˜è®¤ä½¿ç”¨ /workspace/Agent/AI_Agent_Complete ä½œä¸ºæ ¹è·¯å¾„ä¸‹çš„ integrated_analysis å­ç›®å½•ï¼Œ
        ä¾¿äº Agent è¯»å–æ‰€æœ‰ç”Ÿæˆçš„æŠ¥å‘Šå’Œä¸­é—´äº§ç‰©ã€‚
    """
    DEFAULT_BASE_DIR = Path("/workspace/Agent/AI_Agent_Complete")

    def __init__(self, output_dir: str = "integrated_analysis", env: Optional[Dict[str, str]] = None):
        # å¦‚æœç”¨æˆ·ä¼ å…¥çš„æ˜¯ç»å¯¹è·¯å¾„åˆ™ä½¿ç”¨åŸå€¼ï¼Œå¦åˆ™æ‹¼æ¥åˆ°é»˜è®¤åŸºè·¯å¾„ä¸‹
        base = self.DEFAULT_BASE_DIR
        if output_dir.startswith('/'):
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = base / output_dir
        self.output_dir.mkdir(exist_ok=True)
        self.hot_kernels = []
        self.nsys_stats = {}
        self.ncu_results = {}
        # ä¸ºå­è¿›ç¨‹è°ƒç”¨ï¼ˆnsys/ncuï¼‰é¢„å…ˆä¿å­˜ç¯å¢ƒå˜é‡ï¼Œä¾¿äºæ§åˆ¶ GPU ç»‘å®š
        self.env = env or os.environ.copy()
        
    def step1_nsys_analysis(self, target_command: List[str], profile_name: str = "overview") -> str:
        """ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨nsysè¿›è¡Œå…¨å±€æ€§èƒ½åˆ†æ"""
        
        nsys_profile = self.output_dir / f"{profile_name}.nsys-rep"
        
        # æ„å»ºnsyså‘½ä»¤
        nsys_cmd = [
            'nsys', 'profile',
            '-o', str(nsys_profile.with_suffix('')),  # nsysä¼šè‡ªåŠ¨æ·»åŠ .nsys-rep
            '-t', 'cuda,nvtx,osrt',
            '--cuda-memory-usage=true',
            '--force-overwrite=true'
            # '--capture-range=nvtx',
            # '--capture-range-end=stop',
        ] + target_command 
        
        print("ğŸš€ æ­¥éª¤1: è¿è¡Œnsyså…¨å±€æ€§èƒ½åˆ†æ...")
        print(f"å‘½ä»¤: {' '.join(nsys_cmd)}")
        
        try:
            result = subprocess.run(nsys_cmd, capture_output=True, text=True, check=True, env=self.env)
            print(f"âœ… nsysåˆ†æå®Œæˆ: {nsys_profile}")
            return str(nsys_profile)
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ nsysåˆ†æå¤±è´¥: {e.stderr}")
            raise
    
    def step2_extract_hot_kernels(self, nsys_file: str, 
                                  top_k: int = 10, 
                                  min_duration_ms: float = 0.1) -> List[Dict]:
        """ç¬¬äºŒæ­¥ï¼šè§£æ nsys å¹¶ç”Ÿæˆ layer_kernels.csvï¼ˆè·³è¿‡çƒ­ç‚¹æå–ï¼Œç›´æ¥ä½¿ç”¨ NVTX å…³è”çš„ kernelsï¼‰
        ç°åœ¨é»˜è®¤ä»…ä½¿ç”¨ Run[2] çš„å­é›†è¿›è¡Œåç»­åˆ†æä¸æŠ¥å‘Šã€‚
        """
        
        print("ğŸ” æ­¥éª¤2: è§£æ nsys å¹¶ç”Ÿæˆ layer_kernels.csvï¼ˆè·³è¿‡çƒ­ç‚¹æå–ï¼‰...")
        
        parser = NsysParser(nsys_file)
        parser.parse()
        # å¯¼å‡ºç”±ä¸‰è¡¨å…³è”å¾—åˆ°çš„ layer_kernels.csvï¼ˆä½äº .sqlite åŒç›®å½•ï¼‰
        lk_csv = parser.export_kernel_summary_csv(nsys_file, self.output_dir / f"{Path(nsys_file).stem}_kernels")
        target_csv = self.output_dir / "layer_kernels.csv"
        if lk_csv:
            try:
                src = Path(lk_csv)
                if src.resolve() == target_csv.resolve():
                    # æºç›®æ ‡ä¸€è‡´ï¼Œè§†ä¸ºæˆåŠŸ
                    print(f"ğŸ“„ layer_kernels.csv å·²å­˜åœ¨äºè¾“å‡ºç›®å½•: {target_csv}")
                else:
                    # å¼ºåˆ¶è¦†ç›–
                    import shutil, os
                    if target_csv.exists():
                        os.remove(target_csv)
                    shutil.copy2(src, target_csv)
                    print(f"ğŸ“„ å·²ç”Ÿæˆ layer_kernels.csv: {target_csv}")
            except Exception as e:
                print(f"âš ï¸ æ‹·è´ layer_kernels.csv å¤±è´¥: {e}. åŸè·¯å¾„: {lk_csv}")
        else:
            print("âš ï¸ æœªç”Ÿæˆ layer_kernels.csvï¼Œè¯·ç¡®è®¤ NVTX_EVENTS/StringIds/CUPTI è¡¨å­˜åœ¨ä¸”æœ‰ Layer[...] æ ‡ç­¾")

        # ä»…ä¿ç•™ Run[2] çš„å­é›†
        run_tag = "#Run[2]"
        run_csv = self.output_dir / "layer_kernels_run2.csv"
        try:
            import csv
            kept = 0
            with open(target_csv, newline='', encoding='utf-8') as f, open(run_csv, 'w', newline='', encoding='utf-8') as g:
                rdr = csv.DictReader(f)
                w = csv.DictWriter(g, fieldnames=rdr.fieldnames)
                w.writeheader()
                for r in rdr:
                    if run_tag in (r.get("layer") or ""):
                        w.writerow(r)
                        kept += 1
            print(f"ğŸ“„ å·²ç”Ÿæˆä»… Run[2] çš„å­é›†: {run_csv}ï¼ˆ{kept} è¡Œï¼‰")
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆ Run[2] å­é›†å¤±è´¥: {e}")

        # ç”¨ Run[2] å­é›†ç”Ÿæˆç²¾ç®€ JSONï¼ˆä»… nameã€dur_msï¼‰ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºåç»­å”¯ä¸€æ•°æ®æº
        rows_run2 = []
        try:
            import csv, json
            with open(run_csv, newline='', encoding='utf-8') as f:
                rdr = csv.DictReader(f)
                for r in rdr:
                    rows_run2.append({
                        'name': r.get('kernel_name',''),
                        'dur_ms': float(r.get('dur_ms', 0.0) or 0.0)
                    })
            hot_json_path = self.output_dir / "layer_kernels_run2_hot.json"
            hot_json_path.write_text(json.dumps(rows_run2, ensure_ascii=False, indent=2), encoding='utf-8')
            print(f"ğŸ“„ å·²ç”Ÿæˆç²¾ç®€çƒ­ç‚¹æ–‡ä»¶(é¡ºåºä¿ç•™ï¼Œä¸å»é‡): {hot_json_path}")
        except Exception as e:
            print(f"âš ï¸ è¯»å– Run[2] å­é›†å¤±è´¥: {e}")
            rows_run2 = []

        # ç”¨ Run[2] JSON æ›´æ–° nsys æ¦‚è§ˆï¼ˆtotal_time ä¸º dur_ms æ±‚å’Œï¼Œä¸ç»Ÿè®¡ count/maxï¼‰
        total_time_ms = sum(item.get('dur_ms', 0.0) for item in rows_run2)
        total_kernels = len(rows_run2)
        avg_time_ms = (total_time_ms / total_kernels) if total_kernels else 0.0

        analyzer = NsysAnalyzer(parser)
        self.nsys_stats = {
            'kernel_analysis': {
                'total_kernels': total_kernels,
                'total_kernel_time': total_time_ms,
                'avg_kernel_time': avg_time_ms,
            },
            'layer_kernels_rows': rows_run2,           # ç”¨ç²¾ç®€ç»“æ„æ›¿ä»£
            'layer_kernels_source': str(hot_json_path) # æŒ‡å‘ run2_hot.json
        }

        # å°†â€œçƒ­ç‚¹â€åˆ—è¡¨è®¾ç½®ä¸º run2_hot.json çš„é¡ºåºåˆ—è¡¨ï¼ˆä¸å»é‡ã€ä¸æ’åºï¼‰
        self.hot_kernels = rows_run2[:]
        print(f"â„¹ï¸ å·²æŒ‰è¦æ±‚ä½¿ç”¨ Run[2] JSONï¼ˆ{len(self.hot_kernels)} æ¡ï¼‰ï¼Œé¡ºåºä¿ç•™ä¸”ä¸å»é‡ã€‚")
        return self.hot_kernels
    
    def step3_ncu_targeted_analysis(self, target_command: List[str], 
                                   kernels_to_analyze: List[Dict],
                                   max_kernels: Optional[int] = None) -> List[str]:
        """ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ncuå¯¹çƒ­ç‚¹kernelsè¿›è¡Œæ·±åº¦åˆ†æ
        ç°åœ¨é»˜è®¤å¯¹ä¼ å…¥åˆ—è¡¨çš„æ¯ä¸€ä¸ª kernel è¿›è¡Œåˆ†æï¼ˆä¿æŒé¡ºåºï¼‰ï¼Œé™¤éæ˜¾å¼æä¾› max_kernelsã€‚
        """
        
        print("âš¡ æ­¥éª¤3: ä½¿ç”¨ncuæ·±åº¦åˆ†æçƒ­ç‚¹kernels...")
        
        ncu_results = []

        # å…è®¸ä»…åŒ…å« {name, dur_ms} çš„æœ€ç®€ç»“æ„
        # è‹¥æ£€æµ‹åˆ°å ä½/æ•°å­— kernel åï¼Œå°è¯• list-kernels å‘ç°çœŸå®å
        if any(self._is_placeholder_name(str(k.get('name',''))) for k in kernels_to_analyze):
            print("ğŸ” æ£€æµ‹åˆ°å ä½/æ•°å­—kernelåï¼Œè§¦å‘ ncu --list-kernels è¿›è¡ŒçœŸå®åç§°å‘ç°...")
            discovered = self.list_kernels_with_ncu(target_command)
            selected = self._select_real_kernels(discovered, len(kernels_to_analyze) if max_kernels is None else max_kernels)
            print(f"ğŸ§­ é€‰æ‹©ç”¨äºæ·±åº¦åˆ†æçš„çœŸå®kernelåç§°: {selected}")
            # æ›¿æ¢å‰è‹¥é•¿åº¦ä¸è¶³åˆ™è¡¥é½
            for i, real_name in enumerate(selected):
                if i < len(kernels_to_analyze):
                    kernels_to_analyze[i]['name'] = real_name
                    kernels_to_analyze[i]['discovered'] = True
                else:
                    kernels_to_analyze.append({'name': real_name, 'discovered': True})

        # åˆ†ææ•°é‡ï¼šé»˜è®¤å…¨éƒ¨ï¼›è‹¥æŒ‡å®š max_kernels åˆ™æˆªæ–­
        if max_kernels is not None:
            kernels_to_analyze = kernels_to_analyze[:max_kernels]
        
        for i, kernel_info in enumerate(kernels_to_analyze):
            kernel_name = str(kernel_info.get('name', 'kernel')).strip()

            # æ¸…ç†kernelåç§°ï¼Œç”¨äºæ–‡ä»¶å
            safe_name = re.sub(r'[^\w\-_]', '_', kernel_name)[:50]
            ncu_profile = self.output_dir / f"ncu_kernel_{i}_{safe_name}"
            
            print(f"ğŸ¯ æ­£åœ¨åˆ†ækernel {i+1}/{len(kernels_to_analyze)}: {kernel_name[:60]}...")

            def attempt_profile(attempt_cmd: List[str], attempt_tag: str) -> Optional[str]:
                """å°è£…ä¸€æ¬¡ ncu å°è¯•ï¼Œè¿”å› .ncu-rep è·¯å¾„æˆ– None"""
                try:
                    res = subprocess.run(attempt_cmd, env=self.env)
                    if res.returncode != 0:
                        snippet = (res.stderr or '')[:200].replace('\n', ' ')
                        print(f"âš ï¸ å°è¯• {attempt_tag} å¤±è´¥(returncode={res.returncode}): {snippet}")
                        return None
                    ncu_file = str(ncu_profile) + '.ncu-rep'
                    if Path(ncu_file).exists():
                        print(f"âœ… æˆåŠŸç”Ÿæˆ NCU æŠ¥å‘Š ({attempt_tag}): {ncu_file}")
                        return ncu_file
                    else:
                        print(f"âš ï¸ å°è¯• {attempt_tag} æœªç”Ÿæˆ .ncu-rep æ–‡ä»¶: {ncu_file}")
                except subprocess.TimeoutExpired:
                    print(f"â° å°è¯• {attempt_tag} è¶…æ—¶")
                except Exception as e:
                    print(f"âŒ å°è¯• {attempt_tag} å¼‚å¸¸: {e}")
                return None

            # å›é€€ç­–ç•¥ï¼šç²¾ç¡®åŒ¹é… -> æ­£åˆ™å‰ç¼€ -> æ— è¿‡æ»¤
            attempts = []
            attempts.append({
                'tag': 'exact-demangled',
                'cmd': ['ncu', '--kernel-name-base', 'demangled', '--kernel-name', kernel_name,
                        '--rename-kernels=0', '--set', 'full', '-o', str(ncu_profile), '--force-overwrite'] + target_command
            })
            prefix_raw = re.sub(r'"', '', kernel_name)[:60]
            prefix_regex = re.sub(r'([\\.^$|?*+\[\](){}])', r'\\\1', prefix_raw)
            attempts.append({
                'tag': 'regex-prefix',
                'cmd': ['ncu', '--kernel-name-base', 'demangled', '--kernel-name', f'regex:^{prefix_regex}',
                        '--rename-kernels=0', '--set', 'full', '-o', str(ncu_profile), '--force-overwrite'] + target_command
            })
            attempts.append({
                'tag': 'unfiltered-basic',
                'cmd': ['ncu', '--launch-count', '50', '--set', 'compute', '-o', str(ncu_profile), '--force-overwrite'] + target_command
            })

            produced = None
            for att in attempts:
                print(f"ğŸ” NCU å°è¯•: {att['tag']}")
                print(att['cmd'])
                produced = attempt_profile(att['cmd'], att['tag'])
                if produced:
                    break

            if produced:
                ncu_results.append(produced)
                self._export_ncu_to_csv(produced)
            else:
                print(f"âŒ æ‰€æœ‰å°è¯•å‡æœªç”Ÿæˆ NCU æŠ¥å‘Š: {kernel_name[:80]}")
        
        return ncu_results

    def step3_ncu_global_focus(self, target_command: List[str], hot_kernels: List[Dict], top_focus: int = 5,
                               set_name: str = 'compute', launch_limit: Optional[int] = None) -> Tuple[Optional[str], Dict[str, Dict]]:
        """æ›¿ä»£å®šå‘åˆ†æï¼šä¸€æ¬¡å…¨é‡ ncu é‡‡é›†ï¼Œç„¶åä»…é’ˆå¯¹ nsys å‘ç°çš„å‰ top_focus ä¸ªçƒ­ç‚¹ kernel æå–ä¸å½’å¹¶æŒ‡æ ‡ã€‚

        è¿”å›: (å…¨é‡ ncu æŠ¥å‘Šè·¯å¾„æˆ– None, focus_metrics dict)
        focus_metrics ç»“æ„ (é”®ä¸ºçƒ­ç‚¹ kernel åŸå):
            {
              kernel_display_name: {
                  'kernels_analyzed': int,
                  'gpu_utilization': {...},
                  'memory_analysis': {...},
                  'bottleneck_summary': [...]
              }
            }
        """
        if not hot_kernels:
            print("âš ï¸ æ— çƒ­ç‚¹ kernelï¼Œè·³è¿‡å…¨é‡ NCU é‡‡é›†")
            return None, {}
        # è¿è¡Œä¸€æ¬¡å…¨é‡é‡‡é›†
        full_rep = self.full_ncu_capture(target_command, profile_name='ncu_full_capture_global', set_name=set_name, launch_limit=launch_limit)
        if not full_rep:
            return None, {}
        csv_file = full_rep.replace('.ncu-rep', '.csv')
        if not Path(csv_file).exists() or Path(csv_file).stat().st_size == 0:
            print("âš ï¸ å…¨é‡é‡‡é›† CSV ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ— æ³•æå–ç„¦ç‚¹å†…æ ¸æŒ‡æ ‡")
            return full_rep, {}
        # è§£æ CSV
        try:
            parser = NCUParser(csv_file)
            parser.parse()
        except Exception as e:
            print(f"âš ï¸ å…¨é‡ CSV è§£æå¤±è´¥: {e}")
            return full_rep, {}
        # æ„å»ºç„¦ç‚¹åˆ†æ
        focus = {}
        # å»ºç«‹å¿«é€Ÿåˆ—è¡¨
        metrics_list = parser.kernels  # List[KernelMetrics]
        def _match_entries(target: str) -> List[Any]:
            t_low = target.lower()
            matched = [km for km in metrics_list if t_low in km.name.lower() or km.name.lower() in t_low]
            # è‹¥æ— ç›´æ¥åŒ…å«åŒ¹é…ï¼Œå°è¯•æŒ‰åˆ†è¯å…¬å…±å­ä¸² >=5 char
            if not matched:
                # ç®€å•åˆ‡å‰²éå­—æ¯æ•°å­—
                import re
                tokens = [tok for tok in re.split(r'[^A-Za-z0-9_]+', t_low) if len(tok) >= 5]
                if tokens:
                    for tok in tokens:
                        part = [km for km in metrics_list if tok in km.name.lower()]
                        matched.extend(part)
            # å»é‡
            uniq = []
            seen = set()
            for m in matched:
                if id(m) not in seen:
                    seen.add(id(m)); uniq.append(m)
            return uniq[:50]  # é˜²æ­¢è¿‡å¤š
        def _avg(vals: List[Optional[float]]) -> Optional[float]:
            nums = [v for v in vals if isinstance(v, (int, float))]
            return sum(nums)/len(nums) if nums else None
        focus_targets = hot_kernels[:top_focus]
        for hk in focus_targets:
            kname = str(hk.get('name',''))
            entries = _match_entries(kname)
            if not entries:
                continue
            sm_eff = _avg([e.sm_efficiency for e in entries])
            occ = _avg([e.achieved_occupancy for e in entries])
            dram = _avg([e.dram_bandwidth for e in entries])
            l2 = _avg([e.l2_hit_rate for e in entries])
            warp_eff = _avg([e.warp_execution_efficiency for e in entries])
            tensor_active = _avg([e.tensor_active for e in entries])
            # ç“¶é¢ˆåˆ¤å®š (å¯å‘å¼)
            bottlenecks = []
            def add_bottleneck(cond: bool, desc: str, severity: str):
                if cond:
                    bottlenecks.append({'type': 'heuristic', 'severity': severity, 'description': desc})
            add_bottleneck(sm_eff is not None and sm_eff < 40, 'SMæ•ˆç‡åä½', 'high')
            add_bottleneck(dram is not None and dram < 150, 'å†…å­˜å¸¦å®½å¯èƒ½å—é™', 'medium')
            add_bottleneck(occ is not None and occ < 25, 'Occupancyè¾ƒä½', 'medium')
            add_bottleneck(warp_eff is not None and warp_eff < 70, 'Warpæ‰§è¡Œæ•ˆç‡ä¸€èˆ¬', 'low')
            focus[kname] = {
                'kernels_analyzed': len(entries),
                'gpu_utilization': {
                    'average_sm_efficiency': sm_eff,
                    'achieved_occupancy': occ,
                    'tensor_core_active': tensor_active,
                },
                'memory_analysis': {
                    'bandwidth_stats': {
                        'average_bandwidth': dram,
                        'l2_hit_rate': l2,
                    }
                },
                'bottleneck_summary': bottlenecks
            }
        print(f"ğŸ” å…¨é‡é‡‡é›†ä¸­å·²ç”Ÿæˆ {len(focus)} ä¸ªç„¦ç‚¹å†…æ ¸çš„èšåˆæŒ‡æ ‡")
        return full_rep, focus

    def full_ncu_capture(self, target_command: List[str], profile_name: str = "ncu_full_capture",
                          set_name: str = "compute", launch_limit: Optional[int] = None,
                          timeout: int = 1200) -> Optional[str]:
        """æ‰§è¡Œä¸€æ¬¡ä¸åš kernel è¿‡æ»¤çš„å®Œæ•´ NCU é‡‡é›†ã€‚

        å‚æ•°:
            target_command: åŸå§‹å¾…åˆ†æå‘½ä»¤ (['python', 'script.py', ...])
            profile_name: è¾“å‡ºæŠ¥å‘ŠåŸºå
            set_name: ä½¿ç”¨çš„ NCU æŒ‡æ ‡é›†åˆ (--set)ã€‚å¯é€‰: 'compute', 'full' ç­‰
            launch_limit: ä½¿ç”¨ --launch-count é™åˆ¶é‡‡é›†çš„ kernel æ¬¡æ•° (é™ä½é•¿ä»»åŠ¡å¼€é”€)
            timeout: è¶…æ—¶æ—¶é—´ (ç§’)

        è¡Œä¸º:
            ç”Ÿæˆ <profile_name>.ncu-rep åŠå¯¹åº”çš„ CSV/JSON (è‹¥å¯èƒ½)
            è¾“å‡ºè·¯å¾„ä½äºç»Ÿä¸€çš„ self.output_dir ä¸‹ã€‚
        """
        ncu_profile_base = self.output_dir / profile_name
        ncu_rep = str(ncu_profile_base) + '.ncu-rep'
        cmd = ['ncu', '--set', set_name, '-o', str(ncu_profile_base), '--force-overwrite']
        if launch_limit:
            cmd += ['--launch-count', str(launch_limit)]
        # ä¸åŠ  --kernel-name è¿‡æ»¤, æ•è·å…¨éƒ¨å¯è§å†…æ ¸
        cmd += target_command
        print(f"ğŸŒ€ å…¨é‡ NCU é‡‡é›†: {' '.join(cmd)}")
        try:
            res = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout, env=self.env)
            if res.returncode != 0:
                print(f"âš ï¸ å…¨é‡é‡‡é›†å¤±è´¥(returncode={res.returncode}): {(res.stderr or '')[:300].replace('\n',' ')}")
                return None
            if not Path(ncu_rep).exists():
                print(f"âš ï¸ æœªç”Ÿæˆ ncu æŠ¥å‘Šæ–‡ä»¶: {ncu_rep}")
                return None
            print(f"âœ… å…¨é‡ NCU é‡‡é›†å®Œæˆ: {ncu_rep}")
            # å°è¯•å¯¼å‡º CSV
            self._export_ncu_to_csv(ncu_rep)
            return ncu_rep
        except subprocess.TimeoutExpired:
            print("â³ å…¨é‡ NCU é‡‡é›†è¶…æ—¶")
        except Exception as e:
            print(f"âŒ å…¨é‡ NCU é‡‡é›†å¼‚å¸¸: {e}")
        return None

    def list_kernels_with_ncu(self, target_command: List[str]) -> List[str]:
        """è¿è¡Œ ncu --list-kernels ä»¥è·å–å®é™…å¯åˆ†æçš„ kernel åç§°åˆ—è¡¨"""
        cmd = ['ncu', '--list-kernels'] + target_command
        print(f"ğŸ§ª è¿è¡Œ: {' '.join(cmd)}")
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600, env=self.env)
            output = result.stdout + '\n' + result.stderr
        except Exception as e:
            print(f"âŒ list-kernels å¤±è´¥: {e}")
            return []

        # è§£æè¾“å‡ºï¼šæ¯è¡Œå¯èƒ½åŒ…å« kernel åç§°ã€‚æˆ‘ä»¬è¿‡æ»¤å‡ºå« 'Kernel', 'cuda', 'cutlass', 'flash', 'aten', 'cublas', 'gemm', 'matmul', 'triton'
        lines = [l.strip() for l in output.splitlines() if l.strip()]
        kernels = []
        import re
        pattern = re.compile(r'(Kernel|cuda|cutlass|flash|aten|cublas|gemm|matmul|triton)', re.IGNORECASE)
        for line in lines:
            # å¸¸è§æ ¼å¼: index + name æˆ– ç›´æ¥ name
            # æ’é™¤å¤ªçŸ­è¡Œ
            if len(line) < 4:
                continue
            if pattern.search(line):
                # å»æ‰å‰å¯¼ç¼–å·æˆ–è£…é¥°ç¬¦
                cleaned = re.sub(r'^\s*\d+\s*[:\-]?\s*', '', line)
                kernels.append(cleaned)
        # å»é‡ä¿æŒé¡ºåº
        seen = set(); uniq = []
        for k in kernels:
            if k not in seen:
                seen.add(k); uniq.append(k)
        print(f"ğŸ“‹ list-kernels è·å¾—å€™é€‰ {len(uniq)} ä¸ª (å‰10): {uniq[:10]}")
        return uniq

    def _select_real_kernels(self, discovered: List[str], max_kernels: int) -> List[str]:
        """æ ¹æ®ä¼˜å…ˆçº§ä»å‘ç°çš„ kernel åç§°åˆ—è¡¨ä¸­æŒ‘é€‰ç”¨äºåˆ†æçš„åç§°"""
        if not discovered:
            return []
        priority_patterns = [
            'FlashAttn', 'flash', 'cutlass', 'triton', 'gemm', 'matmul', 'cublas', 'aten', 'reduce', 'norm'
        ]
        scored = []
        for name in discovered:
            low = name.lower()
            score = 0
            for idx, pat in enumerate(priority_patterns):
                if pat.lower() in low:
                    score += (100 - idx)  # earlier pattern higher score
            # é•¿åº¦å’ŒåŒ…å« Kernel å­—æ ·åŠ ä¸€ç‚¹åˆ†
            if 'kernel' in low:
                score += 5
            if len(name) > 30:
                score += 1
            scored.append((score, name))
        # æ’åºï¼Œåˆ†æ•°é«˜çš„é å‰
        scored.sort(reverse=True)
        selected = [n for s, n in scored[:max_kernels]]
        return selected

    def _is_placeholder_name(self, name: str) -> bool:
        # å°†æ•°å­—ã€__unnamed_ ä»¥åŠè‹¥å¹²ä½ä¿¡å· / æ¡†æ¶æ€§åå­—è§†ä¸ºå ä½ã€‚å¢åŠ  __cudart_ å‰ç¼€ï¼Œä»¥ä¾¿åç»­ç”¨ CSV çœŸå® kernel åæ›¿æ¢ã€‚
        return (
            name.isdigit()
            or name.startswith('__unnamed_')
            or name in ('cudafe++', 'sleep', 'python', 'node')
            or name.startswith('__cudart_')
        )
    
    def _export_ncu_to_csv(self, ncu_file: str) -> Optional[str]:
        """å¯¼å‡ºncuç»“æœä¸ºCSVæ ¼å¼"""
        csv_file = ncu_file.replace('.ncu-rep', '.csv')
        
        export_cmd = [
            'ncu', '--csv',
            '--log-file', csv_file,
            '--import', ncu_file
        ]
        
        try:
            subprocess.run(export_cmd, capture_output=True, text=True, check=True)
            if Path(csv_file).exists():
                return csv_file
        except:
            pass
        
        return None
    
    def step4_comprehensive_analysis(self, ncu_files: List[str], focus_metrics: Optional[Dict[str, Dict]] = None) -> Dict:
        """ç¬¬å››æ­¥ï¼šç»¼åˆåˆ†ænsyså’Œncuç»“æœ"""
        
        print("ğŸ“Š æ­¥éª¤4: ç»¼åˆåˆ†æç»“æœ...")
        
        comprehensive_results = {
            'timestamp': datetime.now().isoformat(),
            'nsys_overview': self.nsys_stats,
            'hot_kernels_count': len(self.hot_kernels),
            'ncu_detailed_analysis': {},
            'ncu_focus_analysis': focus_metrics or {}
        }
        
        # åˆ†ææ¯ä¸ªncuç»“æœ
        # è‹¥æä¾›ç„¦ç‚¹èšåˆæŒ‡æ ‡ï¼Œåˆ™ä¸å¿…å¯¹å…¨é‡ ncu_full_capture_global é€æ–‡ä»¶åšæ ‡å‡†åˆ†æï¼ˆä»å¯ä¿ç•™ targeted æ–‡ä»¶åˆ†æï¼‰
        for ncu_file in ncu_files:
            csv_file = ncu_file.replace('.ncu-rep', '.csv')
            print(csv_file)
            if Path(csv_file).exists():
                try:
                    # ä½¿ç”¨æˆ‘ä»¬çš„ncuåˆ†æå™¨
                    parser = NCUParser(csv_file)
                    parser.parse()
                    
                    analyzer = NCUAnalyzer(parser)
                    stats = analyzer.analyze()
                    
                    kernel_name = Path(ncu_file).stem
                    comprehensive_results['ncu_detailed_analysis'][kernel_name] = {
                        'kernels_analyzed': len(parser.kernels),
                        'bottlenecks_found': len(analyzer.bottlenecks),
                        'gpu_utilization': stats.get('gpu_utilization', {}),
                        'memory_analysis': stats.get('memory_analysis', {}),
                        'bottleneck_summary': [
                            {
                                'type': b.type,
                                'severity': b.severity,
                                'description': b.description
                            }
                            for b in analyzer.bottlenecks[:3]  # åªå–å‰3ä¸ª
                        ]
                    }
                    
                    # ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–æŠ¥å‘Š
                    visualizer = NCUVisualizer(parser, analyzer)
                    vis_output_dir = self.output_dir / f"visualization_{kernel_name}"
                    visualizer.output_dir = vis_output_dir
                    vis_output_dir.mkdir(exist_ok=True)
                    visualizer.create_visualizations()
                    
                except Exception as e:
                    print(f"âš ï¸  åˆ†æ {ncu_file} å¤±è´¥: {e}")
        
        # ä¿å­˜ç»¼åˆåˆ†æç»“æœ
        results_file = self.output_dir / "comprehensive_analysis.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“‹ ç»¼åˆåˆ†æç»“æœå·²ä¿å­˜: {results_file}")
        return comprehensive_results
    
    def generate_final_report(self, comprehensive_results: Dict) -> str:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        report_file = self.output_dir / "integrated_performance_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("é›†æˆæ€§èƒ½åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().astimezone().strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n")
            # nsysæ¦‚è§ˆï¼ˆä¼˜å…ˆå±•ç¤º layer_kernelsï¼‰
            f.write("ä¸€ã€Nsys å…¨å±€æ€§èƒ½æ¦‚è§ˆ\n")
            nsys_overview = comprehensive_results.get('nsys_overview', {})
            # layer_rows = nsys_overview.get('layer_kernels_rows', [])
            # src_hint = nsys_overview.get('layer_kernels_source')
            # if layer_rows:
            #     if src_hint:
            #         f.write(f"- æ¥æºï¼š{src_hint}\n")
            #     else:
            #         f.write("- æ¥æºï¼šNVTX_EVENTS + StringIds + CUPTI_ACTIVITY_KIND_KERNEL ä¸‰è¡¨å…³è”\n")
            #     f.write("- æ˜ç»†è§: layer_kernels_run2_hot.json\n\n")
            #     preview = layer_rows[:20]
            #     for r in preview:
            #         f.write(f"- {str(r.get('name',''))[:80]} | {r.get('dur_ms',0)} ms\n")
            #     f.write("\n")
            # # æ¦‚è§ˆ
            if 'kernel_analysis' in nsys_overview:
                kernel_stats = nsys_overview['kernel_analysis']
                f.write(f" æ€»kernelsæ•°é‡: {kernel_stats.get('total_kernels', 0)}\n")
                f.write(f" æ€»kernelæ‰§è¡Œæ—¶é—´: {kernel_stats.get('total_kernel_time', 0):.2f} ms\n")
                # f.write(f"- å¹³å‡kernelæ‰§è¡Œæ—¶é—´: {kernel_stats.get('avg_kernel_time', 0):.3f} ms\n")
            f.write("\n")
            # çƒ­ç‚¹kernelsï¼ˆä¿æŒé¡ºåºï¼Œä¸å»é‡ï¼Œåªæ˜¾ç¤ºæ¯æ¡çš„ dur_msï¼‰
            # f.write(f"## ğŸ”¥ Run[2] Kernelsï¼ˆæŒ‰å‡ºç°é¡ºåºï¼‰\n\n")
            total_kernel_time_ms = sum(k.get('dur_ms', 0.0) for k in self.hot_kernels)
            for i, kernel in enumerate(self.hot_kernels, 1):
                display_name = str(kernel.get('name',''))
                dur = float(kernel.get('dur_ms', 0.0))
                percent = (dur / total_kernel_time_ms * 100.0) if total_kernel_time_ms > 0 else 0.0
                f.write(f"{i}. {display_name}\n")
                f.write(f"   - æ‰§è¡Œæ—¶é—´: {dur:.3f} ms\n")
                f.write(f"   - æ—¶é—´å æ¯”: {percent:.2f}%\n\n")
            
            # NCU æ·±åº¦åˆ†æ
            f.write("äºŒã€ NCU æ·±åº¦åˆ†æç»“æœ\n\n")
            ncu_analysis = comprehensive_results.get('ncu_detailed_analysis', {})
            focus_analysis = comprehensive_results.get('ncu_focus_analysis', {})
            
            # å†™å…¥é€ kernel è¯¦ç»†
            items = []
            for kernel_name, analysis in ncu_analysis.items():
                m = re.match(r'^ncu_kernel_(\d+)_', kernel_name)
                idx = int(m.group(1)) if m else 10**9  # æ— å‰ç¼€çš„æ”¾åœ¨æœ€å
                items.append((idx, kernel_name, analysis))
            items.sort(key=lambda x: x[0])

            for i, (idx, kernel_name, analysis) in enumerate(items, 1):
                # æ ‡é¢˜æ”¹ä¸ºç¼–å·è¡Œ
                f.write(f"{i}. {kernel_name}\n\n")
                # åŸºæœ¬ç»Ÿè®¡
                f.write(f"   - è¯†åˆ«ç“¶é¢ˆæ•°: {analysis.get('bottlenecks_found', 0)}\n")
                # GPU åˆ©ç”¨ç‡
                gu = analysis.get('gpu_utilization', {})
                if gu:
                    f.write(f"   - å¹³å‡SMæ•ˆç‡: {gu.get('average_sm_efficiency', 'N/A')}\n")
                    f.write(f"   - æœ€é«˜SMæ•ˆç‡: {gu.get('max_sm_efficiency', 'N/A')}\n")
                    f.write(f"   - æœ€ä½SMæ•ˆç‡: {gu.get('min_sm_efficiency', 'N/A')}\n")
                    f.write(f"   - ä½äº50%æ•°é‡: {gu.get('kernels_below_50_percent', 0)} / {gu.get('total_kernels', 0)}\n")
                # å†…å­˜åˆ†æ
                mem = analysis.get('memory_analysis', {})
                bw = mem.get('bandwidth_stats', {})
                if bw:
                    f.write(f"   - å¹³å‡å¸¦å®½: {bw.get('average_bandwidth', 'N/A')} GB/s\n")
                    f.write(f"   - æœ€é«˜å¸¦å®½: {bw.get('max_bandwidth', 'N/A')} GB/s\n")
                    f.write(f"   - æœ€ä½å¸¦å®½: {bw.get('min_bandwidth', 'N/A')} GB/s\n")
                l2 = mem.get('l2_cache_stats', {})
                if l2:
                    f.write(f"   - å¹³å‡L2å‘½ä¸­ç‡: {l2.get('average_l2_hit_rate', 'N/A')}\n")
                    f.write(f"   - ä½L2å‘½ä¸­ç‡kernelæ•°: {l2.get('kernels_low_l2_hit_rate', 0)}\n")
                l1 = mem.get('l1_cache_stats', {})
                if l1:
                    f.write(f"   - å¹³å‡L1å‘½ä¸­ç‡: {l1.get('average_l1_hit_rate', 'N/A')}\n")
                    f.write(f"   - ä½L1å‘½ä¸­ç‡kernelæ•°: {l1.get('kernels_low_l1_hit_rate', 0)}\n")
                # ç“¶é¢ˆ
                bsum = analysis.get('bottleneck_summary', [])
                if bsum:
                    f.write("   - ä¸»è¦ç“¶é¢ˆ:\n")
                    for b in bsum:
                        f.write(f"     - {b.get('description','')} ({b.get('severity','')})\n")
                f.write("\n")

            # å†™å…¥ç„¦ç‚¹èšåˆï¼ˆè‹¥æœ‰ï¼‰
            if focus_analysis:
                f.write("## ğŸ¯ ç„¦ç‚¹å†…æ ¸èšåˆæŒ‡æ ‡ (å…¨é‡é‡‡é›†æå–)\n\n")
                for kname, fan in focus_analysis.items():
                    f.write(f"### {kname}\n\n")
                    gu = fan.get('gpu_utilization', {})
                    if gu:
                        f.write(f"- å¹³å‡SMæ•ˆç‡: {gu.get('average_sm_efficiency','N/A')}\n")
                        f.write(f"- Occupancy: {gu.get('achieved_occupancy','N/A')}\n")
                    mem = fan.get('memory_analysis', {}).get('bandwidth_stats', {})
                    if mem:
                        f.write(f"- å¹³å‡å¸¦å®½: {mem.get('average_bandwidth','N/A')} GB/s\n")
                        f.write(f"- L2å‘½ä¸­ç‡: {mem.get('l2_hit_rate','N/A')}%\n")
                    bsum = fan.get('bottleneck_summary', [])
                    if bsum:
                        f.write("- ä¸»è¦ç“¶é¢ˆ:\n")
                        for b in bsum:
                            f.write(f"  - {b.get('description','')} ({b.get('severity','')})\n")
                    f.write("\n")
                
            # æ•°æ®é©±åŠ¨çš„ä¼˜åŒ–å»ºè®®
            # f.write("## ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼ˆåŸºäºæ•°æ®ï¼‰\n\n")
            # agg = _collect_ncu_aggregates(ncu_analysis, focus_analysis)
            # def _fmt_avg(vals, unit=""):
            #     return f"{(sum(vals)/len(vals)):.1f}{unit}" if vals else "N/A"
            # avg_sm = _fmt_avg(agg['avg_sm'], "%")
            # min_sm = f"{min(agg['min_sm']):.1f}%" if agg['min_sm'] else "N/A"
            # avg_bw = _fmt_avg(agg['avg_bw'], " GB/s")
            # min_bw = f"{min(agg['min_bw']):.1f} GB/s" if agg['min_bw'] else "N/A"
            # avg_l2 = _fmt_avg(agg['avg_l2'], "%")
            # avg_l1 = _fmt_avg(agg['avg_l1'], "%")
            # occ_eff = _fmt_avg(agg['occ_eff_ratio'], "")  # æ¯”ç‡
            # low_sm_info = f"{agg['low_sm_cnt']} / {agg['total_sm_cnt']}" if agg['total_sm_cnt'] else "N/A"

            # # å»ºè®®é¡¹
            # suggestions = []
            # if agg['total_sm_cnt'] and agg['low_sm_cnt'] / max(1, agg['total_sm_cnt']) > 0.2:
            #     suggestions.append(f"- å¤šä¸ª Kernel çš„ SM æ•ˆç‡ä½äº 50%ï¼ˆ{low_sm_info}ï¼‰ï¼Œå»ºè®®ï¼šå¢åŠ å¹¶è¡Œåº¦/æ‰¹æ¬¡ï¼Œæ£€æŸ¥å†…æ ¸ç®—æ³•ä¸è®¿å­˜æ¨¡å¼ã€‚")
            # if agg['avg_bw'] and (sum(agg['avg_bw'])/len(agg['avg_bw'])) < 150:
            #     suggestions.append(f"- DRAM å¸¦å®½å‡å€¼åä½ï¼ˆå‡å€¼ {avg_bw}, æœ€ä½ {min_bw}ï¼‰ï¼Œå»ºè®®ï¼šæé«˜è®¿é—®é¡ºåºæ€§ä¸åˆå¹¶è®¿é—®ï¼Œå‡å°‘éšæœº/å°æ­¥é•¿è®¿å­˜ã€‚")
            # if agg['avg_l2'] and (sum(agg['avg_l2'])/len(agg['avg_l2'])) < 70:
            #     suggestions.append(f"- L2 å‘½ä¸­ç‡åä½ï¼ˆå‡å€¼ {avg_l2}ï¼‰ï¼Œå»ºè®®ï¼šæå‡æ•°æ®å±€éƒ¨æ€§ã€ä¼˜åŒ– tile/block å°ºå¯¸ï¼Œå‡å°‘è·¨å—ä¸è¿ç»­è®¿é—®ã€‚")
            # if agg['occ_eff_ratio'] and (sum(agg['occ_eff_ratio'])/len(agg['occ_eff_ratio'])) < 0.8:
            #     suggestions.append("- å ç”¨ç‡æ•ˆç‡ä½äº 80%ï¼Œå»ºè®®ï¼šé™ä½æ¯çº¿ç¨‹å¯„å­˜å™¨/å…±äº«å†…å­˜å ç”¨ï¼Œè°ƒæ•´ block/gridï¼Œæé«˜å¹¶å‘ã€‚")
            # if 'memory' in agg['bottleneck_types']:
            #     suggestions.append("- æ£€æµ‹åˆ°å†…å­˜ç“¶é¢ˆï¼Œå»ºè®®ï¼šä½¿ç”¨æ›´å¤§ tileã€å…±äº«å†…å­˜ç¼“å­˜çƒ­ç‚¹æ•°æ®ã€å‡å°‘ bank å†²çªã€‚")
            # if 'compute' in agg['bottleneck_types']:
            #     suggestions.append("- å­˜åœ¨è®¡ç®—ç“¶é¢ˆï¼Œå»ºè®®ï¼šä½¿ç”¨ Tensor Coreï¼ˆFP16/BF16ï¼‰æˆ–ä¼˜åŒ–ç®—å­å®ç°ï¼ˆCUTLASS/TRITONï¼‰ã€‚")
            # if 'latency' in agg['bottleneck_types']:
            #     suggestions.append("- å¯èƒ½å­˜åœ¨å»¶è¿Ÿç“¶é¢ˆï¼Œå»ºè®®ï¼šå¢åŠ å¹¶è¡Œé‡å ï¼ˆæ•°æ®æ¬è¿ä¸è®¡ç®—ï¼‰ã€å‡å°‘ä¸²è¡ŒåŒæ­¥ã€‚")

            # if not suggestions:
            #     suggestions.append("- æŒ‡æ ‡æ€»ä½“æ­£å¸¸ï¼›å¯é’ˆå¯¹ä¸ªåˆ«ä½æ•ˆ Kernel å¾®è°ƒ block/grid å½¢çŠ¶ä¸è®¿å­˜ã€‚")

            # # æ¦‚è§ˆè¡Œ
            # f.write(f"- æ¦‚è§ˆï¼šSMå‡å€¼ {avg_sm}, æœ€ä½ {min_sm}; å¸¦å®½å‡å€¼ {avg_bw}, æœ€ä½ {min_bw}; L2å‡å€¼ {avg_l2}, L1å‡å€¼ {avg_l1}\n")
            # f.write("\n".join(suggestions) + "\n")

        print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)

def create_sglang_analysis_workflow():
    """åˆ›å»ºSGlangä¸“ç”¨çš„åˆ†æå·¥ä½œæµ"""
    DEFAULT_MODEL_DIR = os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH') or '/workspace/models/'

    def run_sglang_integrated_analysis(model_path: Optional[str] = None, 
                                      batch_size: int = 1,
                                      input_len: int = 128, 
                                      output_len: int = 1,
                                      disable_chunked_prefill: bool = True,
                                      gpu_ids: Optional[List[str]] = None):
        print(f"[DEBUG] run_sglang_integrated_analysis entered: bs={batch_size}, in={input_len}, out={output_len}")
        """è¿è¡ŒSGlangçš„é›†æˆåˆ†æï¼ˆå›ºå®šè®¾ç½®ï¼šbs=1, in=128, out=1ï¼‰ï¼Œå¯åœ¨å¤šä¸ªGPUä¸Šé¡ºåºæ‰§è¡Œ"""
        if not model_path:
            model_path = DEFAULT_MODEL_DIR.rstrip('/')
            print(f"â„¹ï¸ æœªæä¾› model_pathï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {model_path}")

        gpu_list = gpu_ids or ["0", "1"]
        base_env = os.environ.copy()
        base_env['SGLANG_ENABLE_CHUNKED_PREFILL'] = '0'

        outputs: List[Dict[str, str]] = []
        for gpu_id in gpu_list:
            env = base_env.copy()
            env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
            print(f"ğŸ” åœ¨ GPU {gpu_id} ä¸Šè¿è¡Œ nsys/ncu æµç¨‹ ...")
            sglang_cmd = [
                'python', '-m', 'sglang.bench_one_batch',
                '--model-path', model_path,
                '--batch-size', str(batch_size),
                '--input-len', str(input_len),
                '--output-len', str(output_len),
                '--load-format', 'dummy',
                '--chunked-prefill-size', '0',
                '--disable-cuda-graph'
            ]

            analyzer = NSysToNCUAnalyzer(
                f"sglang_analysis_b{batch_size}_i{input_len}_o{output_len}_gpu{gpu_id}",
                env=env,
            )
            nsys_file = analyzer.step1_nsys_analysis(sglang_cmd, "sglang_overview")
            hot = analyzer.step2_extract_hot_kernels(nsys_file, top_k=8)
            if not hot:
                print("âŒ æœªå‘ç°çƒ­ç‚¹kernelsï¼Œåˆ†æç»ˆæ­¢");
                continue
            ncu_files = analyzer.step3_ncu_targeted_analysis(sglang_cmd, hot, max_kernels=len(hot))
            results = analyzer.step4_comprehensive_analysis(ncu_files)
            report_file = analyzer.generate_final_report(results)
            outputs.append({
                "gpu": str(gpu_id),
                "dir": str(analyzer.output_dir)
            })
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {analyzer.output_dir}\nğŸ“„ æŠ¥å‘Š: {report_file}")

        return outputs
    return run_sglang_integrated_analysis

# --- è¾…åŠ©å‡½æ•°: å°†é«˜é˜¶æŠ¥å‘Š Markdown ç²—ç•¥ç»“æ„åŒ–ä¸º JSON ---
def _extract_advanced_json(md_text: str) -> Dict[str, Any]:  # type: ignore
    sections: Dict[str, Any] = {}
    current = None
    for line in md_text.splitlines():
        if line.startswith('#'):
            # è·å–æ ‡é¢˜
            title = line.strip('# ').strip()
            current = title
            sections[current] = []
        else:
            if current is not None:
                sections[current].append(line)
    # ç®€å•æŠ½å–ä»»åŠ¡åˆ—è¡¨ä¸åˆ†ç±»
    tasks = []
    for k, v in sections.items():
        if 'ä»»åŠ¡åˆ—è¡¨' in k or 'ç»†ç²’åº¦' in k:
            tasks.extend([ln for ln in v if ln.strip().startswith('- ')])
    summary = sections.get('6. æ€»ç»“ (Summary)', [])
    return {
        'sections': list(sections.keys()),
        'tasks_lines': tasks,
        'summary': '\n'.join(summary[:10]),
        'raw_length': len(md_text)
    }

def main():
    import argparse, os
    parser = argparse.ArgumentParser(description='é›†æˆ nsys å’Œ ncu çš„æ€§èƒ½åˆ†æå·¥å…·')
    # SGlang å‚æ•°
    parser.add_argument('--sglang-model', type=str, default=os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH'),
                        help='SGlangæ¨¡å‹è·¯å¾„')
    parser.add_argument('--sglang-batch', type=int, default=1, help='SGlangæ‰¹æ¬¡å¤§å°')
    parser.add_argument('--sglang-input-len', type=int, default=128, help='SGlangè¾“å…¥é•¿åº¦')
    parser.add_argument('--sglang-output-len', type=int, default=1, help='SGlangè¾“å‡ºé•¿åº¦')
    # å…¼å®¹ï¼šä¿ç•™æœªçŸ¥å‚æ•°ä½†ä¸ä½¿ç”¨
    known_args, unknown_tail = parser.parse_known_args()
    # å¿½ç•¥ unknown_tailï¼Œç»Ÿä¸€èµ°å·¥ä½œæµ
    if unknown_tail:
        print(f"[WARN] å¿½ç•¥åŸå§‹ç›®æ ‡å‘½ä»¤ï¼ˆunknown_tailï¼‰ï¼š{' '.join(unknown_tail)}")

    try:
        run_workflow = create_sglang_analysis_workflow()
        print("[DEBUG] è°ƒç”¨ create_sglang_analysis_workflow()")
        run_workflow(
            model_path=known_args.sglang_model,
            batch_size=known_args.sglang_batch,
            input_len=known_args.sglang_input_len,
            output_len=known_args.sglang_output_len,
            disable_chunked_prefill=True
        )
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback; traceback.print_exc()

if __name__ == "__main__":
    main()

