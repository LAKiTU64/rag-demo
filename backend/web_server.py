#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Agent WebæœåŠ¡å™¨
"""

import os
import sys
import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, Any, Iterable
from pydantic import BaseModel
ROOT_DIR = Path(__file__).resolve().parent.parent
BACKEND_DIR = ROOT_DIR / "backend"
UTILS_DIR = BACKEND_DIR / "utils"

for path in (ROOT_DIR, BACKEND_DIR, UTILS_DIR):
    str_path = str(path)
    if str_path not in sys.path:
        sys.path.insert(0, str_path)

from utils.nsys_to_ncu_analyzer import NSysToNCUAnalyzer
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, UploadFile, File, Body, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import yaml
from backend.offline_llm import get_offline_qwen_client

# å¯¼å…¥AI Agentæ ¸å¿ƒ & çŸ¥è¯†åº“æ‘„å–
try:
    from backend.agent_core import AIAgent
except Exception as e:
    print(f"æ— æ³•å¯¼å…¥ AIAgent: {e}")
try:
    from backend.knowledge_bases.kb_ingest import ingest_json_to_faiss, ingest_model_config
except Exception as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥çŸ¥è¯†åº“æ‘„å–æ¨¡å—: {e}")
    ingest_json_to_faiss = None  # type: ignore
    ingest_model_config = None  # type: ignore
class FinalReportRequest(BaseModel):
    job_id: str
    extra_query: Optional[str] = None

class SGLangAnalyzeRequest(BaseModel):
    """è¯·æ±‚: ä¸€æ¬¡æ€§è§¦å‘ sglang nsys+ncu å¿«é€Ÿåˆ†æã€‚

    ä»…ç”¨äºåŒæ­¥æ¥å£ /analyze/sglang (ä¸ç”Ÿæˆé«˜çº§/å¢å¼ºæŠ¥å‘Š)ã€‚"""
    model_path: str
    batch_size: int = 1
    input_len: int = 128
    output_len: int = 1
    top_k: int = 20
    min_duration_ms: float = 1.0
    max_ncu_kernels: int = 10

class AnalysisSubmitRequest(BaseModel):
    """æäº¤å¼‚æ­¥åˆ†æä½œä¸šçš„è¯·æ±‚ä½“ã€‚

    æ”¯æŒé™„åŠ ç”Ÿæˆé«˜çº§ / å¢å¼ºæŠ¥å‘Šï¼Œä»¥åŠè¿œç¨‹ä»£ç ä¿¡ä»»ç­‰å‚æ•°ã€‚"""
    model_path: str
    batch_size: int = 1
    input_len: int = 128
    output_len: int = 1
    top_k: int = 20
    min_duration_ms: float = 1.0
    max_ncu_kernels: int = 10
    allow_remote_code: bool = False
    advanced: bool = False
    advanced_detailed: bool = False
    advanced_json: bool = False
    generate_enriched: bool = False
    ingest_advanced: bool = False
    kb_path: Optional[str] = None
    note: Optional[str] = None

class FullAnalysisRequest(BaseModel):
    """ç«¯åˆ°ç«¯ç»¼åˆåˆ†æè¯·æ±‚ä½“ã€‚

    ç”¨äº /analysis/full æ¥å£: nsys + ncu + åŸºç¡€æŠ¥å‘Š + (å¯é€‰) é«˜é˜¶/å¢å¼ºæŠ¥å‘Š + (å¯é€‰) KB æ‘„å–ã€‚"""
    model_path: str
    batch_size: int = 1
    input_len: int = 128
    output_len: int = 1
    top_k: int = 20
    min_duration_ms: float = 1.0
    max_ncu_kernels: int = 10
    allow_remote_code: bool = False
    advanced: bool = False
    advanced_detailed: bool = False
    advanced_json: bool = False
    generate_enriched: bool = False
    ingest_advanced: bool = False
    kb_path: Optional[str] = None
    note: Optional[str] = None

# åŠ è½½é…ç½®
config_path = Path(__file__).parent.parent / "config.yaml"
with open(config_path, 'r', encoding='utf-8') as f:
    CONFIG = yaml.safe_load(f)

app = FastAPI(
    title="AI Agent LLMæ€§èƒ½åˆ†æå™¨",
    description="æ™ºèƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½åˆ†æWebæœåŠ¡",
    version="1.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é™æ€æ–‡ä»¶æœåŠ¡
frontend_dir = Path(__file__).parent.parent / "frontend"
if frontend_dir.exists():
    app.mount("/static", StaticFiles(directory=str(frontend_dir)), name="static")

agent = None
active_connections: Dict[str, WebSocket] = {}
OFFLINE_QWEN_PATH = Path(os.getenv("QWEN_LOCAL_MODEL_PATH", "/workspace/Qwen3-32B"))


class AnalysisJob:
    def __init__(self, job_id: str, params: Dict[str, Any]):
        self.job_id = job_id
        self.params = params
        self.status = 'pending'
        self.error: Optional[str] = None
        self.output_dir: Optional[str] = None
        self.artifacts: Dict[str, Any] = {}
        self.started_at = datetime.now().isoformat()
        self.finished_at: Optional[str] = None

class JobManager:
    def __init__(self):
        self.jobs: Dict[str, AnalysisJob] = {}
    def create(self, params: Dict[str, Any]) -> AnalysisJob:
        import uuid
        jid = uuid.uuid4().hex[:12]
        job = AnalysisJob(jid, params)
        self.jobs[jid] = job
        return job
    def get(self, job_id: str) -> Optional[AnalysisJob]:
        return self.jobs.get(job_id)

job_manager = JobManager()

API_LABELS = {
    "auto": "æ™ºèƒ½æ¨è",
    "langchain": "LangChain Agent",
    "nsys": "NSys æ€§èƒ½åˆ†æ",
    "ncu": "NCU æ·±åº¦åˆ†æ",
    "custom": "è‡ªå®šä¹‰å·¥å…·é“¾"
}

class ConnectionManager:
    """WebSocketè¿æ¥ç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
    
    async def connect(self, websocket: WebSocket, session_id: str):
        await websocket.accept()
        self.active_connections[session_id] = websocket
        print(f"ğŸ”— è¿æ¥å»ºç«‹: {session_id}")
    
    def disconnect(self, session_id: str):
        if session_id in self.active_connections:
            del self.active_connections[session_id]
            print(f"âŒ è¿æ¥æ–­å¼€: {session_id}")
    
    async def send_message(self, session_id: str, message: dict):
        if session_id in self.active_connections:
            try:
                await self.active_connections[session_id].send_text(json.dumps(message))
            except Exception as e:
                print(f"å‘é€æ¶ˆæ¯å¤±è´¥: {e}")
                self.disconnect(session_id)

manager = ConnectionManager()

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global agent
    if AIAgent:
        try:
            agent = AIAgent(CONFIG)
            print("âœ… AI Agentåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ AI Agentåˆå§‹åŒ–å¤±è´¥: {e}")
    
    print("ğŸ¤– AI Agent WebæœåŠ¡å™¨å¯åŠ¨å®Œæˆ")
    print(f"ğŸ“¡ æœåŠ¡åœ°å€: http://{CONFIG['server']['host']}:{CONFIG['server']['port']}")

@app.get("/", response_class=HTMLResponse)
async def root():
    """ä¸»é¡µ"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>AI Agent LLMæ€§èƒ½åˆ†æå™¨</title>
        <meta charset="utf-8">
    </head>
    <body>
        <h1>ğŸ¤– AI Agent LLMæ€§èƒ½åˆ†æå™¨</h1>
        <p>è¯·è®¿é—® <a href="/chat">/chat</a> å¼€å§‹ä½¿ç”¨</p>
        <p>APIæ–‡æ¡£: <a href="/docs">/docs</a></p>
    </body>
    </html>
    """

@app.get("/chat", response_class=HTMLResponse)
async def chat_page():
    """èŠå¤©é¡µé¢"""
    chat_file = Path(__file__).parent.parent / "frontend" / "chat.html"
    if chat_file.exists():
        return chat_file.read_text(encoding='utf-8')
    else:
        return HTMLResponse(
            content="<h1>èŠå¤©é¡µé¢æœªæ‰¾åˆ°</h1>",
            status_code=404
        )

@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """WebSocketè¿æ¥ç«¯ç‚¹"""
    await manager.connect(websocket, session_id)
    
    # å‘é€æ¬¢è¿æ¶ˆæ¯
    await manager.send_message(session_id, {
        "type": "assistant_message",
        "content": """ğŸ¤– **æ¬¢è¿ä½¿ç”¨AI Agent LLMæ€§èƒ½åˆ†æå™¨ï¼**

æˆ‘å¯ä»¥å¸®æ‚¨ï¼š
â€¢ ğŸ” åˆ†æå„ç§LLMæ¨¡å‹çš„æ€§èƒ½
â€¢ ğŸ“Š è¿›è¡ŒNSyså…¨å±€æ€§èƒ½åˆ†æ
â€¢ ğŸ”¬ æ‰§è¡ŒNCUæ·±åº¦kernelåˆ†æ
â€¢ ğŸ’¡ æä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®

è¯·å‘Šè¯‰æˆ‘æ‚¨çš„åˆ†æéœ€æ±‚ï¼ä¾‹å¦‚ï¼š
"åˆ†æ llama-7b æ¨¡å‹ï¼Œbatch_size=1"
""",
        "timestamp": datetime.now().isoformat()
    })
    
    try:
        while True:
            data = await websocket.receive_text()
            message_data = json.loads(data)
            
            await handle_websocket_message(session_id, message_data)
            
    except WebSocketDisconnect:
        manager.disconnect(session_id)
    except Exception as e:
        print(f"WebSocketé”™è¯¯: {e}")
        manager.disconnect(session_id)

async def handle_websocket_message(session_id: str, message_data: dict):
    """å¤„ç†WebSocketæ¶ˆæ¯"""
    
    message_type = message_data.get("type", "")
    content = message_data.get("content", "")
    api_choice = message_data.get("api", "auto")
    
    if message_type == "user_message":
        await process_user_message(session_id, content, api_choice)
    
    elif message_type == "ping":
        await manager.send_message(session_id, {
            "type": "pong",
            "timestamp": datetime.now().isoformat()
        })

def resolve_api_selection(api: str) -> str:
    if api == "auto":
        return "langchain" if agent else "nsys"
    return api or "langchain"


async def process_user_message(session_id: str, message: str, api: str = "auto"):
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
    
    try:
        resolved_api = resolve_api_selection(api)
        api_label = API_LABELS.get(resolved_api, resolved_api)

        await manager.send_message(session_id, {
            "type": "assistant_message",
            "content": (
                "ğŸ”„ æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚\n\n"
                f"â€¢ é€‰æ‹©çš„API: **{api_label}**\n"
                f"â€¢ è¯·æ±‚å†…å®¹: {message}"
            ),
            "timestamp": datetime.now().isoformat()
        })
        
        response = await dispatch_api_request(resolved_api, message)
        
        if resolved_api == "langchain" and agent is not None:
            latest_dir = getattr(agent, "last_analysis_dir", None)
            if latest_dir:
                global last_analysis_dir
                last_analysis_dir = str(latest_dir)

        await manager.send_message(session_id, {
            "type": "assistant_message",
            "content": response,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        await manager.send_message(session_id, {
            "type": "error",
            "content": f"âŒ å¤„ç†å¤±è´¥: {str(e)}",
            "timestamp": datetime.now().isoformat()
        })


async def dispatch_api_request(api: str, message: str) -> str:
    if api == "langchain":
        if agent:
            return await agent.process_message(message)
        return generate_placeholder_response("langchain", message)
    if api == "nsys":
        return generate_placeholder_response("nsys", message)
    if api == "ncu":
        return generate_placeholder_response("ncu", message)
    if api == "custom":
        return generate_placeholder_response("custom", message)
    if api == "auto":
        # ç†è®ºä¸Šä¸ä¼šèµ°åˆ°è¿™é‡Œï¼Œä½†ä¿åº•å¤„ç†
        return await dispatch_api_request(resolve_api_selection(api), message)
    return generate_placeholder_response("unknown", message)


def generate_placeholder_response(api: str, message: str) -> str:
    if api == "langchain":
        return f"""âœ… å·²æ¥æ”¶åˆ°æ‚¨çš„è¯·æ±‚

**è¯·æ±‚å†…å®¹**: {message}

ğŸ“‹ **è§£æç»“æœ**:
â€¢ è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå“åº”ï¼ˆæœªåŠ è½½LangChain Agentï¼‰
â€¢ å®Œæ•´åŠŸèƒ½éœ€è¦é…ç½®SGlangå’Œæ¨¡å‹è·¯å¾„
â€¢ è¯·æŸ¥çœ‹ config.yaml è¿›è¡Œé…ç½®

ğŸ’¡ **ä¸‹ä¸€æ­¥**:
1. é…ç½® config.yaml ä¸­çš„è·¯å¾„
2. ç¡®ä¿SGlangå·²å®‰è£…
3. å‡†å¤‡å¥½æ¨¡å‹æ–‡ä»¶
4. é‡æ–°å¯åŠ¨æœåŠ¡

è¯¦ç»†é…ç½®è¯´æ˜è¯·æŸ¥çœ‹ README.md
"""
    if api == "nsys":
        return f"""ğŸ§ª **NSys æ€§èƒ½åˆ†æå·¥ä½œæµ (ç¤ºä¾‹)**

**è¯·æ±‚å†…å®¹**: {message}

ğŸ”§ å»ºè®®æ­¥éª¤:
1. ä½¿ç”¨ `nsys profile` æ”¶é›†å…¨å±€æ€§èƒ½æ•°æ®
2. å°†ç”Ÿæˆçš„ `.nsys-rep` æ–‡ä»¶ä¸Šä¼ è‡³ `analysis_results/`
3. è¿è¡Œ `utils/nsys_to_ncu_analyzer.py` è·å–ç“¶é¢ˆå‡½æ•°
4. è‹¥éœ€è¦æ·±åº¦åˆ†æï¼Œåˆ‡æ¢åˆ° NCU API ç»§ç»­

ğŸ“˜ è¯¦ç»†è¯´æ˜: è¯·å‚è€ƒ `configs_and_docs/README_AI_Agent.md` ä¸­çš„ NSys éƒ¨åˆ†
"""
    if api == "ncu":
        return f"""ğŸ”¬ **NCU æ·±åº¦åˆ†æ (ç¤ºä¾‹)**

**è¯·æ±‚å†…å®¹**: {message}

ğŸ”§ å»ºè®®æ­¥éª¤:
1. åŸºäº NSys è¾“å‡ºå®šä½çƒ­ç‚¹ kernel
2. ä½¿ç”¨ `ncu --set full` é’ˆå¯¹çƒ­ç‚¹ kernel æ”¶é›†æ•°æ®
3. å°†è¾“å‡ºå¯¼å…¥ `utils/nsys_to_ncu_analyzer.py` ç”Ÿæˆç»¼åˆæŠ¥å‘Š
4. å¯¹æ¯”ä¸åŒ batch/input å‚æ•°ä¸‹çš„ç“¶é¢ˆ

ğŸ“˜ ä½¿ç”¨æŒ‡å—: è¯·å‚è€ƒ `configs_and_docs/README_AI_Agent.md` ä¸­çš„ NCU éƒ¨åˆ†
"""
    if api == "custom":
        return f"""ğŸ› ï¸ **è‡ªå®šä¹‰å·¥å…·é“¾å·¥ä½œæµ**

**è¯·æ±‚å†…å®¹**: {message}

å¯ä»¥åœ¨ `backend/` ç›®å½•ä¸‹æ–°å¢è‡ªå®šä¹‰å¤„ç†é€»è¾‘ï¼Œä¾‹å¦‚:
â€¢ é›†æˆå†…éƒ¨ç›‘æ§æˆ–å‘Šè­¦ç³»ç»Ÿ
â€¢ è°ƒç”¨ä¼ä¸šå†…éƒ¨ LangChain å·¥å…·
â€¢ ç»“åˆ FAISS / Chroma çŸ¥è¯†åº“æ£€ç´¢

ğŸ’¡ æç¤º: æ–°å¢çš„å¤„ç†å‡½æ•°å¯åœ¨ `dispatch_api_request` ä¸­æ³¨å†Œ
"""
    return f"""â„¹ï¸ å½“å‰é€‰æ‹©çš„ API ({api}) æš‚æœªå®ç°å®šåˆ¶é€»è¾‘ã€‚

æ‚¨å¯ä»¥åœ¨ `backend/web_server.py` ä¸­æ‰©å±• `dispatch_api_request` æ¥æ»¡è¶³ç‰¹å®šéœ€æ±‚ã€‚

åŸå§‹è¯·æ±‚: {message}
"""

@app.post("/upload_config")
async def upload_config(file: UploadFile = File(...)):
    """ä¸Šä¼ é…ç½®æ–‡ä»¶"""
    
    try:
        content = await file.read()
        content_str = content.decode('utf-8')
        
        # è§£æé…ç½®
        if file.filename.endswith('.json'):
            config_data = json.loads(content_str)
        elif file.filename.endswith(('.yaml', '.yml')):
            config_data = yaml.safe_load(content_str)
        else:
            return {"error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"}
        
        return {
            "filename": file.filename,
            "message": "é…ç½®æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
            "config": config_data
        }
        
    except Exception as e:
        return {"error": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}

@app.post("/knowledge/upload")
async def upload_knowledge_json(
    request: Request,
    file: UploadFile = File(None),
    raw_json: Optional[str] = Body(None, description="å­—ç¬¦ä¸²å½¢å¼çš„ JSON æˆ–ç›´æ¥ä¼ é€’ JSON å¯¹è±¡"),
    embedding_model: Optional[str] = Body(None, description="å¯é€‰çš„åµŒå…¥æ¨¡å‹åï¼›æ”¯æŒ ms:<model-id> é€šè¿‡ ModelScope ä¸‹è½½ (ä¾‹å¦‚ ms:damo/nlp_gte-base-zh)"),
    force_tfidf: Optional[bool] = Body(False, description="å¼ºåˆ¶ä½¿ç”¨ TF-IDF fallback è€Œä¸åŠ è½½ä»»ä½•åµŒå…¥æ¨¡å‹"),
    segmentation_mode: Optional[str] = Body("window", description="æ–‡æœ¬åˆ‡åˆ†æ¨¡å¼: window(æ»‘çª—)/sentence(æŒ‰å¥)/auto(æœ¬åœ°åµŒå…¥è‡ªåŠ¨å¥å­)"),
    request_body: Optional[dict] = Body(None),
    debug: Optional[bool] = Body(False)
):
    """ä¸Šä¼ å¹¶æ‘„å– JSON çŸ¥è¯†åº“åˆ° FAISS

    æ”¯æŒå¤šç§æäº¤ä¸åµŒå…¥æ¥æº:
        1. multipart/form-data æ–‡ä»¶ä¸Šä¼ : file=@xxx.json (+ å¯é€‰ embedding_model)
        2. application/json æ–¹å¼: {"raw_json": "{...}"} æˆ– {"raw_json": {...}}
        3. ç›´æ¥æäº¤çº¯ JSON å¯¹è±¡ (ä¸åŒ…è£¹ raw_json), ä¾‹å¦‚: {"section": "Intro", "text": "Hello"}
        4. åµŒå…¥æ¨¡å‹æ¥æº embedding_model:
           - HuggingFace (é»˜è®¤): sentence-transformers/all-MiniLM-L6-v2 æˆ–ä»»æ„ HF åç§°
           - ModelScope: ä½¿ç”¨å‰ç¼€ ms:<model-id> (éœ€ pip install modelscope)ï¼Œç¤ºä¾‹ ms:damo/nlp_gte-base-zh
           - TF-IDF fallback: è‡ªåŠ¨åœ¨åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥æ—¶å¯ç”¨ï¼Œæˆ–è®¾ç½® force_tfidf=true (åç»­å¯æ‰©å±•)

    è¿”å›: ç´¢å¼•æ„å»ºç»“æœç»Ÿè®¡ (æˆåŠŸ) æˆ–é”™è¯¯è¯¦æƒ…
    é”™è¯¯ç :
        400 - è¾“å…¥ç¼ºå¤±æˆ–ä¸åˆæ³•
        500 - æœåŠ¡å†…éƒ¨å¼‚å¸¸ / æ‘„å–æ¨¡å—æœªåŠ è½½
    """
    if ingest_json_to_faiss is None:
        return JSONResponse(status_code=500, content={"error": "çŸ¥è¯†åº“æ‘„å–æ¨¡å—æœªåŠ è½½"})

    try:
        json_str: Optional[str] = None
        diagnostics = {}
        # ä¼˜å…ˆæ–‡ä»¶
        if file is not None:
            content_bytes = await file.read()
            try:
                json_str = content_bytes.decode('utf-8')
            except Exception:
                return JSONResponse(status_code=400, content={"error": "æ–‡ä»¶ç¼–ç è§£æå¤±è´¥ï¼Œéœ€ UTF-8"})
        else:
            # raw_json å‚æ•°å¤„ç†
            if raw_json is not None:
                if isinstance(raw_json, (dict, list)):
                    # FastAPI å¯èƒ½ä¼šæŠŠåŸå§‹ JSON æ˜ å°„ä¸º dict/list (è‹¥å‚æ•°ç±»å‹ä¸º Any)
                    try:
                        json_str = json.dumps(raw_json, ensure_ascii=False)
                    except Exception as e:
                        return JSONResponse(status_code=400, content={"error": f"raw_json åºåˆ—åŒ–å¤±è´¥: {e}"})
                else:
                    # å­—ç¬¦ä¸²å½¢å¼
                    json_str = raw_json
            elif request_body is not None:
                # æœªæ˜¾å¼æä¾› raw_json, ç›´æ¥ä½¿ç”¨ä¸»ä½“å¯¹è±¡
                try:
                    json_str = json.dumps(request_body, ensure_ascii=False)
                except Exception as e:
                    return JSONResponse(status_code=400, content={"error": f"è¯·æ±‚ä¸»ä½“åºåˆ—åŒ–å¤±è´¥: {e}"})
            else:
                # æœ€åå°è¯•ç›´æ¥è¯»å–åŸå§‹ body (è§£å†³æœªå£°æ˜å­—æ®µæ—¶ç›´æ¥ä¼ å¯¹è±¡çš„æƒ…å†µ)
                if request.headers.get("content-type", "").startswith("application/json"):
                    try:
                        raw_bytes = await request.body()
                        if raw_bytes:
                            parsed = json.loads(raw_bytes.decode('utf-8'))
                            json_str = json.dumps(parsed, ensure_ascii=False)
                            diagnostics['fallback_body_parse'] = True
                    except Exception as e:
                        return JSONResponse(status_code=400, content={"error": f"ç›´æ¥ä¸»ä½“è§£æå¤±è´¥: {e}"})

        if not json_str:
            return JSONResponse(status_code=400, content={"error": "ç¼ºå°‘ JSON å†…å®¹: è¯·æä¾› fileã€raw_json æˆ–ç›´æ¥ JSON å¯¹è±¡", "received_headers": dict(request.headers), "debug": True if debug else None})

        model_name = embedding_model or "sentence-transformers/all-MiniLM-L6-v2"
        # ç¯å¢ƒå˜é‡è§¦å‘å¼ºåˆ¶ TF-IDF (ä¼˜å…ˆçº§ä½äºæ˜ç¡®ä¼ å‚ force_tfidf)
        env_force = os.getenv('OFFLINE_FORCE_TFIDF', '0').lower() in ('1', 'true', 'yes')
        effective_force_tfidf = bool(force_tfidf) or env_force
        # å°è¯•æ‘„å–ç´¢å¼•å¹¶æ•è·æ„å»ºé˜¶æ®µå…·ä½“é”™è¯¯
        try:
            seg_mode_eff = segmentation_mode or 'window'
            if seg_mode_eff == 'auto':
                if model_name.startswith('local:') or model_name == 'local-simple':
                    seg_mode_eff = 'sentence'
                else:
                    seg_mode_eff = 'window'
            # æœ¬åœ°å“ˆå¸ŒåµŒå…¥é»˜è®¤æ”¹ä¸ºå¥å­æ¨¡å¼ï¼ˆè‹¥ç”¨æˆ·æœªæ˜ç¡®æŒ‡å®šï¼‰
            if segmentation_mode == 'window' and (model_name.startswith('local:') or model_name == 'local-simple'):
                seg_mode_eff = 'sentence'
            result = ingest_json_to_faiss(json_str, embedding_model=model_name, force_tfidf=effective_force_tfidf, segmentation_mode=seg_mode_eff)
        except Exception as ingest_exc:
            # æœªåœ¨åº•å±‚å‡½æ•°å†…è¢«å¤„ç†çš„å¼‚å¸¸
            err_payload = {
                "status": "error",
                "stage": "ingest_call_exception",
                "message": str(ingest_exc),
                "embedding_model": model_name,
                "force_tfidf": effective_force_tfidf,
                "embedding_provider": None,
            }
            return JSONResponse(status_code=500 if not debug else 400, content=err_payload)

        # è‹¥åº•å±‚è¿”å›é”™è¯¯çŠ¶æ€, ä»¥ 400 æ˜¾ç¤º
        if result.get("status") == "error":
            if debug:
                result['diagnostics'] = diagnostics
                result['embedding_model'] = model_name
                result['force_tfidf'] = effective_force_tfidf
                result['force_tfidf_used'] = effective_force_tfidf
                result['embedding_provider'] = result.get('embedding_provider')
                result['segmentation_mode_used'] = seg_mode_eff
                result['json_length'] = len(json_str)
                try:
                    parsed_tmp = json.loads(json_str)
                    if isinstance(parsed_tmp, dict):
                        result['root_keys'] = list(parsed_tmp.keys())
                except Exception:
                    pass
            return JSONResponse(status_code=400, content=result)
        if debug:
            result['diagnostics'] = diagnostics
            result['content_type'] = request.headers.get('content-type')
            result['embedding_model'] = model_name
            result['force_tfidf'] = effective_force_tfidf
            result['force_tfidf_used'] = effective_force_tfidf or (result.get('embedding_provider') == 'tfidf_fallback')
            result['embedding_provider'] = result.get('embedding_provider')
            result['segmentation_mode_used'] = seg_mode_eff
        # æ­£å¸¸è¿”å›ä¹Ÿè¡¥å…… force_tfidf_used å­—æ®µ
        result['force_tfidf_used'] = effective_force_tfidf or (result.get('embedding_provider') == 'tfidf_fallback')
        result['segmentation_mode_used'] = seg_mode_eff
        return JSONResponse(status_code=200, content=result)

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"æ‘„å–å¤±è´¥: {e}"})

@app.get("/report/full")
async def generate_full_report():
    """ç”Ÿæˆå¢å¼ºç‰ˆæ€§èƒ½æŠ¥å‘Š (ç»“åˆç†è®ºçŸ¥è¯†åº“)ã€‚"""
    try:
        from backend.report_generator import generate_enriched_report
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"æŠ¥å‘Šç”Ÿæˆæ¨¡å—ä¸å¯ç”¨: {e}"})
    # é€‰æ‹©ç›®æ ‡ç›®å½•
    target_dir = last_analysis_dir or "/workspace/Agent/AI_Agent_Complete"
    from pathlib import Path
    try:
        enriched = generate_enriched_report(Path(target_dir))
        return {"status": "ok", "enriched_report": enriched, "output_dir": target_dir}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"ç”Ÿæˆå¤±è´¥: {e}"})

@app.get("/report/advanced")
async def generate_advanced(detailed: bool = False, ncu_metrics: bool = False):
    """ç”Ÿæˆé«˜é˜¶ä¼˜åŒ–å»ºè®®æŠ¥å‘Š (ä¸è¦æ±‚ç†è®ºç´¢å¼•å­˜åœ¨)

    å‚æ•°:
        detailed: è§£æ comprehensive_analysis.json ç”Ÿæˆå…³é”®æŒ‡æ ‡å¿«ç…§ä¸ granular kernel tasks
        ncu_metrics: é¢å¤–å°è¯•è§£æ NCU CSV æ–‡ä»¶ (è‹¥å­˜åœ¨) æå– SM Efficiency / Memory Bandwidth
    """
    try:
        from backend.advanced_report import generate_advanced_report
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"æ¨¡å—ä¸å¯ç”¨: {e}"})
    target_dir = last_analysis_dir or "/workspace/Agent/AI_Agent_Complete/sglang_analysis_b8_i512_o64"
    from pathlib import Path
    try:
        path = generate_advanced_report(Path(target_dir), detailed=detailed)
        extra = {}
        if ncu_metrics:
            from pathlib import Path as _P
            import glob, csv
            metrics_list = []
            for csv_file in glob.glob(str(_P(target_dir) / 'ncu_kernel_*/*.csv')):
                # Some visualization dirs; skip for now
                continue
            # flat pattern (top-level)
            for csv_file in glob.glob(str(_P(target_dir) / 'ncu_kernel_*.csv')):
                p = _P(csv_file)
                if p.stat().st_size == 0:
                    continue
                try:
                    with open(p, 'r', encoding='utf-8') as f:
                        reader = csv.reader(f)
                        header = next(reader, [])
                        rows = list(reader)[:5]
                    metrics_list.append({"file": p.name, "header": header, "sample_rows": rows})
                except Exception:
                    pass
            extra['ncu_csv_samples'] = metrics_list
        return {"status": "ok", "advanced_report": path, "output_dir": target_dir, **extra}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"ç”Ÿæˆå¤±è´¥: {e}"})

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "active_connections": len(manager.active_connections),
        "agent_ready": agent is not None,
        "config_loaded": CONFIG is not None
    }

@app.get("/config")
async def get_config():
    """è·å–é…ç½®ä¿¡æ¯"""
    return {
        "sglang_path": CONFIG.get('sglang_path'),
        "models_path": CONFIG.get('models_path'),
        "server": CONFIG.get('server'),
        "model_mappings": CONFIG.get('model_mappings', {})
    }

@app.post("/analyze/sglang")
async def analyze_sglang(req: SGLangAnalyzeRequest):
    """è§¦å‘ä¸€ä½“åŒ– nsys + ncu æ€§èƒ½åˆ†æ (SGlang ä¸“ç”¨)"""
    if NSysToNCUAnalyzer is None:
        return JSONResponse(status_code=500, content={"error": "åˆ†ææ¨¡å—æœªåŠ è½½"})
    try:
        # æ„å»º sglang å‘½ä»¤
        sglang_cmd = [
            'python', '-m', 'sglang.bench_one_batch',
            '--model-path', req.model_path,
            '--batch-size', str(req.batch_size),
            '--input-len', str(req.input_len),
            '--output-len', str(req.output_len),
            '--load-format', 'dummy'
        ]
        analyzer = NSysToNCUAnalyzer(
            f"sglang_analysis_b{req.batch_size}_i{req.input_len}_o{req.output_len}"
        )
        nsys_file = analyzer.step1_nsys_analysis(sglang_cmd, "sglang_overview")
        hot = analyzer.step2_extract_hot_kernels(
            nsys_file, top_k=req.top_k, min_duration_ms=req.min_duration_ms
        )
        full_rep, focus_metrics = analyzer.step3_ncu_global_focus(
            sglang_cmd, hot, top_focus=min(req.max_ncu_kernels, len(hot)), set_name='compute'
        )
        ncu_files = [full_rep] if full_rep else []
        results = analyzer.step4_comprehensive_analysis(ncu_files, focus_metrics=focus_metrics)
        report = analyzer.generate_final_report(results)
        global last_analysis_dir
        last_analysis_dir = str(analyzer.output_dir)
        return {
            "status": "ok",
            "output_dir": str(analyzer.output_dir),
            "report_file": report,
            "hot_kernels": hot[:10],
            "ncu_files": ncu_files,
            "focus_metrics_count": len(focus_metrics),
            "json_kernel_candidates": results.get('nsys_overview', {}).get('kernel_analysis', {}).get('unique_kernels', None)
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post('/analysis/submit')
async def submit_analysis(req: AnalysisSubmitRequest):
    """æäº¤å¼‚æ­¥æ€§èƒ½åˆ†æä½œä¸šã€‚"""
    if NSysToNCUAnalyzer is None:
        return JSONResponse(status_code=500, content={'error': 'åˆ†ææ¨¡å—æœªåŠ è½½'})
    job = job_manager.create(req.dict())
    import threading

    def _worker():
        job.status = 'running'
        try:
            analyzer = NSysToNCUAnalyzer(
                f"sglang_analysis_b{req.batch_size}_i{req.input_len}_o{req.output_len}_job_{job.job_id}"
            )
            cmd = [
                'python', '-m', 'sglang.bench_one_batch',
                '--model-path', req.model_path,
                '--batch-size', str(req.batch_size),
                '--input-len', str(req.input_len),
                '--output-len', str(req.output_len),
                '--load-format', 'dummy'
            ]
            if req.allow_remote_code:
                cmd.append('--trust-remote-code')
            nsys_rep = analyzer.step1_nsys_analysis(cmd, 'overview')
            hot = analyzer.step2_extract_hot_kernels(nsys_rep, top_k=req.top_k, min_duration_ms=req.min_duration_ms)
            full_rep, focus_metrics = analyzer.step3_ncu_global_focus(cmd, hot, top_focus=min(req.max_ncu_kernels, len(hot)), set_name='compute')
            ncu_files = [full_rep] if full_rep else []
            comp = analyzer.step4_comprehensive_analysis(ncu_files, focus_metrics=focus_metrics)
            base_report = analyzer.generate_final_report(comp)
            # é«˜é˜¶æŠ¥å‘Š
            if req.advanced:
                try:
                    from backend.advanced_report import generate_advanced_report
                    generate_advanced_report(analyzer.output_dir, detailed=req.advanced_detailed)
                except Exception as _e:
                    print(f"é«˜çº§æŠ¥å‘Šå¤±è´¥: {_e}")
            # å¢å¼ºæŠ¥å‘Š
            if req.generate_enriched:
                try:
                    from backend.report_generator import generate_enriched_report
                    generate_enriched_report(analyzer.output_dir, comprehensive=comp)
                except Exception as _e:
                    print(f"å¢å¼ºæŠ¥å‘Šå¤±è´¥: {_e}")
            job.output_dir = str(analyzer.output_dir)
            job.artifacts = {
                'hot_kernels': hot,
                'ncu_files': ncu_files,
                'focus_metrics_count': len(focus_metrics),
                'base_report': base_report
            }
            job.status = 'done'
        except Exception as e:
            job.status = 'error'
            job.error = str(e)
        finally:
            job.finished_at = datetime.now().isoformat()
    threading.Thread(target=_worker, daemon=True).start()
    return {'job_id': job.job_id, 'status': job.status}

@app.get('/analysis/status/{job_id}')
async def analysis_status(job_id: str):
    job = job_manager.get(job_id)
    if not job:
        return JSONResponse(status_code=404, content={'error': 'job not found'})
    return {
        'job_id': job.job_id,
        'status': job.status,
        'error': job.error,
        'started_at': job.started_at,
        'finished_at': job.finished_at,
        'output_dir': job.output_dir
    }

@app.post('/analysis/final_report')
async def final_report(req: FinalReportRequest):
    job = job_manager.get(req.job_id)
    if not job:
        return JSONResponse(status_code=404, content={'error': 'job not found'})
    if job.status != 'done':
        return JSONResponse(status_code=409, content={'error': f'job not finished: {job.status}'})
    from pathlib import Path as _P
    try:
        from backend.langchain_synthesis import synthesize_final_report
    except Exception as e:
        return JSONResponse(status_code=500, content={'error': f'æ— æ³•åŠ è½½ç»¼åˆç”Ÿæˆæ¨¡å—: {e}'})
    perf_dir = _P(job.output_dir)
    result = synthesize_final_report(perf_dir, extra_query_text=req.extra_query)
    return {
        'job_id': req.job_id,
        'final_report': result['markdown_path'],
        'summary': result['summary'],
        'kb_queries': list(result['kb_hits'].keys()),
        'model_info': result.get('model_info', {})
    }

@app.post("/analysis/full")
async def full_analysis(req: FullAnalysisRequest):
    """ä¸€é”®ç«¯åˆ°ç«¯åˆ†æ: nsys + ncu + åŸºç¡€æŠ¥å‘Š + é«˜é˜¶æŠ¥å‘Š + (å¯é€‰) enriched + (å¯é€‰) KB æ‘„å–

    å‰ç«¯åªéœ€æäº¤æ¨¡å‹ä¸æ‰¹æ¬¡é…ç½®ï¼Œå³è¿”å›æ‰€æœ‰äº§ç‰©è·¯å¾„ä¸å…³é”®æŒ‡æ ‡ã€‚
    """
    if NSysToNCUAnalyzer is None:
        return JSONResponse(status_code=500, content={"error": "åˆ†ææ¨¡å—æœªåŠ è½½"})
    try:
        sglang_cmd = [
            'python', '-m', 'sglang.bench_one_batch',
            '--model-path', req.model_path,
            '--batch-size', str(req.batch_size),
            '--input-len', str(req.input_len),
            '--output-len', str(req.output_len),
            '--load-format', 'dummy'
        ]
        if req.allow_remote_code:
            sglang_cmd.append('--trust-remote-code')
        analyzer = NSysToNCUAnalyzer(
            f"sglang_analysis_b{req.batch_size}_i{req.input_len}_o{req.output_len}"
        )
        # Step1-4
        nsys_file = analyzer.step1_nsys_analysis(sglang_cmd, "sglang_overview")
        hot = analyzer.step2_extract_hot_kernels(
            nsys_file, top_k=req.top_k, min_duration_ms=req.min_duration_ms
        )
        full_rep, focus_metrics = analyzer.step3_ncu_global_focus(
            sglang_cmd,
            hot,
            top_focus=min(req.max_ncu_kernels, len(hot)),
            set_name='compute'
        )
        ncu_files = [full_rep] if full_rep else []
        results = analyzer.step4_comprehensive_analysis(
            ncu_files, focus_metrics=focus_metrics
        )
        base_report = analyzer.generate_final_report(results)
        # Advanced report
        advanced_report_path = None
        advanced_json = None
        if req.advanced:
            try:
                from backend.advanced_report import generate_advanced_report
                advanced_report_path = generate_advanced_report(analyzer.output_dir, detailed=req.advanced_detailed)
                if req.advanced_json:
                    # å¤ç”¨ analyzer ä¸­çš„è¾…åŠ©å‡½æ•° (å·²åœ¨ utils è„šæœ¬é‡Œå®šä¹‰)ï¼Œæ­¤å¤„è½»é‡ re-import
                    from backend.utils.nsys_to_ncu_analyzer import _extract_advanced_json  # type: ignore
                    md_text = Path(advanced_report_path).read_text(encoding='utf-8')
                    advanced_json = _extract_advanced_json(md_text)
                    json_path = analyzer.output_dir / 'advanced_performance_report.json'
                    json_path.write_text(json.dumps(advanced_json, ensure_ascii=False, indent=2), encoding='utf-8')
            except Exception as e:
                print(f"âš ï¸ é«˜é˜¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        # Enriched report (FAISS support)
        enriched_path = None
        if req.generate_enriched:
            try:
                from backend.report_generator import generate_enriched_report
                enriched_path = generate_enriched_report(analyzer.output_dir, comprehensive=results)
            except Exception as e:
                print(f"âš ï¸ å¢å¼ºæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        # Optional KB ingestion
        kb_ingest_status = None
        if req.ingest_advanced and advanced_json:
            try:
                from backend.knowledge_bases.kb_ingest import ingest_json_to_faiss
                ingest_json_to_faiss(json.dumps(advanced_json, ensure_ascii=False), embedding_model="sentence-transformers/all-MiniLM-L6-v2", kb_path=req.kb_path or 'knowledge_store')
                kb_ingest_status = 'ok'
            except Exception as e:
                kb_ingest_status = f'failed: {e}'
        # Metrics aggregation
        metrics = {}
        try:
            from backend.perf_data_parser import aggregate_metrics
            metrics = aggregate_metrics({**results, 'hot_kernels': hot})
        except Exception as e:
            print(f"âš ï¸ æŒ‡æ ‡èšåˆå¤±è´¥: {e}")
        global last_analysis_dir
        last_analysis_dir = str(analyzer.output_dir)
        return {
            'status': 'ok',
            'output_dir': str(analyzer.output_dir),
            'base_report': base_report,
            'advanced_report': advanced_report_path,
            'enriched_report': enriched_path,
            'advanced_json_excerpt': advanced_json.get('summary') if isinstance(advanced_json, dict) else None,
            'hot_kernels_top': hot[:10],
            'ncu_files': ncu_files,
            'focus_metrics_count': len(focus_metrics),
            'metrics': metrics,
            'kb_ingest_status': kb_ingest_status,
            'note': req.note
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={'error': str(e)})

async def _summarize_report_to_table(report_path: Path) -> str:
    if not report_path.exists():
        raise FileNotFoundError(f"æŠ¥å‘Šç¼ºå¤±: {report_path}")
    report_text = report_path.read_text(encoding='utf-8')
    loop = asyncio.get_running_loop()
    client = get_offline_qwen_client(OFFLINE_QWEN_PATH)
    return await loop.run_in_executor(None, client.report_to_table, report_text)

@app.get("/report/table")
async def report_table():
    if agent is not None:
        table_md = getattr(agent, "last_analysis_table", None)
        if table_md:
            report_paths = getattr(agent, "last_analysis_reports", None)
            response = {"status": "ok", "table_markdown": table_md}
            if report_paths:
                response["report_paths"] = report_paths
                response["report_path"] = report_paths[0]
            return response

    if not last_analysis_dir:
        return JSONResponse(status_code=404, content={"error": "æš‚æ— åˆ†æç»“æœ"})
    report_path = Path(last_analysis_dir) / "integrated_performance_report.md"
    try:
        table_md = await _summarize_report_to_table(report_path)
    except FileNotFoundError as e:
        return JSONResponse(status_code=404, content={"error": str(e)})
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})
    return {"status": "ok", "table_markdown": table_md, "report_path": str(report_path)}

if __name__ == "__main__":
    # è·å–é…ç½®
    host = CONFIG.get('server', {}).get('host', '0.0.0.0')
    port = CONFIG.get('server', {}).get('port', 8000)
    
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        "web_server:app",
        host=host,
        port=port,
        reload=False,
        log_level="info"
    )

