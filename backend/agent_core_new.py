import re
import os
import asyncio
import json
from pathlib import Path
import sys
from typing import Dict, List, Optional, Tuple

import yaml

from prompt_toolkit import PromptSession
from prompt_toolkit.history import InMemoryHistory
from prompt_toolkit.formatted_text import HTML
from prompt_toolkit.styles import Style

# å¯¼å…¥åˆ†æå·¥å…· (ç¡®ä¿è·¯å¾„æ­£ç¡®)
sys.path.insert(0, str(Path(__file__).parent))

from utils.nsys_to_ncu_analyzer import create_sglang_analysis_workflow
from offline_llm import get_offline_qwen_client
from knowledge_bases.vector_kb_manager import VectorKBManager


class AIAgent:
    """AI Agentæ ¸å¿ƒç±» - V3ç‰ˆ (Intent-First: Analysis/Chat/QA)"""

    def __init__(self, config: Dict):
        self.config = config

        # sglang å’Œæ¨¡å‹è·¯å¾„
        self.sglang_path = Path(config.get("sglang_path"))
        self.models_path = Path(config.get("models_path"))
        self.model_mappings = config.get("model_mappings", {})

        # è¾“å‡ºç›®å½•
        self.results_dir = Path(config.get("output", {}).get("results_dir", "results"))
        self.results_dir.mkdir(exist_ok=True, parents=True)

        # æœ¬åœ° LLM å®¢æˆ·ç«¯
        self.offline_qwen_path = Path(config.get("offline_qwen_path"))
        self.llm_client = get_offline_qwen_client(self.offline_qwen_path)

        # åˆ†æå·¥å…·é…ç½®
        self.profiling_config = config.get("profiling_tools", {})
        self.analysis_defaults = config.get("analysis_defaults", {})

        # ç¼“å­˜
        self.last_analysis_dir: Optional[str] = None
        self.last_analysis_dirs: List[str] = []
        self.last_analysis_reports: List[str] = []
        self.last_analysis_table: Optional[str] = None

        # å‘é‡çŸ¥è¯†åº“ç›¸å…³
        self.kb = VectorKBManager()
        kb_config = config.get("vector_store", {})
        self.persist_directory = kb_config.get("persist_directory")
        self.embedding_model = kb_config.get("embedding_model")

        # å¯¹è¯å†å²ç¼“å†²åŒº
        self.chat_history: List[Dict[str, str]] = []
        self.max_history_turns = 6  # ä¿ç•™æœ€è¿‘ 6 è½®å¯¹è¯

    async def process_message(self, message: str) -> str:
        """
        Agentic-RAG ä¸»æµç¨‹ (V3 - ä¸‰è½¨å¹¶è¡Œ):
        1. [Router] æ„å›¾è¯†åˆ« (Analysis / Chat / QA)
        2. [Branch]
           - Analysis: æ‰§è¡Œå·¥å…· (Action)
           - Chat: è‡ªç”±é—²èŠ (Free Style)
           - QA: æ£€ç´¢çŸ¥è¯†åº“ (Strict RAG)
        """
        # Step 1: æ„å›¾è·¯ç”±
        try:
            decision = await self._parse_intent_three_way(message, self.chat_history)
        except Exception as e:
            return f"âŒ **æ„å›¾è¯†åˆ«å¤±è´¥**: {str(e)}"

        intent = decision.get("intent", "qa")
        response_text = ""

        # Step 2: åˆ†æ”¯å¤„ç†
        if intent == "analysis":
            # === åˆ†æ”¯ A: æ€§èƒ½åˆ†æ (Action) ===
            print(
                f"[DEBUG] è¯†åˆ«ä¸ºåˆ†ææ„å›¾: æ¨¡å‹={decision.get('model')}, å‚æ•°={decision.get('params')}"
            )
            try:
                analysis_result = await self._execute_analysis_flow(
                    model_name=decision.get("model"),
                    analysis_type=decision.get("analysis_type", "auto"),
                    params=decision.get("params", {}),
                )
                response_text = analysis_result
            except Exception as e:
                response_text = f"âŒ **åˆ†æå¯åŠ¨å¤±è´¥**: {str(e)}"

        elif intent == "chat":
            # === åˆ†æ”¯ B: çº¯é—²èŠ (Free Style) ===
            # ä¸æŸ¥åº“ï¼Œç»™äºˆæ¨¡å‹è‡ªç”±åº¦
            chat_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šä½†å‹å¥½çš„ AI æ€§èƒ½åˆ†æä¸“å®¶ã€‚è¯·ç®€çŸ­ã€è‡ªç„¶åœ°å›å¤ç”¨æˆ·çš„é—²èŠã€‚
ä¸è¦èƒ¡ç¼–ä¹±é€ æŠ€æœ¯æ•°æ®ï¼Œä½†å¯ä»¥è¿›è¡Œè‡ªæˆ‘ä»‹ç»æˆ–æ—¥å¸¸å¯¹è¯ã€‚

ç”¨æˆ·: {message}
åŠ©æ‰‹:
"""
            try:
                raw_res = self.llm_client.generate(chat_prompt, max_tokens=256).strip()
                response_text = f"ğŸ¤– **é—²èŠæ¨¡å¼**\n{raw_res}"
            except Exception as e:
                response_text = f"âŒ **å›å¤ç”Ÿæˆå¤±è´¥**: {str(e)}"

        else:
            # === åˆ†æ”¯ C: ä¸“ä¸šé—®ç­” (Strict RAG) ===
            # å…³é”®ä¿®æ”¹ï¼šk=6ï¼Œå¤§å¹…å¢åŠ é•¿å°¾ Kernel åç§°çš„å¬å›ç‡
            retrieved_contexts = self.kb.search(query=message, k=6)
            rag_context = ""
            if retrieved_contexts:
                rag_snippets = [
                    f"ã€æ–‡æ¡£ç‰‡æ®µ {i + 1}ã€‘\n{res['content']}"
                    for i, res in enumerate(retrieved_contexts)
                ]
                rag_context = "\n\n".join(rag_snippets)

            try:
                answer = await self._generate_strict_qa_response(message, rag_context)
                ref_count = len(retrieved_contexts)
                response_text = f"ğŸ¤– **ä¸“ä¸šé—®ç­”**\n{answer}\n\n---\nğŸ’¡ *åŸºäº {ref_count} æ¡çŸ¥è¯†åº“ç‰‡æ®µå›ç­”*"
            except Exception as e:
                response_text = f"âŒ **å›ç­”ç”Ÿæˆå¤±è´¥**: {str(e)}"

        # Step 3: æ›´æ–°å¯¹è¯å†å²
        self.chat_history.append({"role": "user", "content": message})

        # ç®€åŒ–å†å²å­˜å‚¨
        history_response = response_text
        if intent == "analysis":
            model_used = decision.get("model")
            history_response = f"å·²å®Œæˆå¯¹ {model_used} çš„æ€§èƒ½åˆ†æã€‚"

        self.chat_history.append({"role": "assistant", "content": history_response})
        if len(self.chat_history) > self.max_history_turns * 2:
            self.chat_history = self.chat_history[-self.max_history_turns * 2 :]

        return response_text

    async def _parse_intent_three_way(
        self, user_query: str, history: List[Dict[str, str]]
    ) -> Dict:
        """
        é˜¶æ®µä¸€ï¼šä¸‰åˆ†ç±»æ„å›¾è¯†åˆ« (ä¿®å¤ç‰ˆ - å¼ºåŒ–è¯­ä¹‰ç†è§£ï¼Œæ‹’ç»æ— è„‘å…³é”®è¯)
        """
        available_models = list(self.model_mappings.keys())
        models_str = ", ".join([f'"{m}"' for m in available_models])

        history_str = "æ— "
        if history:
            history_lines = []
            for msg in history:
                role = "User" if msg["role"] == "user" else "Assistant"
                content = msg["content"].replace("\n", " ")[:100]
                history_lines.append(f"{role}: {content}")
            history_str = "\n".join(history_lines)

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸­æ¢è·¯ç”± Agentã€‚è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥çš„**è¯­ä¹‰**ï¼ˆè€Œä¸ä»…ä»…æ˜¯å…³é”®è¯ï¼‰åˆ¤æ–­æ„å›¾ã€‚

### å¯ç”¨æ¨¡å‹å‚è€ƒ
[{models_str}]

### æ ¸å¿ƒåˆ¤åˆ«é€»è¾‘ (Logic) - è¯·ä»”ç»†åŒºåˆ† "è¯¢é—®" ä¸ "æ‰§è¡Œ"

1. **Analysis (æ‰§è¡Œåˆ†æ)**:
   - **æ ¸å¿ƒç‰¹å¾**: ç”¨æˆ·æƒ³**ç«‹å³è¿è¡Œ**æŸä¸ªä»»åŠ¡ï¼Œæˆ–è€…**è®¾ç½®**å‚æ•°æ¥è·‘æµ‹è¯•ã€‚
   - **å¼ºè§¦å‘è¯**: "åˆ†æ", "è¿è¡Œ", "æµ‹ä¸€ä¸‹", "è·‘", "profile", "ncu", "nsys".
   - **å‚æ•°è®¾ç½®**: åªæœ‰å½“åŒ…å«**èµ‹å€¼æ„å›¾**æ—¶ï¼ˆå¦‚ "bs=1", "bsè®¾ä¸º4", "batch_size ä¸º 1"ï¼‰ï¼Œæ‰ç®— Analysisã€‚
   - **ç¤ºä¾‹**: "è·‘ä¸€ä¸‹ qwen", "åˆ†æ qwen batch_size=1", "æµ‹è¯•æ€§èƒ½".

2. **QA (ä¸“ä¸šé—®ç­”/å’¨è¯¢)**:
   - **æ ¸å¿ƒç‰¹å¾**: ç”¨æˆ·æƒ³**è·å–çŸ¥è¯†**ã€è¯¢é—®å»ºè®®ã€æŸ¥è¯¢æ–‡æ¡£æˆ–æ•°æ®ã€‚
   - **å¼ºè§¦å‘è¯**: "æ¨è", "æ˜¯å¤šå°‘", "èŒƒå›´", "ä»€ä¹ˆ", "ç“¶é¢ˆ", "æ–‡æ¡£".
   - **å…³é”®åŒºåˆ†**: å¦‚æœç”¨æˆ·é—® "æ¨è batch_size æ˜¯å¤šå°‘"ï¼Œè¿™æ˜¯ **QA**ï¼Œä¸æ˜¯ Analysisï¼
   - **ç¤ºä¾‹**: "qwen æ¨èçš„ batch_size æ˜¯å¤šå°‘", "kernel 0 çš„ç“¶é¢ˆæ˜¯ä»€ä¹ˆ", "æ˜¾å­˜å ç”¨é«˜å—".

3. **Chat (é—²èŠ)**:
   - çº¯ç²¹çš„ç¤¾äº¤ã€æ‰“æ‹›å‘¼ã€è‡ªæˆ‘ä»‹ç»ã€‚
   - ç¤ºä¾‹: "ä½ å¥½", "ä½ æ˜¯è°", "è°¢è°¢".

### ç”¨æˆ·è¾“å…¥
{user_query}

### å¯¹è¯å†å²
{history_str}

### è¾“å‡ºæ ¼å¼ (JSON)
{{
    "intent": "analysis" | "qa" | "chat",
    "model": "æ¨¡å‹å (Analysisæ¨¡å¼å¿…å¡«ï¼ŒQAæ¨¡å¼å¯ç•™ç©º)",
    "params": {{ "batch_size": [1], ... }}
}}
"""
        raw_output = self.llm_client.generate(
            prompt, max_tokens=256, mode="structured"
        ).strip()

        # JSON æ¸…æ´—
        if raw_output.startswith("```json"):
            raw_output = raw_output[7:]
        if raw_output.endswith("```"):
            raw_output = raw_output[:-3]

        try:
            result = json.loads(raw_output)
        except Exception:
            # å…œåº•ç­–ç•¥ä¿®æ­£ï¼šä¸è¦çœ‹åˆ° batch_size å°±è®¤ä¸ºæ˜¯ analysis
            # åªæœ‰åŒ…å«æ˜ç¡®åŠ¨ä½œåŠ¨è¯æ—¶ï¼Œæ‰å…œåº•ä¸º analysis
            action_keywords = ["åˆ†æ", "è·‘", "æµ‹", "profile", "è¿è¡Œ"]
            if any(k in user_query for k in action_keywords):
                return {"intent": "analysis", "model": "", "params": {}}
            return {"intent": "qa"}

        # Analysis å‚æ•°è¡¥å…¨ (ä¿æŒä¸å˜)
        if result.get("intent") == "analysis":
            if "params" not in result or not isinstance(result["params"], dict):
                result["params"] = {}
            defaults = self.analysis_defaults
            for key in ["batch_size", "input_len", "output_len"]:
                if key not in result["params"]:
                    result["params"][key] = defaults.get(key, [1])

        return result

    async def _generate_strict_qa_response(
        self, user_query: str, rag_context: str
    ) -> str:
        """
        é˜¶æ®µäºŒï¼ˆä»… QAï¼‰ï¼šæåº¦ä¸¥æ ¼çš„ RAG ç”Ÿæˆ
        """
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ•°æ®åˆ†æå‘˜ã€‚ä½ å¿…é¡»å®Œå…¨ä¾æ®ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·å…³äº GPU æ€§èƒ½æ•°æ®çš„æé—®ã€‚

### å‚è€ƒèµ„æ–™
{rag_context if rag_context else "ï¼ˆè­¦å‘Šï¼šæœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå¯èƒ½éœ€è¦å‘ŠçŸ¥ç”¨æˆ·èµ„æ–™ç¼ºå¤±ï¼‰"}

### ç”¨æˆ·é—®é¢˜
{user_query}

### ä¸¥æ ¼çº¦æŸ (Strict Rules)
1. **æ•°æ®ç²¾ç¡®æ€§**ï¼šå¦‚æœç”¨æˆ·è¯¢é—®æŸä¸ª Kernel çš„å…·ä½“æŒ‡æ ‡ï¼ˆå¦‚ç“¶é¢ˆæ•°ã€å¸¦å®½ï¼‰ï¼Œ**å¿…é¡»**åœ¨å‚è€ƒèµ„æ–™ä¸­æ‰¾åˆ°**å®Œå…¨åŒ¹é…**çš„ Kernel åç§°åæ‰èƒ½å›ç­”ã€‚
2. **æ‹’ç»çŒœæµ‹**ï¼šå¦‚æœèµ„æ–™é‡Œæœ‰ "Kernel A" å’Œ "Kernel B"ï¼Œä½†ç”¨æˆ·é—® "Kernel C"ï¼Œä½ å¿…é¡»å›ç­”ï¼š"èµ„æ–™ä¸­æœªæ‰¾åˆ° Kernel C çš„æ•°æ®"ã€‚**ä¸¥ç¦**æŠŠ A çš„æ•°æ®å®‰åœ¨ C å¤´ä¸Šã€‚
3. **åŸæ–‡å¼•ç”¨**ï¼šå›ç­”æ—¶å°½é‡ä½¿ç”¨èµ„æ–™ä¸­çš„åŸè¯æˆ–æ•°æ®ã€‚
4. **ç©ºå€¼å¤„ç†**ï¼šå¦‚æœèµ„æ–™ä¸ºç©ºæˆ–ä¸ç›¸å…³ï¼Œç›´æ¥å›ç­”ï¼šâ€œæŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ã€‚â€

### å›ç­”ï¼š
"""
        return self.llm_client.generate(prompt, max_tokens=1024).strip()

    async def _execute_analysis_flow(
        self, model_name: str, analysis_type: str, params: Dict
    ) -> str:
        model_path = self._resolve_model_path(model_name)
        if not model_path:
            # æ˜ç¡®æŠ›å‡ºé”™è¯¯ï¼Œè®©ç”¨æˆ·çŸ¥é“æ˜¯æ¨¡å‹é…ç½®é—®é¢˜
            raise ValueError(
                f"æ¨¡å‹è·¯å¾„è§£æå¤±è´¥: '{model_name}'ã€‚\n"
                f"è¯·æ£€æŸ¥ config.yaml ä¸­çš„ 'model_mappings' æ˜¯å¦åŒ…å«è¯¥æ¨¡å‹ï¼Œ"
                f"æˆ–è€…æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨äº: {self.models_path}"
            )
        return await self._run_analysis(
            model_path=model_path, analysis_type=analysis_type, params=params
        )

    async def _run_analysis(
        self, model_path: str, analysis_type: str, params: Dict
    ) -> str:
        # å‚æ•°æå–
        batch_sizes = params.get("batch_size", [1])
        input_lens = params.get("input_len", [128])
        output_lens = params.get("output_len", [1])
        batch_size = batch_sizes[0] if isinstance(batch_sizes, list) else batch_sizes
        input_len = input_lens[0] if isinstance(input_lens, list) else input_lens
        output_len = output_lens[0] if isinstance(output_lens, list) else output_lens

        # Mock æ¨¡å¼ (å¼€å‘è°ƒè¯•ç”¨)
        if os.getenv("AGENT_TEST_MODE", "0") == "1":
            print("[DEBUG] è¿è¡Œåœ¨æµ‹è¯•æ¨¡å¼ (Mock Analysis)")
            mock_dir = self.results_dir / f"mock_analysis_b{batch_size}"
            mock_dir.mkdir(exist_ok=True)
            mock_report = f"""
ä¸€ã€æ€»ä½“ç»Ÿè®¡
- æ¨¡å‹: {Path(model_path).name}
- Batch: {batch_size}, Input: {input_len}
- æ€»kernelsæ•°é‡: 42
- æ€»kernelæ‰§è¡Œæ—¶é—´: 125.6 ms

äºŒã€çƒ­ç‚¹Kernelsï¼ˆæŒ‰æ—¶é—´é™åºï¼‰
1. flash_attn_fwd_kernel
   - æ‰§è¡Œæ—¶é—´: 45.2 ms
   - æ—¶é—´å æ¯”: 36.0%
2. rms_norm_kernel
   - æ‰§è¡Œæ—¶é—´: 28.7 ms
   - æ—¶é—´å æ¯”: 22.8%
"""
            report_path = mock_dir / "integrated_performance_report.md"
            report_path.write_text(mock_report.strip(), encoding="utf-8")
            run_records = [("0", mock_dir)]
        else:
            # çœŸå®è¿è¡Œ
            try:
                analysis_workflow = create_sglang_analysis_workflow()
                workflow_output = await asyncio.get_event_loop().run_in_executor(
                    None,
                    analysis_workflow,
                    str(model_path),
                    batch_size,
                    input_len,
                    output_len,
                )

                run_records: List[Tuple[str, Path]] = []
                if isinstance(workflow_output, list):
                    for idx, item in enumerate(workflow_output):
                        output_path = (
                            item.get("dir") or item.get("path")
                            if isinstance(item, dict)
                            else str(item)
                        )
                        gpu_label = (
                            str(item.get("gpu", idx))
                            if isinstance(item, dict)
                            else str(idx)
                        )
                        if output_path:
                            run_records.append((gpu_label, Path(output_path)))
                elif workflow_output:
                    run_records.append(("0", Path(str(workflow_output))))

                if not run_records:
                    raise RuntimeError("åˆ†æå®Œæˆä½†æœªè¿”å›è¾“å‡ºç›®å½•")

            except Exception as e:
                import traceback

                return f"""
âŒ **åˆ†ææ‰§è¡Œå¤±è´¥**
é”™è¯¯ä¿¡æ¯: {str(e)}
è¯¦ç»†é”™è¯¯:
{traceback.format_exc()}
"""

        # ç»“æœåå¤„ç†
        report_infos = []
        for idx, (gpu_label, output_dir) in enumerate(run_records):
            report_path = output_dir / "integrated_performance_report.md"
            if report_path.exists():
                report_text = report_path.read_text(encoding="utf-8")
                report_infos.append(
                    {
                        "gpu": gpu_label,
                        "dir": output_dir,
                        "report": report_path,
                        "text": report_text,
                    }
                )

        if not report_infos:
            return f"âš ï¸ åˆ†æå®Œæˆï¼Œä½†æœªç”ŸæˆæŠ¥å‘Šæ–‡ä»¶ã€‚\nç›®å½•: {[str(p[1]) for p in run_records]}"

        primary_info = report_infos[0]
        summary = self._extract_report_summary(primary_info["text"])

        # ç”Ÿæˆè¡¨æ ¼
        try:
            if len(report_infos) > 1:
                table_markdown = self._generate_multi_gpu_table(
                    [info["text"] for info in report_infos],
                    [info["gpu"] for info in report_infos],
                )
            else:
                table_markdown = self._generate_report_table(primary_info["text"])
        except Exception:
            table_markdown = "âš ï¸ (è¡¨æ ¼ç”Ÿæˆå¤±è´¥)"

        dir_lines = "\n".join(
            f"  â€¢ {info['gpu']}: {info['dir']}" for info in report_infos
        )

        return f"""
âœ… **åˆ†æå®Œæˆ!**

ğŸ“ **ç»“æœç›®å½•**:
{dir_lines}
ğŸ“„ **æŠ¥å‘Šæ–‡ä»¶**: {primary_info["report"]}
{summary}

ğŸ“Œ **çƒ­ç‚¹Kernelè¡¨æ ¼é¢„è§ˆ**:
{table_markdown}
"""

    def _resolve_model_path(self, model_name: str) -> Optional[str]:
        if not model_name:
            return None
        # 1. æ˜ å°„è¡¨
        if model_name in self.model_mappings:
            mapped_path = self.model_mappings[model_name]
            if Path(mapped_path).is_absolute():
                return mapped_path
            return str(self.models_path / mapped_path)
        # 2. ç‰©ç†è·¯å¾„æ£€æŸ¥
        if Path(model_name).exists():
            return model_name
        potential_path = self.models_path / model_name
        if potential_path.exists():
            return str(potential_path)
        return None

    @staticmethod
    def _generate_report_table(report_text: str) -> str:
        # ç®€æ˜“è¡¨æ ¼ç”Ÿæˆ
        from offline_llm import get_offline_qwen_client

        client = get_offline_qwen_client(Path(__file__).parent / "dummy")
        return client.report_to_table(report_text)

    def _generate_multi_gpu_table(
        self, report_texts: List[str], gpu_labels: List[str]
    ) -> str:
        # å¤ç”¨å¤šå¡é€»è¾‘ (ç®€åŒ–ç‰ˆ)
        if not report_texts:
            return ""
        entries = self._parse_kernel_entries_from_report(report_texts[0])
        header = (
            "| Kernel | " + " | ".join([f"{lbl} Duration" for lbl in gpu_labels]) + " |"
        )
        sep = "|---" * (len(gpu_labels) + 1) + "|"
        rows = []
        for entry in entries[:5]:  # Top 5
            rows.append(
                f"| {entry['name']} | {entry['duration']} |"
                + " ... |" * (len(gpu_labels) - 1)
            )
        return f"{header}\n{sep}\n" + "\n".join(rows)

    def _parse_kernel_entries_from_report(
        self, report_text: str
    ) -> List[Dict[str, str]]:
        entries = []
        lines = report_text.splitlines()
        current_entry = {}
        for line in lines:
            name_match = re.match(r"^\s*\d+\.\s+(.*)$", line)
            if name_match:
                if current_entry:
                    entries.append(current_entry)
                current_entry = {
                    "name": name_match.group(1).strip(),
                    "duration": "-",
                    "ratio": "-",
                }
            dur_match = re.search(r"æ‰§è¡Œæ—¶é—´[:\s]+([0-9.]+\s*ms)", line)
            if dur_match and current_entry:
                current_entry["duration"] = dur_match.group(1)
            if "äºŒã€" in line:
                break
        if current_entry:
            entries.append(current_entry)
        return entries

    def _extract_report_summary(self, report_content: str) -> str:
        lines = report_content.split("\n")
        summary_lines = []
        for i, line in enumerate(lines):
            if "æ€»kernelsæ•°é‡" in line or "æ€»kernelæ‰§è¡Œæ—¶é—´" in line:
                summary_lines.append(line)
            elif "çƒ­ç‚¹Kernels" in line:
                summary_lines.append("\n**ğŸ”¥ çƒ­ç‚¹Kernels (Top 3):**")
                count = 0
                for j in range(i + 1, len(lines)):
                    if re.match(r"^\s*\d+\.", lines[j]):
                        summary_lines.append(lines[j][:100])
                        count += 1
                        if count >= 3:
                            break
                break
        return "\n".join(summary_lines) if summary_lines else ""


# ==================== Main CLI ====================
if __name__ == "__main__":
    # 1. Load Config
    config_path = "config.yaml"
    if not os.path.exists(config_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {config_path}")
        sys.exit(1)

    with open(config_path, "r", encoding="utf-8") as f:
        config_yaml = yaml.safe_load(f)

    # 2. Init Agent
    print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– AI Agent...")
    try:
        agent = AIAgent(config_yaml)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)

    # 3. Load Knowledge Base
    document_dir = Path("documents")
    if document_dir.exists():
        print("ğŸ“š æ­£åœ¨åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£...")
        count = 0
        for file_path in document_dir.iterdir():
            if file_path.is_file() and file_path.suffix in [".md", ".txt"]:
                agent.kb.add_document(str(file_path))
                count += 1
        print(f"âœ… å·²åŠ è½½ {count} ä¸ªæ–‡æ¡£ã€‚")
    else:
        print("âš ï¸ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½ã€‚")

    # 4. Interactive Loop
    async def interactive_chat_loop():
        style = Style.from_dict({"user-prompt": "#00aa00 bold", "text": "#ffffff"})
        session = PromptSession(history=InMemoryHistory())

        print("\n" + "=" * 60)
        print("ğŸ¤– AI æ€§èƒ½åˆ†æåŠ©æ‰‹ (V3 - æ··åˆæ„å›¾æ¨¡å¼)")
        print("ğŸ’¡ æ”¯æŒæŒ‡ä»¤: 'åˆ†æ qwen' | æé—®: 'ç“¶é¢ˆæ˜¯ä»€ä¹ˆ' | é—²èŠ: 'ä½ æ˜¯è°'")
        print("=" * 60 + "\n")

        while True:
            try:
                user_input = await session.prompt_async(
                    HTML("<user-prompt>User ></user-prompt> "), style=style
                )
                user_input = user_input.strip()
                if not user_input:
                    continue
                if user_input.lower() in ["exit", "quit", "q"]:
                    print("\nğŸ‘‹ å†è§ï¼")
                    break

                print("\nâ³ Agent æ­£åœ¨æ€è€ƒ...")
                response = await agent.process_message(user_input)
                print("-" * 20 + " Agent å›å¤ " + "-" * 20)
                print(response)
                print("-" * 52 + "\n")

            except (KeyboardInterrupt, EOFError):
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")

    asyncio.run(interactive_chat_loop())
