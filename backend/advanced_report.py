#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Advanced Performance Report Generator

ç”Ÿæˆæ›´é«˜é˜¶çš„æ€§èƒ½ä¼˜åŒ–å»ºè®®æŠ¥å‘Šï¼ŒåŒ…å«ï¼š
1. åˆ†å±‚ç“¶é¢ˆæ’åºä¸ç±»å‹å½’ç±» (Compute / Memory / Launch / Fusion)
2. ä¼˜åŒ–ç­–ç•¥åˆ—è¡¨ (çŸ­æœŸ / ä¸­æœŸ / é•¿æœŸ)
3. ä»»åŠ¡æ¸…å• (T1/T2/... å«é¢„è®¡è€—æ—¶ä¸è§’è‰²)
4. é¢„è®¡æ”¶ç›Šä¸é£é™©è¯„ä¼°
5. çŸ¥è¯†åº“å†™å›å€™é€‰ç‰‡æ®µ (å¯ç”¨äºåç»­ ingestion)

è‹¥ Agent / æ·±åº¦åˆ†ææ•°æ®ç¼ºå¤±ï¼Œå°†ç”Ÿæˆå ä½éª¨æ¶ã€‚
"""
from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import json

try:
    from backend.agent_core import AIAgent  # optional
except Exception:
    AIAgent = None  # type: ignore

DEFAULT_ANALYSIS_FILE = 'integrated_performance_report.md'

# ----------------- Helpers -----------------

def _load_basic_report(dir_path: Path) -> Optional[str]:
    fp = dir_path / DEFAULT_ANALYSIS_FILE
    if not fp.exists():
        return None
    try:
        return fp.read_text(encoding='utf-8')
    except Exception:
        return None

def _extract_hot_kernels(report_text: str) -> List[Dict[str, Any]]:
    hot_list: List[Dict[str, Any]] = []
    lines = report_text.splitlines()
    in_hot = False
    for line in lines:
        if 'è¯†åˆ«çš„çƒ­ç‚¹Kernels' in line:
            in_hot = True
            continue
        if in_hot:
            if line.startswith('##'):
                break
            if line.strip().startswith(tuple(str(i)+'.' for i in range(1,10))):
                # example: '1. **kernel_name**...'
                try:
                    idx_dot = line.index('.')
                except ValueError:
                    continue
                rank_part = line[:idx_dot].strip()
                rest = line[idx_dot+1:].strip()
                name = ''
                if '**' in rest:
                    # between first pair of **
                    parts = rest.split('**')
                    if len(parts) >= 3:
                        name = parts[1]
                hot_list.append({'rank': rank_part, 'name': name, 'raw': rest})
    return hot_list

def _classify_bottlenecks(hot_kernels: List[Dict[str, Any]]) -> Dict[str, List[str]]:
    classes = {'compute': [], 'memory': [], 'launch': [], 'fusion': []}
    for k in hot_kernels:
        name = k['name'].lower()
        if any(x in name for x in ['gemm','matmul','mm','cublas']):
            classes['compute'].append(k['name'])
        elif any(x in name for x in ['mem','ld','st','copy']):
            classes['memory'].append(k['name'])
        elif any(x in name for x in ['cudart','runtime','cudaLaunch']):
            classes['launch'].append(k['name'])
        else:
            # default to compute or fusion candidate
            classes['fusion'].append(k['name'])
    return classes

def _generate_tasks(classes: Dict[str, List[str]]) -> Dict[str, List[Dict[str, Any]]]:
    tasks: Dict[str, List[Dict[str, Any]]] = {'high': [], 'medium': [], 'low': []}
    # High priority examples
    if classes['compute']:
        tasks['high'].append({
            'id': 'T1',
            'title': 'æ›¿æ¢/ä¼˜åŒ–ä¸» MatMul/GEMM å†…æ ¸',
            'estimate_days': '1-2',
            'kernels': classes['compute'][:5],
            'action': 'åŸºäº CUTLASS/cublasGemmEx åšåŸºå‡†ï¼Œé€‰æ‹©æœ€ä¼˜é…ç½®æˆ–è‡ªå®šä¹‰ kernel'
        })
    if classes['launch']:
        tasks['high'].append({
            'id': 'T2',
            'title': 'å¯ç”¨ CUDA Graph æ•è· decode æ‰§è¡Œè·¯å¾„',
            'estimate_days': '0.5-1',
            'kernels': classes['launch'][:5],
            'action': 'å‡å°‘å°æ ¸ launch å¼€é”€ä¸ host-side è°ƒåº¦ç©ºæ´'
        })
    # Medium priority examples
    if classes['memory']:
        tasks['medium'].append({
            'id': 'T3',
            'title': 'ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼ / FlashAttention',
            'estimate_days': '2-3',
            'kernels': classes['memory'][:5],
            'action': 'KV å¸ƒå±€ä¼˜åŒ– + å¯ç”¨ FlashAttention åˆ†å—å‡å°‘ DRAM å¸¦å®½å‹åŠ›'
        })
    tasks['medium'].append({
        'id': 'T4',
        'title': 'å®ç° LayerNorm+æ¿€æ´» Fusion',
        'estimate_days': '2-3',
        'kernels': [],
        'action': 'Triton/nvFuser èåˆé™ä½ launch & memory å¼€é”€'
    })
    # Low priority examples
    tasks['low'].append({
        'id': 'T5',
        'title': 'å»ºç«‹éªŒè¯ä¸çŸ¥è¯†åº“å†™å›æµæ°´çº¿',
        'estimate_days': '1-2',
        'kernels': [],
        'action': 'A/B éªŒè¯ (latency, throughput, perplexity) è‡ªåŠ¨å†™å›å‘é‡åº“'
    })
    return tasks

def _expected_gains(tasks: Dict[str, List[Dict[str, Any]]]) -> str:
    return (
        'è‹¥é«˜/ä¸­ä¼˜å…ˆçº§ä»»åŠ¡å…¨éƒ¨è½åœ°ï¼šååæå‡é¢„ä¼° 1.3Ã—â€“2.0Ã—ï¼Œ token å»¶è¿Ÿé™ä½ 20%â€“40% (å–å†³äºç®—å­èåˆä¸å›¾æ•è·æ•ˆæœ)ã€‚'
    )

def _generate_granular_kernel_tasks(hot_kernels: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    tasks = []
    for idx, k in enumerate(hot_kernels[:20], 1):
        cls = k.get('classification','other')
        base_id = f"KT{idx}"
        if cls == 'compute':
            action = 'éªŒè¯ Tensor Core ä½¿ç”¨ç‡ï¼Œè¿è¡Œ CUTLASS åŸºå‡†ï¼Œå°è¯• autotune GEMM é…ç½®'
        elif cls == 'memory':
            action = 'æ£€æŸ¥å…¨å±€å†…å­˜è®¿é—®æ­¥å¹…ä¸å¯¹é½ï¼Œè¯„ä¼°æ˜¯å¦å¯ç”¨å…±äº«å†…å­˜ / FlashAttention åˆ†å—'
        elif cls == 'launch':
            action = 'æ•´åˆåˆ° CUDA Graph æˆ–åˆå¹¶å°æ ¸ï¼Œå‡å°‘ host-side é—´éš™'
        else:
            action = 'å°è¯•ç®—å­èåˆ (LayerNorm+æ¿€æ´») æˆ–å‰–ææŒ‡ä»¤çº§ç“¶é¢ˆ'
        tasks.append({
            'id': base_id,
            'kernel': k.get('name'),
            'classification': cls,
            'time_pct': round(k.get('time_pct',0),2),
            'avg_time_ms': round(k.get('avg_time_ms',0),4),
            'action': action
        })
    return tasks

def generate_advanced_report(output_dir: Path, detailed: bool = False) -> str:
    report_path = output_dir / 'advanced_performance_report.md'
    base_text = _load_basic_report(output_dir)
    hot_kernels = _extract_hot_kernels(base_text) if base_text else []
    classes = _classify_bottlenecks(hot_kernels) if hot_kernels else {'compute': [], 'memory': [], 'launch': [], 'fusion': []}
    tasks = _generate_tasks(classes)
    gains = _expected_gains(tasks)
    metrics_block = ''
    granular_tasks: List[Dict[str, Any]] = []
    if detailed:
        from backend.perf_data_parser import load_comprehensive, aggregate_metrics
        comp = load_comprehensive(output_dir / 'comprehensive_analysis.json')
        if comp:
            agg = aggregate_metrics(comp)
            idle_pct = f"{agg['idle_fraction']*100:.1f}%" if agg.get('idle_fraction') is not None else 'N/A'
            bw = agg.get('bandwidth', {})
            metrics_block = ("## 0. å…³é”®æŒ‡æ ‡å¿«ç…§\n\n" +
                f"- Idle Fraction (ä¼°ç®—): {idle_pct}\n" +
                f"- å¹³å‡å¸¦å®½ (GB/s): {bw.get('avg_bandwidth_gb_s','N/A')}\n" +
                f"- æ€»æ•°æ®ä¼ è¾“ (MB): {bw.get('total_data_mb','N/A')}\n" +
                f"- çƒ­ç‚¹ kernel æ•°: {len(agg.get('hot_kernels', []))}\n\n")
            # ç”Ÿæˆæ¯ kernel ä»»åŠ¡
            granular_tasks = _generate_granular_kernel_tasks(agg.get('hot_kernels', []))
        else:
            metrics_block = '## 0. å…³é”®æŒ‡æ ‡å¿«ç…§\n\n- (æœªæ‰¾åˆ° comprehensive_analysis.json)\n\n'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('# ğŸ§  é«˜é˜¶æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š\n\n')
        f.write(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n\n')
        if metrics_block:
            f.write(metrics_block)
        if not base_text:
            f.write('> âš ï¸ åŸºç¡€åˆ†ææŠ¥å‘Šç¼ºå¤±ï¼Œä»…ç”Ÿæˆéª¨æ¶ã€‚è¯·å…ˆè¿è¡Œé›†æˆåˆ†æä»¥è·å¾—çƒ­ç‚¹ kernel æ•°æ®ã€‚\n\n')
        f.write('## 1. çƒ­ç‚¹ Kernel åˆ†ç±»\n\n')
        for cat, items in classes.items():
            f.write(f'- {cat}: {items if items else "(æ— )"}\n')
        f.write('\n')
        f.write('## 2. ä¼˜åŒ–ç­–ç•¥æ¦‚è§ˆ\n\n')
        f.write('- Compute: ä½¿ç”¨ CUTLASS / Tensor Core é…ç½®ï¼Œå‡å°‘éæœ€ä¼˜ GEMM\n')
        f.write('- Memory: FlashAttention / KV é‡å¸ƒå±€ / å‡å°‘ä¸å¿…è¦çš„å…¨é‡è®¿é—®\n')
        f.write('- Launch: CUDA Graph æ•è·å‡å°‘ host-side idle ä¸ launch overhead\n')
        f.write('- Fusion: Triton/nvFuser è¿›è¡Œç®—å­èåˆå‡å°‘ä¸­é—´å†™å›\n\n')
        f.write('## 3. ä»»åŠ¡åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰\n\n')
        for prio in ['high','medium','low']:
            f.write(f'### {prio.upper()}\n')
            for t in tasks[prio]:
                f.write(f"- {t['id']}: {t['title']} (é¢„è®¡ {t['estimate_days']} å¤©)\n  æ ¸å¿ƒ: {t['action']}\n  æ¶‰åŠ Kernels: {t['kernels'] if t['kernels'] else '(é€šç”¨)'}\n")
            f.write('\n')
        f.write('## 4. éªŒè¯è®¡åˆ’\n\n')
        f.write('- A/B: å¯¹æ¯”ä¼˜åŒ–å‰å throughput / å• token latency / perplexity\n')
        f.write('- Profiling: ä½¿ç”¨ nsys + ncu éªŒè¯çƒ­ç‚¹æ˜¯å¦é‡æ–°æ’åº\n')
        f.write('- KB å›å†™: è‡ªåŠ¨æ‘„å–æŠ¥å‘Šç»“è®ºä¸æŒ‡æ ‡åˆ°å‘é‡åº“\n\n')
        if granular_tasks:
            f.write('## 4.1 ç»†ç²’åº¦ Kernel ä»»åŠ¡ (Granular)\n\n')
            for gt in granular_tasks:
                f.write(f"- {gt['id']}: {gt['kernel']} [{gt['classification']}] å æ¯” {gt['time_pct']}% å¹³å‡ {gt['avg_time_ms']} ms\n  è¡ŒåŠ¨: {gt['action']}\n")
            f.write('\n')
        f.write('## 5. é¢„è®¡æ”¶ç›Šä¸é£é™©\n\n')
        f.write(gains + '\n')
        f.write('- é£é™©: å¯èƒ½ç²¾åº¦ä¸‹é™ / éœ€è¦é¢å¤–æ˜¾å­˜ / å¼€å‘æ—¶é—´ä¸ç¡®å®š\n\n')
        f.write('## 6. æ€»ç»“ (Summary)\n\n')
        f.write('å½“å‰ç“¶é¢ˆæ’åº (ä¼°è®¡): Compute MatMul/GEMM > Memory-bound Attention > Kernel launch/fusion gaps.\n')
        f.write('æ‰§è¡Œé¡ºåºå»ºè®®: MatMul å†…æ ¸ / ç²¾åº¦è°ƒæ•´ â†’ CUDA Graph & Fusion â†’ Attention KV ä¼˜åŒ– â†’ å…¨é¢ç®—å­èåˆä¸è‡ªåŠ¨è°ƒå‚ä¸ KB å†™å›ã€‚\n')
    return str(report_path)

__all__ = ['generate_advanced_report']
