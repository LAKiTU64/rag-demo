#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""LangChain Synthesis Utilities

å°†æ€§èƒ½åˆ†æç»“æœ (comprehensive_analysis.json + advanced_performance_report.md) ä¸çŸ¥è¯†åº“æ£€ç´¢ç‰‡æ®µèåˆï¼Œ
é€šè¿‡ LangChain (è‹¥å¯ç”¨) ç”Ÿæˆç»ˆæç»¼åˆæŠ¥å‘Šæ‘˜è¦ä¸è¡ŒåŠ¨å»ºè®®ã€‚

ä¸»è¦å…¥å£:
    synthesize_final_report(perf_dir: Path, queries: List[str]|None) -> Dict[str, Any]

è¿”å›ç»“æ„:
{
  'markdown_path': str,
  'summary': str,
  'kb_hits': Dict[str, List[str]],
  'model_info': Dict[str, Any]
}

å®¹é”™ç­–ç•¥:
1. è‹¥ LangChain ä¸å¯ç”¨ => ä½¿ç”¨æ¨¡æ¿æ‹¼æ¥ fallback æ€»ç»“
2. è‹¥ FAISS ä¸å¯ç”¨ => kb_hits ä¸ºç©º
3. è‹¥ comprehensive/advanced æŠ¥å‘Šç¼ºå¤± => é™„åŠ è­¦å‘Š
"""
from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
from datetime import datetime

try:
    from langchain.chat_models import ChatOpenAI  # type: ignore
    from langchain.prompts import ChatPromptTemplate  # type: ignore
    LANGCHAIN_AVAILABLE = True
except Exception:
    LANGCHAIN_AVAILABLE = False

try:
    from knowledge_bases.faiss_store import load_index, query as kb_query  # type: ignore
except Exception:
    load_index = None
    kb_query = None

try:
    from backend.model_intel import extract_model_info, build_theory_queries
except Exception:
    from model_intel import extract_model_info, build_theory_queries  # type: ignore

DEFAULT_FAISS_DIR = Path('/workspace/Agent/AI_Agent_Complete/faiss_index')

def _load_json(path: Path) -> Dict[str, Any]:
    if not path.exists():
        return {}
    try:
        return json.loads(path.read_text(encoding='utf-8'))
    except Exception:
        return {}

def _load_text(path: Path) -> str:
    if not path.exists():
        return ''
    try:
        return path.read_text(encoding='utf-8')
    except Exception:
        return ''

def _faiss_ready() -> bool:
    return load_index is not None and kb_query is not None and DEFAULT_FAISS_DIR.exists()

def _collect_kb_hits(queries: List[str], top_k: int = 3) -> Dict[str, List[str]]:
    if not _faiss_ready():
        return {}
    hits: Dict[str, List[str]] = {}
    try:
        store = load_index(DEFAULT_FAISS_DIR, model_name='sentence-transformers/all-MiniLM-L6-v2')
        for q in queries:
            try:
                res = kb_query(store, q, top_k=top_k)
                hits[q] = [r['text'][:400] for r in res]
            except Exception:
                hits[q] = []
    except Exception:
        pass
    return hits

def _build_prompt(perf_summary: str, kb_hits: Dict[str, List[str]], advanced_excerpt: str) -> str:
    kb_part = '\n'.join([
        f'Query: {q}\n' + '\n'.join([f'- {t}' for t in texts])
        for q, texts in kb_hits.items()
    ]) or '(æ— çŸ¥è¯†åº“æ£€ç´¢ç»“æœ)'
    return (
        "ä½ æ˜¯ä¸€å GPU / LLM æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ€§èƒ½åˆ†ææ‘˜è¦ä¸çŸ¥è¯†åº“ç‰‡æ®µï¼Œç”Ÿæˆç»“æ„åŒ–ç²¾ç‚¼æ€»ç»“ï¼š\n\n"
        + "[æ€§èƒ½æ‘˜è¦]\n" + perf_summary[:4000] + "\n\n"
        + "[é«˜çº§æŠ¥å‘Šç‰‡æ®µ]\n" + advanced_excerpt[:3000] + "\n\n"
        + "[çŸ¥è¯†åº“æ£€ç´¢]\n" + kb_part + "\n\n"
        + "è¾“å‡ºæ ¼å¼: \n1. å…³é”®ç“¶é¢ˆæ¦‚è¿° (â‰¤5æ¡)\n2. ä¼˜å…ˆä¼˜åŒ–è¡ŒåŠ¨ (T1/T2ç­‰, æ¯æ¡ä¸€å¥)\n3. ç†è®ºæ”¯æ’‘è¦ç‚¹ (å¼•ç”¨çŸ¥è¯†åº“æ‘˜è¦)\n4. é¢„ä¼°æ”¶ç›Šä¸é£é™©ä¸€å¥è¯æ€»ç»“\n"
        + "è¯·ä½¿ç”¨ä¸­æ–‡ã€‚"
    )

def _fallback_summary(perf_summary: str, kb_hits: Dict[str, List[str]], advanced_excerpt: str) -> str:
    top_q = list(kb_hits.keys())[:3]
    return (
        "## ç»¼åˆæ‘˜è¦ (Fallback)\n\n" +
        f"æ€§èƒ½æ¦‚è§ˆ: {perf_summary[:300] or 'ç¼ºå¤±'}...\n\n" +
        "ä»»åŠ¡ä¼˜å…ˆçº§: å‚è€ƒ T1 GEMM ä¼˜åŒ– / T2 CUDA Graph / T3 æ³¨æ„åŠ›ä¸å†…å­˜æ¨¡å¼ / Fusionã€‚\n\n" +
        "çŸ¥è¯†åº“å…³é”®è¯: " + (', '.join(top_q) if top_q else 'æ— ') + "\n\n" +
        "å»ºè®®: å…ˆèšç„¦æœ€å¤§æ—¶é—´å æ¯” compute kernels, å¹¶å¹¶è¡Œè§„åˆ’å›¾æ•è·ä¸å†…å­˜è®¿é—®ä¼˜åŒ–, æœ€åè¿›è¡Œèåˆä¸ KB å†™å›ã€‚"
    )

def synthesize_final_report(perf_dir: Path, queries: Optional[List[str]] = None, extra_query_text: Optional[str] = None) -> Dict[str, Any]:
    comp = _load_json(perf_dir / 'comprehensive_analysis.json')
    adv_text = _load_text(perf_dir / 'advanced_performance_report.md')
    enriched_text = _load_text(perf_dir / 'integrated_performance_report_enriched.md')
    basic_text = _load_text(perf_dir / 'integrated_performance_report.md')

    kernel_overview = comp.get('nsys_overview', {}).get('kernel_analysis', {})
    hot_list = comp.get('hot_kernels', [])
    perf_parts = []
    if kernel_overview:
        perf_parts.append(
            f"æ€»kernels {kernel_overview.get('total_kernels','?')} | æ€»æ—¶é—´ {kernel_overview.get('total_kernel_time','?')} ms | å¹³å‡ {kernel_overview.get('avg_kernel_time','?')} ms"
        )
    if hot_list:
        perf_parts.append('çƒ­ç‚¹: ' + ', '.join([k.get('name','')[:60] for k in hot_list[:5]]))
    perf_summary = '\n'.join(perf_parts) or '(ç¼ºå¤±åŸºç¡€æ€§èƒ½æ•°æ®)'

    model_info = extract_model_info(basic_text + adv_text + (extra_query_text or ''))
    theory_queries = build_theory_queries(model_info)
    if queries:
        theory_queries.extend([q for q in queries if isinstance(q, str)])
    if extra_query_text:
        theory_queries.append(extra_query_text.strip()[:100])

    kb_hits = _collect_kb_hits(theory_queries)

    adv_excerpt = ''
    if adv_text:
        lines = adv_text.splitlines()
        for ln in lines:
            if any(h in ln for h in ['## 1. çƒ­ç‚¹ Kernel åˆ†ç±»', '## 3. ä»»åŠ¡åˆ—è¡¨', '## 6. æ€»ç»“']):
                adv_excerpt += ln + '\n'
        adv_excerpt += '\n'.join(lines[-20:])

    if LANGCHAIN_AVAILABLE:
        try:
            prompt_text = _build_prompt(perf_summary, kb_hits, adv_excerpt)
            template = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸“ä¸šçš„ GPU / LLM æ€§èƒ½ä¼˜åŒ–é¡¾é—®"),
                ("human", "{input}")
            ])
            chain = template | ChatOpenAI(temperature=0.2, model='gpt-3.5-turbo')
            resp = chain.invoke({"input": prompt_text})
            summary = resp.content
        except Exception:
            summary = _fallback_summary(perf_summary, kb_hits, adv_excerpt)
    else:
        summary = _fallback_summary(perf_summary, kb_hits, adv_excerpt)

    final_path = perf_dir / 'final_langchain_integrated_report.md'
    with open(final_path, 'w', encoding='utf-8') as f:
        f.write('# ğŸŒ ç»ˆæç»¼åˆæ€§èƒ½æŠ¥å‘Š\n\n')
        f.write(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n\n')
        if enriched_text:
            f.write('> ç»¼åˆåˆ†æå¢å¼ºæŠ¥å‘Šç‰‡æ®µ\n\n')
            f.write(enriched_text[:15000])
            f.write('\n\n---\n\n')
        f.write('## LangChain ç»¼åˆæ‘˜è¦\n\n')
        f.write(summary + '\n\n')
        if kb_hits:
            f.write('## çŸ¥è¯†åº“æ£€ç´¢ç‰‡æ®µ\n\n')
            for q, texts in kb_hits.items():
                f.write(f'### Query: {q}\n')
                for t in texts:
                    f.write(f'- {t}\n')
                f.write('\n')
        else:
            f.write('## çŸ¥è¯†åº“æ£€ç´¢ç‰‡æ®µ\n\n- (æ— ï¼Œå¯èƒ½æœªæ„å»ºå‘é‡ç´¢å¼•æˆ–ç¦»çº¿)\n')
        f.write('\n## åç»­å»ºè®®\n\n')
        f.write('- æ‰©å±• batch/input å‚æ•°æ‰«éªŒè¯ç“¶é¢ˆç¨³å®šæ€§\n')
        f.write('- Autotune ä¸» compute kernels (GEMM)\n')
        f.write('- å¼•å…¥æŒç»­å†™å›ç®¡çº¿å°†æŠ¥å‘Šæ‘„å–åˆ°å‘é‡åº“\n')
    return {
        'markdown_path': str(final_path),
        'summary': summary,
        'kb_hits': kb_hits,
        'model_info': model_info
    }

__all__ = ['synthesize_final_report']
