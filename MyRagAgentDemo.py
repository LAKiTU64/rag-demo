import os
from typing import List, TypedDict

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFacePipeline
from langgraph.graph import StateGraph, END

from VectorKBManager import VectorKBManager


# --- Config ---
MODEL_PATH = "./.models/Qwen/Qwen3-4B"


# ===============================
# 1. LLM æ„å»ºï¼ˆæœ¬åœ° Qwen3-4Bï¼‰
# ===============================
def load_local_llm():
    model_path = MODEL_PATH

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")

    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        trust_remote_code=True,
        local_files_only=True,
    )

    gen_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.2,
        do_sample=True,
        repetition_penalty=1.05,
        return_full_text=False,
    )

    return HuggingFacePipeline(pipeline=gen_pipeline)


# ===============================
# 2. Graph State
# ===============================
class RAGState(TypedDict):
    question: str
    documents: List[Document]
    answer: str


# ===============================
# 3. Agentic-RAG
# ===============================
class MyRagAgent:
    def __init__(self, kb: VectorKBManager):
        self.kb = kb
        self.llm = load_local_llm()
        self.retriever = kb.as_retriever(k=3, t=0.5)

        self.graph = self._build_graph()

    # ---------- Node: æ£€ç´¢ ----------
    def retrieve_node(self, state: RAGState):
        docs = self.retriever.invoke(state["question"])
        return {"documents": docs}

    # ---------- Node: ç”Ÿæˆ ----------
    def generate_node(self, state: RAGState):
        context = "\n\n".join(d.page_content for d in state["documents"])

        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–å¼•æ“ï¼Œåªèƒ½ä»ä¸Šä¸‹æ–‡ä¸­æå–å·²æœ‰ä¿¡æ¯ï¼Œ"
                    "ä¸å¾—æ€»ç»“ã€ä¸å¾—æ¨ç†ã€ä¸å¾—è¡¥å……ä¸å­˜åœ¨çš„å†…å®¹ã€‚"
                    "å¦‚æœæ— æ³•ç¡®å®šï¼Œè¯·å›ç­”â€œæœªåœ¨æ–‡æ¡£ä¸­æ˜ç¡®ç»™å‡ºâ€ã€‚",
                ),
                (
                    "human",
                    "ä¸Šä¸‹æ–‡ï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªKernelï¼Œè¯·é€ä¸€åˆ¤æ–­ï¼‰ï¼š\n{context}\n\n"
                    "é—®é¢˜ï¼š{question}\n\n"
                    "è¦æ±‚ï¼š\n"
                    "1. åªåˆ—å‡ºæ»¡è¶³æ¡ä»¶çš„ Kernel åç§°\n"
                    "2. ç»™å‡ºå¯¹åº”çš„å…·ä½“æ•°å€¼\n"
                    "3. ä¸è¦è§£é‡Šæ¨ç†è¿‡ç¨‹",
                ),
            ]
        )

        chain = prompt | self.llm | StrOutputParser()
        response = chain.invoke(
            {
                "context": context,
                "question": state["question"],
            }
        )

        return {"answer": response}

    # ---------- Graph ----------
    def _build_graph(self):
        graph = StateGraph(RAGState)

        graph.add_node("retrieve", self.retrieve_node)
        graph.add_node("generate", self.generate_node)

        graph.set_entry_point("retrieve")
        graph.add_edge("retrieve", "generate")
        graph.add_edge("generate", END)

        return graph.compile()

    # ---------- API ----------
    def ask(self, question: str) -> str:
        result = self.graph.invoke(
            {
                "question": question,
                "documents": [],
                "answer": "",
            }
        )
        return result["answer"]


# ===============================
# 4. main æµ‹è¯•
# ===============================
if __name__ == "__main__":
    # ---- åˆå§‹åŒ–å‘é‡åº“ ----
    kb = VectorKBManager()

    # ---- åŠ è½½ documents ----
    DOCS_DIR = "./documents"

    print(f"\nğŸš€ åŠ è½½æ–‡æ¡£ç›®å½•: {DOCS_DIR}")
    for fname in os.listdir(DOCS_DIR):
        fpath = os.path.join(DOCS_DIR, fname)
        if os.path.isfile(fpath):
            kb.add_document(fpath)

    kb.get_overview()

    # ---- Agent ----
    agent = MyRagAgent(kb)

    # ---- Query 1 ----
    q1 = "å“ªäº›Kernelå‡½æ•°çš„ç“¶é¢ˆæ•°>=3ï¼Ÿ"
    print("\nğŸ§ª Query 1:", q1)
    print("ğŸ¤– Answer:\n", agent.ask(q1))

    # ---- Query 2 ----
    q2 = "å“ªä¸ªKernelçš„æ‰§è¡Œæ—¶é—´å æ¯”æœ€é«˜ï¼Ÿ"
    print("\nğŸ§ª Query 2:", q2)
    print("ğŸ¤– Answer:\n", agent.ask(q2))
