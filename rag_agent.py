import torch

# --- LangGraph æ ¸å¿ƒç»„ä»¶ ---
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage

# --- æ¨¡å‹ç›¸å…³ ---
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# --- ç±»å‹æç¤º ---
from typing import Annotated, Sequence, TypedDict, Literal

# --- çŸ¥è¯†åº“ ---
from VectorKBManager import VectorKBManager

# --- Config ---
MODEL_PATH = "/workspaces/rag-demo/.models/Qwen/Qwen3-4B"


# ==============================================================================
# 1. å®šä¹‰ Agent çŠ¶æ€
# ==============================================================================
class AgentState(TypedDict):
    """Agent çš„çŠ¶æ€å®šä¹‰"""

    # add_messages æ˜¯ä¸€ä¸ª reducerï¼Œä¼šè‡ªåŠ¨åˆå¹¶æ¶ˆæ¯åˆ—è¡¨
    messages: Annotated[Sequence[BaseMessage], add_messages]


# ==============================================================================
# 2. æ¨¡å‹åŠ è½½
# ==============================================================================
def load_local_llm(model_path):
    """åŠ è½½æœ¬åœ° HuggingFace æ¨¡å‹å¹¶åŒ…è£…ä¸º LangChain LLM"""
    print(f"â³ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path} ...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path=model_path,
            device_map="auto",
            dtype=torch.float16,
            trust_remote_code=True,
        )

        # åˆ›å»º HuggingFace Pipeline
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=1024,
            temperature=0.1,
            do_sample=True,
            return_full_text=False,
        )

        llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={"temperature": 0.1})

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return llm

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None


# ==============================================================================
# 3. å®šä¹‰å·¥å…·
# ==============================================================================
kb = VectorKBManager()


@tool
def search_knowledge_base(query: str) -> str:
    """
    æœç´¢å†…éƒ¨çŸ¥è¯†åº“è·å–ç›¸å…³æ–‡æ¡£ã€‚

    å½“éœ€è¦å›ç­”å…³äºç‰¹å®šäº‹å®ã€å†…éƒ¨æ–‡æ¡£ã€å…¬å¸æ•°æ®ï¼Œæˆ–è€…ä½ ä¸ç¡®å®šç­”æ¡ˆæ—¶ï¼Œ
    å¿…é¡»ä½¿ç”¨æ­¤å·¥å…·è¿›è¡Œæœç´¢ã€‚

    Args:
        query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œåº”è¯¥æ˜¯å…·ä½“çš„å…³é”®è¯æˆ–é—®é¢˜

    Returns:
        æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹
    """
    print(f"\nğŸ” [Tool] æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“: '{query}'")

    try:
        results = kb.search(query, k=3)

        if not results:
            return "âŒ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # æ„å»ºç»“æ„åŒ–ä¸Šä¸‹æ–‡
        context_parts = []
        for i, r in enumerate(results, 1):
            context_parts.append(
                f"[æ–‡æ¡£ {i}] (ç›¸ä¼¼åº¦: {r.get('score', 'N/A')})\n{r['content']}\n"
            )

        context = "\n".join(context_parts)
        print(f"âœ… æ‰¾åˆ° {len(results)} æ¡ç›¸å…³æ–‡æ¡£")
        return context

    except Exception as e:
        return f"âš ï¸ æ£€ç´¢è¿‡ç¨‹å‡ºé”™: {str(e)}"


# ==============================================================================
# 4. æ„å»º LangGraph Agent (ä»é›¶æ„å»ºï¼Œå®Œå…¨æ§åˆ¶)
# ==============================================================================
def build_react_agent(llm):
    """
    ä½¿ç”¨ LangGraph æ„å»º ReAct Agentï¼ˆå…¼å®¹ HuggingFace æ¨¡å‹ï¼‰

    ç”±äº HuggingFacePipeline ä¸æ”¯æŒåŸç”Ÿå·¥å…·è°ƒç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ ReAct æç¤ºè¯æ¨¡å¼
    """
    print("\nğŸ”§ æ­£åœ¨æ„å»º ReAct Agent Graph...")

    # å‡†å¤‡å·¥å…·
    tools = [search_knowledge_base]
    tools_dict = {tool.name: tool for tool in tools}

    # æ„å»ºå·¥å…·æè¿°ï¼ˆä¾› LLM å‚è€ƒï¼‰
    tool_descriptions = "\n".join(
        [f"- {tool.name}: {tool.description}" for tool in tools]
    )

    # ========== å®šä¹‰èŠ‚ç‚¹å‡½æ•° ==========

    def call_model(state: AgentState):
        """Agent èŠ‚ç‚¹ï¼šä½¿ç”¨ ReAct æç¤ºè¯è°ƒç”¨ LLM"""
        messages = state["messages"]

        # è·å–å¯¹è¯å†å²
        conversation_history = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                conversation_history.append(f"Human: {msg.content}")
            elif isinstance(msg, AIMessage):
                conversation_history.append(f"Assistant: {msg.content}")

        # æ„å»º ReAct é£æ ¼çš„æç¤ºè¯
        react_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å›ç­”é—®é¢˜ï¼š

{tool_descriptions}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼æ€è€ƒå’Œå›ç­”ï¼š
Thought: [ä½ å¯¹é—®é¢˜çš„æ€è€ƒ]
Action: [å·¥å…·åç§°]
Action Input: [å·¥å…·çš„è¾“å…¥å‚æ•°]
Observation: [å·¥å…·è¿”å›çš„ç»“æœä¼šåœ¨è¿™é‡Œ]
... (é‡å¤ Thought/Action/Action Input/Observation ç›´åˆ°ä½ çŸ¥é“æœ€ç»ˆç­”æ¡ˆ)
Final Answer: [ç»™ç”¨æˆ·çš„æœ€ç»ˆå›ç­”]

å¯¹è¯å†å²ï¼š
{chr(10).join(conversation_history)}

ç°åœ¨å¼€å§‹å›ç­”æœ€åä¸€ä¸ªé—®é¢˜ã€‚è®°ä½ï¼šå¦‚æœéœ€è¦æŸ¥æ‰¾ä¿¡æ¯ï¼Œå¿…é¡»ä½¿ç”¨å·¥å…·ï¼
"""

        # è°ƒç”¨æ¨¡å‹
        response = llm.invoke(react_prompt)

        # è§£æå“åº”ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        ai_message = AIMessage(content=response)

        return {"messages": [ai_message]}

    def should_continue(state: AgentState) -> Literal["tools", "end"]:
        """
        æ¡ä»¶è¾¹ï¼šé€šè¿‡è§£æ AI å“åº”åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        """
        messages = state["messages"]
        last_message = messages[-1]

        if not isinstance(last_message, AIMessage):
            return "end"

        content = last_message.content.strip()

        # æ£€æŸ¥æ˜¯å¦åŒ…å« "Action:" å…³é”®å­—ï¼ˆReAct æ¨¡å¼ï¼‰
        if "Action:" in content and "Final Answer:" not in content:
            return "tools"

        return "end"

    def execute_tools(state: AgentState):
        """å·¥å…·èŠ‚ç‚¹ï¼šè§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        messages = state["messages"]
        last_message = messages[-1]

        # è§£æå·¥å…·è°ƒç”¨
        content = last_message.content

        # ç®€å•çš„è§£æé€»è¾‘
        tool_name = None
        tool_input = None

        for line in content.split("\n"):
            if line.startswith("Action:"):
                tool_name = line.replace("Action:", "").strip()
            elif line.startswith("Action Input:"):
                tool_input = line.replace("Action Input:", "").strip()

        # æ‰§è¡Œå·¥å…·
        if tool_name and tool_name in tools_dict and tool_input:
            print(f"ğŸ› ï¸  æ‰§è¡Œå·¥å…·: {tool_name}('{tool_input}')")
            try:
                result = tools_dict[tool_name].invoke(tool_input)
                observation = f"\nObservation: {result}\n"
            except Exception as e:
                observation = f"\nObservation: å·¥å…·æ‰§è¡Œå‡ºé”™: {str(e)}\n"
        else:
            observation = "\nObservation: æœªèƒ½æ­£ç¡®è§£æå·¥å…·è°ƒç”¨\n"

        # å°†å·¥å…·ç»“æœä½œä¸ºæ–°æ¶ˆæ¯è¿”å›
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å°†ç»“æœè¿½åŠ åˆ° AI æ¶ˆæ¯ä¸­ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°æ¶ˆæ¯
        updated_content = last_message.content + observation
        updated_message = AIMessage(content=updated_content)

        # æ›¿æ¢æœ€åä¸€æ¡æ¶ˆæ¯
        new_messages = list(messages[:-1]) + [updated_message]

        return {"messages": new_messages}

    # ========== æ„å»ºå›¾ ==========

    # åˆ›å»ºå›¾
    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("agent", call_model)  # LLM æ¨ç†èŠ‚ç‚¹
    workflow.add_node("tools", execute_tools)  # å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ï¼ˆæ‰‹åŠ¨å®ç°ï¼‰

    # è®¾ç½®å…¥å£ç‚¹
    workflow.add_edge(START, "agent")

    # æ·»åŠ æ¡ä»¶è¾¹ï¼šagent ä¹‹åæ ¹æ®æƒ…å†µå†³å®šèµ°å‘
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",  # éœ€è¦è°ƒç”¨å·¥å…·
            "end": END,  # ç»“æŸ
        },
    )

    # å·¥å…·æ‰§è¡Œåå›åˆ° agent
    workflow.add_edge("tools", "agent")

    # ç¼–è¯‘å›¾ï¼ˆæ·»åŠ æ£€æŸ¥ç‚¹ä»¥æ”¯æŒè®°å¿†ï¼‰
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)

    print("âœ… ReAct Agent æ„å»ºå®Œæˆ!")
    print("ğŸ“ ä½¿ç”¨ ReAct æç¤ºè¯æ¨¡å¼ï¼ˆå…¼å®¹ HuggingFace æ¨¡å‹ï¼‰")
    return app


# ==============================================================================
# 5. è¿è¡Œ Agent
# ==============================================================================
def run_agent_stream(agent, query: str, thread_id: str = "default"):
    """æµå¼è¿è¡Œ Agentï¼Œå®æ—¶æŸ¥çœ‹ä¸­é—´æ­¥éª¤"""
    config = {"configurable": {"thread_id": thread_id}}

    print(f"\n{'=' * 60}")
    print(f"ğŸ—£ï¸  ç”¨æˆ·: {query}")
    print(f"{'=' * 60}\n")

    # æ„å»ºè¾“å…¥
    input_data = {"messages": [HumanMessage(content=query)]}

    # æµå¼è¾“å‡ºæ¯ä¸ªæ­¥éª¤
    for event in agent.stream(input_data, config=config, stream_mode="values"):
        # event åŒ…å«å®Œæ•´çš„çŠ¶æ€
        messages = event.get("messages", [])
        if messages:
            last_msg = messages[-1]

            # æ ¹æ®æ¶ˆæ¯ç±»å‹æ‰“å°ä¸åŒå†…å®¹
            if isinstance(last_msg, HumanMessage):
                print(f"ğŸ‘¤ ç”¨æˆ·: {last_msg.content}\n")
            elif isinstance(last_msg, AIMessage):
                if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
                    print(f"ğŸ¤– Agent å†³å®šè°ƒç”¨å·¥å…·: {last_msg.tool_calls[0]['name']}\n")
                elif last_msg.content:
                    print(f"ğŸ¤– Agent å›å¤: {last_msg.content}\n")

    print(f"{'=' * 60}\n")


def run_agent_sync(agent, query: str, thread_id: str = "default"):
    """åŒæ­¥è¿è¡Œ Agentï¼Œç›´æ¥è·å–æœ€ç»ˆç»“æœ"""
    config = {"configurable": {"thread_id": thread_id}}
    input_data = {"messages": [HumanMessage(content=query)]}

    # ä½¿ç”¨ invoke è·å–æœ€ç»ˆçŠ¶æ€
    final_state = agent.invoke(input_data, config=config)

    # æå–æœ€åçš„ AI æ¶ˆæ¯
    messages = final_state["messages"]
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and msg.content:
            return msg.content

    return "æœªè·å–åˆ°æœ‰æ•ˆå›å¤"


# ==============================================================================
# ğŸš€ ä¸»è¿è¡Œæµç¨‹
# ==============================================================================
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸš€ LangGraph ReAct Agent å¯åŠ¨")
    print("=" * 60)

    # 0. å‡†å¤‡æµ‹è¯•æ•°æ®
    print("\nğŸ“š å‡†å¤‡çŸ¥è¯†åº“...")
    if kb.vectorstore._collection.count() <= 1:
        demo_content = """
LangGraph æ˜¯ LangChain ç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒç¼–æ’å¼•æ“ã€‚
ç›¸æ¯” AgentExecutorï¼ŒLangGraph æä¾›äº†å¾ªç¯å›¾ç»“æ„å’Œæ›´å¼ºçš„çŠ¶æ€æ§åˆ¶ã€‚
LangGraph æ”¯æŒå¤æ‚çš„å¤šæ­¥éª¤å·¥ä½œæµã€æ¡ä»¶åˆ†æ”¯å’ŒçŠ¶æ€æŒä¹…åŒ–ã€‚
å®ƒå…è®¸å¼€å‘è€…æ„å»ºæ›´çµæ´»çš„ Agent ç³»ç»Ÿï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚
LangGraph çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå…¶æ˜¾å¼çš„å›¾ç»“æ„å®šä¹‰å’Œç»†ç²’åº¦çš„æ§åˆ¶èƒ½åŠ›ã€‚
"""
        with open("demo_graph.txt", "w", encoding="utf-8") as f:
            f.write(demo_content)
        kb.add_document("demo_graph.txt")
        print("âœ… æµ‹è¯•æ–‡æ¡£å·²æ·»åŠ ")

    # 1. åŠ è½½æ¨¡å‹
    llm = load_local_llm(MODEL_PATH)

    if not llm:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        exit(1)

    # 2. æ„å»º Agent
    agent_app = build_react_agent(llm)

    # 3. æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "LangGraph å’Œ AgentExecutor ç›¸æ¯”æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        "LangGraph çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
    ]

    # æ–¹å¼1: æµå¼è¾“å‡º (æ¨èç”¨äºè°ƒè¯•)
    print("\n" + "ğŸ”¹" * 30)
    print("æ–¹å¼1: æµå¼è¾“å‡º")
    print("ğŸ”¹" * 30)
    for query in test_queries[:1]:
        run_agent_stream(agent_app, query, thread_id="session_1")

    # æ–¹å¼2: åŒæ­¥è·å–ç»“æœ (æ¨èç”¨äºç”Ÿäº§)
    print("\n" + "ğŸ”¹" * 30)
    print("æ–¹å¼2: åŒæ­¥è°ƒç”¨")
    print("ğŸ”¹" * 30)
    if len(test_queries) > 1:
        result = run_agent_sync(agent_app, test_queries[1], thread_id="session_2")
        print(f"ğŸ—£ï¸  ç”¨æˆ·: {test_queries[1]}")
        print(f"ğŸ¤– Agent å›å¤:\n{result}\n")

    print("\nâœ¨ æµ‹è¯•å®Œæˆ!")
