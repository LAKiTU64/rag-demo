import os
import shutil
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List

from langchain_chroma import Chroma
from langchain_community.document_loaders import (
    Docx2txtLoader,
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# --- Config ---
EMBEDDING_MODEL = "./.models/BAAI/bge-small-zh-v1.5"
CHROMA_PATH = "./.chroma_db"
CHUNK_SIZE = 200
CHUNK_OVERLAP = 50
DEFAULT_SEARCH_K = 3
SIMILARITY_THRESHOLD = 0.5
BEIJING_TZ = timezone(timedelta(hours=8))


class VectorKBManager:
    """
    å‘é‡çŸ¥è¯†åº“ç®¡ç†ç±»ï¼ˆChromaDBï¼‰ï¼šä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹ã€‚
    """

    def __init__(self, persist_directory=CHROMA_PATH) -> None:
        self.persist_directory = persist_directory

        # ä¸¥æ ¼æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„
        if not os.path.exists(EMBEDDING_MODEL):
            raise FileNotFoundError(
                f"âŒ æ‰¾ä¸åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•: {EMBEDDING_MODEL}ã€‚è¯·ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½åˆ°è¯¥ä½ç½®ã€‚"
            )

        print(f"ğŸ” æ­£åœ¨ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL}")

        # åˆå§‹åŒ– Embeddingï¼šå¼ºåˆ¶å¼€å¯ local_files_onlyï¼Œç¦æ­¢è”ç½‘ä¸‹è½½
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={
                "device": "cpu",
                "local_files_only": True,  # æ ¸å¿ƒæ”¹åŠ¨ï¼šç¦æ­¢ä»»ä½•çº¿ä¸Šæ‹‰å–é€»è¾‘
            },
            encode_kwargs={"normalize_embeddings": True},
        )
        self.vectorstore = None
        self._load_or_create()

    def _load_or_create(self, is_reset: bool = False) -> None:
        """
        åˆå§‹åŒ–åŠ è½½ã€‚å¦‚æœæœ¬åœ°æœ‰æ•°æ®åˆ™è¯»å–ï¼Œå¦åˆ™åˆ›å»ºä¸€ä¸ªç©ºåº“ã€‚
        """
        if is_reset and os.path.exists(self.persist_directory):
            shutil.rmtree(self.persist_directory)

        self.vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings,
            collection_name="rag_collection",
            collection_metadata={"hnsw:space": "cosine"},
        )

        if (
            not os.path.exists(self.persist_directory)
            or self.vectorstore._collection.count() == 0
        ):
            print("ğŸ†• å·²å°±ç»ªå…¨æ–°çš„ç©ºå‘é‡åº“")
        else:
            print(f"ğŸ“¦ å·²ä»æœ¬åœ°åŠ è½½ ChromaDB: {self.persist_directory}")

    def _get_loader(
        self, file_path: str
    ) -> TextLoader | UnstructuredMarkdownLoader | Docx2txtLoader | PyPDFLoader:
        ext = file_path.split(".")[-1].lower()
        if ext == "txt":
            return TextLoader(file_path, encoding="utf-8")
        elif ext == "md":
            return UnstructuredMarkdownLoader(file_path)
        elif ext == "docx":
            return Docx2txtLoader(file_path)
        elif ext == "pdf":
            return PyPDFLoader(file_path)
        else:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")

    def add_document(self, file_path: str) -> None:
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

        file_name = os.path.basename(file_path)
        add_time = datetime.now(BEIJING_TZ)

        self.vectorstore.delete(where={"doc_id": file_name})

        try:
            loader = self._get_loader(file_path)
            docs = loader.load()

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP,
                add_start_index=True,
                separators=[
                    "\n### ",  # ä¼˜å…ˆæŒ‰ Kernel / å­æ¨¡å—
                    "\n## ",  # æ¬¡çº§ç»“æ„
                    "\n\n",  # æ®µè½
                    "\n",  # è¡Œ
                    " ",  # è¯
                    "",  # æœ€å…œåº•
                ],
            )
            splits = text_splitter.split_documents(docs)

            for split in splits:
                split.metadata["doc_id"] = file_name
                split.metadata["add_time"] = add_time.isoformat()

            self.vectorstore.add_documents(documents=splits)
            print(f"âœ… æ–‡æ¡£ '{file_name}' ({len(splits)} ä¸ªåˆ‡ç‰‡) å·²æˆåŠŸå…¥åº“")

        except Exception as e:
            print(f"âŒ è§£ææ–‡ä»¶ {file_name} å‡ºé”™: {e}")

    def delete_document(self, file_name: str) -> None:
        self.vectorstore.delete(where={"doc_id": file_name})
        print(f"ğŸ”¥ å·²ç‰©ç†åˆ é™¤æ–‡æ¡£: {file_name}")

    def search(
        self,
        query: str,
        k: int = DEFAULT_SEARCH_K,
        t: float = SIMILARITY_THRESHOLD,
    ) -> List[Dict[str, Any]]:
        docs_and_scores = self.vectorstore.similarity_search_with_score(query, k=k)

        formatted_results = []
        for doc, score in docs_and_scores:
            if score <= t:
                formatted_results.append(
                    {
                        "content": doc.page_content,
                        "doc_id": doc.metadata.get("doc_id"),
                        "add_time": doc.metadata.get("add_time"),
                        "score": round(float(score), 4),
                    }
                )
        return formatted_results

    def get_overview(self) -> Dict[str, Any]:
        all_data = self.vectorstore.get(include=["metadatas"])
        metadatas = all_data.get("metadatas", [])

        if os.path.exists(self.persist_directory):
            ctime = os.path.getctime(self.persist_directory)
            create_time_str = datetime.fromtimestamp(ctime, BEIJING_TZ).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
        else:
            create_time_str = "Unknown"

        doc_stats = {}
        for meta in metadatas:
            did = meta.get("doc_id")
            atime = meta.get("add_time")
            if did:
                if did not in doc_stats or atime > doc_stats[did]:
                    doc_stats[did] = atime

        sorted_docs = sorted(doc_stats.items(), key=lambda x: x[1], reverse=True)
        latest_update = sorted_docs[0][1] if sorted_docs else "N/A"

        print("\n" + "=" * 25 + " å‘é‡åº“å®æ—¶æ¦‚è§ˆ " + "=" * 25)
        print(f"ğŸ“ è·¯å¾„: {self.persist_directory} | ğŸ“… åˆ›å»º: {create_time_str}")
        print(f"ğŸ•’ æ›´æ–°: {latest_update}")
        print(f"ğŸ“Š è§„æ¨¡: {len(metadatas)} åˆ‡ç‰‡ | {len(doc_stats)} æ–‡æ¡£")

        if sorted_docs:
            print("ğŸ“œ æ–‡æ¡£æ¸…å•:")
            for name, time in sorted_docs:
                display_time = datetime.fromisoformat(time).strftime(
                    "%Y-%m-%d %H:%M:%S"
                )
                print(f"  - {name:<20} | å¯¼å…¥æ—¶é—´: {display_time}")
        else:
            print("ğŸ“œ æ–‡æ¡£æ¸…å•: (ç©º)")
        print("=" * 66 + "\n")

        return {"total_chunks": len(metadatas)}

    def reset_index(self) -> None:
        self._load_or_create(is_reset=True)
        print("âœ¨ å‘é‡åº“å·²å®Œæˆä¸€é”®é‡ç½®ã€‚")

    def as_retriever(self, **kwargs):
        from langchain_core.documents import Document
        from langchain_core.retrievers import BaseRetriever
        from pydantic import PrivateAttr

        class KBRetriever(BaseRetriever):
            _kb_manager: VectorKBManager = PrivateAttr()
            k: int = DEFAULT_SEARCH_K
            t: float = SIMILARITY_THRESHOLD

            def __init__(self, kb_manager, k, t, **data):
                super().__init__(**data)
                self._kb_manager = kb_manager
                self.k = k
                self.t = t

            def _get_relevant_documents(self, query: str) -> List[Document]:
                search_results = self._kb_manager.search(query, k=self.k, t=self.t)
                return [
                    Document(
                        page_content=res["content"],
                        metadata={
                            "doc_id": res["doc_id"],
                            "add_time": res["add_time"],
                            "score": res["score"],
                        },
                    )
                    for res in search_results
                ]

        k = kwargs.get("k", DEFAULT_SEARCH_K)
        t = kwargs.get("t", SIMILARITY_THRESHOLD)
        return KBRetriever(kb_manager=self, k=k, t=t)


if __name__ == "__main__":
    # 1. åˆå§‹åŒ–ï¼ˆç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼‰
    try:
        kb = VectorKBManager()
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        exit(1)

    # 2. æŒ‡å®šæµ‹è¯•ç›®å½•
    DOCS_DIR = "./documents"
    if not os.path.exists(DOCS_DIR):
        os.makedirs(DOCS_DIR)
        with open(os.path.join(DOCS_DIR, "sample.txt"), "w", encoding="utf-8") as f:
            f.write("è¿™æ˜¯ä¸€ä¸ªæœ¬åœ°æµ‹è¯•æ–‡æ¡£ã€‚")

    # --- 3. æ ¸å¿ƒæµ‹è¯•ï¼šç›´æ¥éå†å¹¶è°ƒç”¨ add_document ---
    print(f"\nğŸš€ å¼€å§‹éå†ç›®å½•: {DOCS_DIR}")

    for filename in os.listdir(DOCS_DIR):
        full_path = os.path.join(DOCS_DIR, filename)

        # æ’é™¤æ–‡ä»¶å¤¹ï¼Œåªå¤„ç†æ–‡ä»¶
        if os.path.isfile(full_path):
            # ç›´æ¥è°ƒç”¨ï¼Œå†…éƒ¨ _get_loader ä¼šå¤„ç†å®ƒä¸è®¤è¯†çš„æ–‡ä»¶æ ¼å¼
            kb.add_document(full_path)

    # 4. ç»Ÿè®¡ä¸æŸ¥è¯¢
    kb.get_overview()

    print("\nğŸ” æ­£åœ¨è¿›è¡Œæ£€ç´¢æµ‹è¯•...")
    test_query = "L2ç¼“å­˜å‘½ä¸­ç‡ä½"  # æ ¹æ®ä½ çš„å®é™…æ–‡æ¡£å†…å®¹è°ƒæ•´
    results = kb.search(test_query)

    for res in results:
        print(
            f"ğŸ“„ æ¥æº: {res['doc_id']} | è¯„åˆ†: {res['score']} | å†…å®¹: {res['content'][:50]}..."
        )

    print("\nâœ… æ‰¹é‡æµ‹è¯•æµç¨‹ç»“æŸã€‚")
