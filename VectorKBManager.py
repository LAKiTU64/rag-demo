import os
import shutil

from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ä½¿ç”¨å›½å†…é•œåƒæºä¸‹è½½ HuggingFace æ¨¡å‹
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# Config
EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"
INDEX_PATH = "./faiss_index"
CHUNK_SIZE = 300
CHUNK_OVERLAP = 30


class VectorKBManager:
    """
    å‘é‡çŸ¥è¯†åº“ç®¡ç†ç±»ï¼šæ”¯æŒåŸºäºæ–‡æ¡£ç»´åº¦çš„å¢ã€åˆ ã€æŸ¥ã€‚
    åˆ é™¤ç­–ç•¥ï¼šé‡‡ç”¨â€œè½¯åˆ é™¤æ ‡è®° + ç¡¬åˆ é™¤é‡æ„â€çš„æ–¹æ¡ˆã€‚
    """

    def __init__(self, index_path="./faiss_index"):
        self.index_path = index_path
        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        self.vectorstore = None

        # è½¯åˆ é™¤åå•ï¼šå­˜å‚¨åœ¨å†…å­˜ä¸­çš„æ–‡ä»¶åé›†åˆã€‚å³ä½¿å‘é‡è¿˜åœ¨ç´¢å¼•é‡Œï¼Œåªè¦åœ¨è¿™é‡Œé¢çš„æ–‡ä»¶ï¼Œæœç´¢æ—¶éƒ½ä¼šè¢«è¿‡æ»¤æ‰ã€‚
        self.soft_deleted_sources = set()

        self._load_or_create()

    def _load_or_create(self) -> None:
        """
        åˆå§‹åŒ–åŠ è½½ã€‚å¦‚æœæœ¬åœ°æœ‰ç´¢å¼•åˆ™è¯»å–ï¼Œå¦åˆ™åˆ›å»ºä¸€ä¸ªç©ºåº“ã€‚
        """

        if os.path.exists(self.index_path):
            # åŠ è½½æœ¬åœ° FAISS ç´¢å¼•
            self.vectorstore = FAISS.load_local(
                self.index_path, self.embeddings, allow_dangerous_deserialization=True
            )
            print(f"ğŸ“¦ å·²ä»æœ¬åœ°åŠ è½½ç´¢å¼•: {self.index_path}")
        else:
            # FAISS ä¸å…è®¸å®Œå…¨ç©ºçš„åº“å­˜åœ¨ï¼Œæ‰€ä»¥åˆå§‹åŒ–ä¸€ä¸ªç³»ç»Ÿçº§åˆ«çš„å ä½æ–‡æ¡£
            initial_doc = [
                Document(
                    page_content="init_system_placeholder",
                    metadata={"doc_id": "system"},
                )
            ]
            self.vectorstore = FAISS.from_documents(initial_doc, self.embeddings)
            print("ğŸ†• å·²åˆå§‹åŒ–å…¨æ–°çš„å‘é‡åº“")

    def add_document(self, file_path: str) -> None:
        """
        å¢åŠ æ–‡æ¡£ï¼šå°†æ–‡ä»¶è¯»å–ã€åˆ†å‰²å¹¶å­˜å…¥å‘é‡åº“ã€‚
        :param file_path: æœ¬åœ°æ–‡æ¡£è·¯å¾„
        """
        file_name = os.path.basename(file_path)

        # é€»è¾‘ä¿æŠ¤ï¼šå¦‚æœè¯¥æ–‡ä»¶ä¹‹å‰è¢«è½¯åˆ é™¤äº†ï¼Œç°åœ¨é‡æ–°æ·»åŠ æ—¶åº”ä»åå•ä¸­ç§»é™¤
        if file_name in self.soft_deleted_sources:
            self.soft_deleted_sources.remove(file_name)

        # 1. åŠ è½½æ–‡æœ¬
        loader = TextLoader(file_path, encoding="utf-8")
        docs = loader.load()

        # 2. æ–‡æœ¬åˆ‡åˆ†ï¼šè®¾ç½® chunk å—å¤§å°å’Œé‡å åº¦ï¼Œç¡®ä¿è¯­ä¹‰ä¸å› åˆ‡åˆ†è€Œä¸¢å¤±
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True
        )
        splits = text_splitter.split_documents(docs)

        # 3. æ³¨å…¥å…ƒæ•°æ®ï¼šä¸ºæ¯ä¸ªåˆ†ç‰‡æ‰“ä¸Šdoc_idï¼ˆå³æ–‡ä»¶åï¼‰ï¼Œå¯æŒ‰doc_idè¿›è¡Œç®¡ç†
        for split in splits:
            split.metadata["doc_id"] = file_name

        # 4. æ·»åŠ åˆ°å‘é‡åº“å¹¶æŒä¹…åŒ–
        self.vectorstore.add_documents(documents=splits)
        self.vectorstore.save_local(self.index_path)
        print(f"âœ… æ–‡æ¡£ '{file_name}' å·²å…¥åº“ (å…± {len(splits)} ä¸ªåˆ‡ç‰‡)")

    def soft_delete(self, file_name: str) -> None:
        """
        è½¯åˆ é™¤ï¼šä»…åœ¨å‘é‡åº“ä¸­è®°å½•è¯¥æ–‡ä»¶å·²â€œå¤±æ•ˆâ€ï¼Œæœç´¢æ—¶ä¼šè‡ªåŠ¨è·³è¿‡ã€‚
        """
        self.soft_deleted_sources.add(file_name)
        print(f"ğŸŸ¡ å·²è½¯åˆ é™¤ï¼ˆæ ‡è®°å±è”½ï¼‰: {file_name}ï¼Œç‰©ç†æ•°æ®ä»ä¿ç•™ï¼ŒæŸ¥è¯¢å·²ä¸å¯è§ã€‚")

    def hard_delete(self) -> None:
        """
        ç¡¬åˆ é™¤ï¼šè€—æ—¶æ“ä½œï¼Œå»ºè®®å®šæœŸæ‰§è¡Œã€‚
        åŸç†ï¼šä» docstore ä¸­æå–æ‰€æœ‰æœªè¢«è½¯åˆ çš„æ–‡æ¡£ï¼Œå½»åº•ä¸¢å¼ƒå·²åˆ é™¤æ•°æ®å¹¶é‡æ„ç´¢å¼•ã€‚
        """
        if not self.soft_deleted_sources:
            print("ğŸ’¡ æš‚æ— è½¯åˆ é™¤æ ‡è®°ï¼Œæ— éœ€æ¸…ç†ã€‚")
            return

        # self.vectorstore.docstore._dict å­˜å‚¨äº† ID åˆ° Document å¯¹è±¡çš„æ˜ å°„
        all_docs = self.vectorstore.docstore._dict.values()

        # è¿‡æ»¤å‡ºéœ€è¦ä¿ç•™çš„æ–‡æ¡£
        remaining_docs = [
            doc
            for doc in all_docs
            if doc.metadata.get("doc_id") not in self.soft_deleted_sources
            and doc.metadata.get("doc_id") != "system"
        ]

        if remaining_docs:
            # å½»åº•é‡å»º FAISS ç´¢å¼•ï¼ˆé‡Šæ”¾ç‰©ç†ç©ºé—´ï¼‰
            self.vectorstore = FAISS.from_documents(remaining_docs, self.embeddings)
            self.vectorstore.save_local(self.index_path)
        else:
            # å¦‚æœæ–‡æ¡£è¢«åˆ å…‰äº†ï¼Œåˆ™é‡ç½®åº“
            if os.path.exists(self.index_path):
                shutil.rmtree(self.index_path)
            self._load_or_create()

        # æ¸…ç©ºè½¯åˆ é™¤åå•ï¼Œå› ä¸ºæ•°æ®å·²ç»ä»ç‰©ç†ä¸ŠæŠ¹é™¤äº†
        self.soft_deleted_sources.clear()
        print("ğŸ”¥ ç¡¬åˆ é™¤å®Œæˆï¼šç´¢å¼•å·²é‡æ„ï¼Œè¿‡æ—¶æ•°æ®å·²è¢«ç‰©ç†æ¸…é™¤ã€‚")

    def search(self, query: str, k: int = 3) -> list[Document]:
        """
        æŸ¥è¯¢ï¼šåœ¨ç›¸ä¼¼åº¦æœç´¢çš„åŸºç¡€ä¸Šå¢åŠ å®æ—¶è¿‡æ»¤é€»è¾‘ã€‚
        :param query: ç”¨æˆ·æå‡ºçš„é—®é¢˜
        :param k: è¿”å›æœ€ç›¸å…³çš„ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º3
        """

        # å®šä¹‰è¿‡æ»¤å‡½æ•°ï¼šæ£€æŸ¥è¯¥æ–‡æ¡£æ˜¯å¦åœ¨è½¯åˆ é™¤é»‘åå•ä¸­
        def filter_func(metadata):
            return metadata.get("doc_id") not in self.soft_deleted_sources

        # ä½¿ç”¨ filter å‚æ•°è¿›è¡Œåç½®è¿‡æ»¤ï¼ˆPost-filteringï¼‰
        results = self.vectorstore.similarity_search(query, k=k, filter=filter_func)
        return results


if __name__ == "__main__":
    # 1. æ¨¡æ‹Ÿç”Ÿæˆä¸¤ä¸ªæµ‹è¯•æ–‡ä»¶
    with open("doc_recipe_1.txt", "w", encoding="utf-8") as f:
        f.write("çº¢çƒ§è‚‰çš„ç§˜è¯€æ˜¯äº”èŠ±è‚‰è¦åˆ‡æˆ3å˜ç±³è§æ–¹çš„å—ï¼ŒåŠ å†°ç³–å°ç«æ…¢ç‚–ã€‚")
    with open("doc_recipe_2.txt", "w", encoding="utf-8") as f:
        f.write("å›é”…è‚‰çš„å…³é”®æ˜¯å…ˆå°†è‚‰ç…®è‡³å…­ä¸ƒæˆç†Ÿï¼Œèµ·é”…åå†åˆ‡è–„ç‰‡å›é”…ã€‚")

    manager = VectorKBManager()

    # æµ‹è¯•æ·»åŠ 
    manager.add_document("doc_recipe_1.txt")
    manager.add_document("doc_recipe_2.txt")

    # 2. è½¯åˆ é™¤æµ‹è¯•ï¼šåˆ é™¤â€œçº¢çƒ§è‚‰â€
    print("\n>>> æ‰§è¡Œè½¯åˆ é™¤: doc_recipe_1.txt")
    manager.soft_delete("doc_recipe_1.txt")

    # æŸ¥è¯¢éªŒè¯ï¼šæœçº¢çƒ§è‚‰åº”è¯¥æœä¸åˆ°ï¼ˆæˆ–æœåˆ°æ— å…³å†…å®¹ï¼‰ï¼Œæœå›é”…è‚‰æ­£å¸¸
    print("\n>>> è½¯åˆ é™¤åæŸ¥è¯¢ 'çº¢çƒ§è‚‰'ï¼š")
    res = manager.search("çº¢çƒ§è‚‰")
    if not res:
        print("ï¼ˆç¬¦åˆé¢„æœŸï¼šæœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼‰")
    for doc in res:
        print(f"æ‰¾åˆ°å†…å®¹: {doc.page_content} | æ¥æº: {doc.metadata['doc_id']}")

    # 3. ç¡¬åˆ é™¤æµ‹è¯•ï¼šæ¸…ç†å­˜å‚¨ç©ºé—´
    print("\n>>> æ‰§è¡Œç¡¬åˆ é™¤æ¸…ç†ç‰©ç†ç©ºé—´...")
    manager.hard_delete()

    # 4. å†æ¬¡æŸ¥è¯¢
    print("\n>>> æœ€ç»ˆæŸ¥è¯¢ 'å›é”…è‚‰'ï¼š")
    res_final = manager.search("å›é”…è‚‰")
    for doc in res_final:
        print(f"æ‰¾åˆ°å†…å®¹: {doc.page_content} | æ¥æº: {doc.metadata['doc_id']}")

    # ç°åœºæ¸…ç†ï¼šåˆ é™¤æµ‹è¯•ç”¨çš„ txt æ–‡ä»¶
    for f in ["doc_recipe_1.txt", "doc_recipe_2.txt"]:
        if os.path.exists(f):
            os.remove(f)
