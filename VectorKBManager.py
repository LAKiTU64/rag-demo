import os
import shutil
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List

from langchain_chroma import Chroma
from langchain_community.document_loaders import (
    Docx2txtLoader,
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ä½¿ç”¨å›½å†…é•œåƒæºä¸‹è½½ HuggingFace æ¨¡å‹
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# --- Config ---
EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"
CHROMA_PATH = "./chroma_db"
CHUNK_SIZE = 300  # å¦‚æœå›ç­”æ€»æ˜¯â€œæ–­ç« å–ä¹‰â€ï¼Œéœ€è¦æŠŠè¿™ä¸ªå€¼è°ƒå¤§ï¼›å¦‚æœå‘ç° LLM æ€»æ˜¯æ‰¾ä¸åˆ°é‡ç‚¹ï¼Œå¯èƒ½éœ€è¦è°ƒå°ã€‚
CHUNK_OVERLAP = 50  # å¦‚æœåˆ‡åˆ†åçš„å¥å­ç»å¸¸å‡ºç°â€œå‰å› åæœâ€ä¸è¿è´¯ï¼Œéœ€è¦è°ƒå°è¿™ä¸ªå€¼ã€‚
DEFAULT_SEARCH_K = 3
SIMILARITY_THRESHOLD = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼Œè¶Šå°è¶Šä¸¥è‹›ï¼‰ã€‚
BEIJING_TZ = timezone(timedelta(hours=8))  # å®šä¹‰ä¸œå…«åŒºæ—¶åŒº


class VectorKBManager:
    """
    å‘é‡çŸ¥è¯†åº“ç®¡ç†ç±»ï¼ˆChromaDBï¼‰ï¼šæ”¯æŒåŸºäºæ–‡æ¡£ç»´åº¦çš„å¢ã€åˆ ã€æŸ¥ã€‚
    """

    def __init__(self, persist_directory=CHROMA_PATH) -> None:
        self.persist_directory = persist_directory
        # åˆå§‹åŒ– Embedding
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL, encode_kwargs={"normalize_embeddings": True}
        )
        self.vectorstore = None

        self._load_or_create()

    def _load_or_create(self, is_reset: bool = False) -> None:
        """
        åˆå§‹åŒ–åŠ è½½ã€‚å¦‚æœæœ¬åœ°æœ‰æ•°æ®åˆ™è¯»å–ï¼Œå¦åˆ™åˆ›å»ºä¸€ä¸ªç©ºåº“ã€‚
        """
        if is_reset and os.path.exists(self.persist_directory):
            shutil.rmtree(self.persist_directory)

        # æ˜¾å¼æŒ‡å®šä½¿ç”¨ä½™å¼¦è·ç¦» (Cosine Similarity)
        # æ³¨æ„ï¼šChroma è¿”å›çš„æ˜¯è·ç¦» distance = 1 - similarityï¼Œæ‰€ä»¥ä¾ç„¶æ˜¯è¶Šå°è¶Šç›¸å…³
        self.vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings,
            collection_name="rag_collection",
            collection_metadata={"hnsw:space": "cosine"},
        )

        # æ ¹æ®æ˜¯å¦å­˜åœ¨ç›®å½•æ˜¾ç¤ºçŠ¶æ€
        if (
            not os.path.exists(self.persist_directory)
            or self.vectorstore._collection.count() == 0
        ):
            print("ğŸ†• å·²å°±ç»ªå…¨æ–°çš„ç©ºå‘é‡åº“")
        else:
            print(f"ğŸ“¦ å·²ä»æœ¬åœ°åŠ è½½ ChromaDB: {self.persist_directory}")

    def _get_loader(
        self, file_path: str
    ) -> TextLoader | UnstructuredMarkdownLoader | Docx2txtLoader | PyPDFLoader:
        """
        æ ¹æ®æ–‡ä»¶åç¼€è¿”å›å¯¹åº”çš„ LangChain åŠ è½½å™¨
        """
        ext = file_path.split(".")[-1].lower()
        if ext == "txt":
            return TextLoader(file_path, encoding="utf-8")
        elif ext == "md":
            return UnstructuredMarkdownLoader(file_path)
        elif ext == "docx":
            return Docx2txtLoader(file_path)
        elif ext == "pdf":
            return PyPDFLoader(file_path)
        else:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")

    def add_document(self, file_path: str) -> None:
        """
        å¢åŠ æ–‡æ¡£ï¼šå¦‚æœå­˜åœ¨åŒåæ–‡æ¡£ï¼Œåˆ™ç›´æ¥è¦†å†™ã€‚
        :param file_path: æœ¬åœ°æ–‡æ¡£è·¯å¾„
        """
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

        file_name = os.path.basename(file_path)
        add_time = datetime.now(BEIJING_TZ)

        # è¦†å†™ï¼šå…ˆåˆ é™¤è¯¥æ–‡æ¡£çš„æ‰€æœ‰æ—§åˆ‡ç‰‡
        self.vectorstore.delete(where={"doc_id": file_name})

        try:
            # è‡ªåŠ¨é€‰æ‹©åŠ è½½å™¨å¹¶è§£æ
            loader = self._get_loader(file_path)
            docs = loader.load()

            # æ–‡æœ¬åˆ‡åˆ†
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True
            )
            splits = text_splitter.split_documents(docs)

            for split in splits:
                split.metadata["doc_id"] = file_name
                split.metadata["add_time"] = add_time.isoformat()

            # æ·»åŠ æ–°åˆ‡ç‰‡
            self.vectorstore.add_documents(documents=splits)
            print(f"âœ… æ–‡æ¡£ '{file_name}' ({len(splits)} ä¸ªåˆ‡ç‰‡) å·²æˆåŠŸå…¥åº“/è¦†ç›–")

        except Exception as e:
            print(f"âŒ è§£ææ–‡ä»¶ {file_name} å‡ºé”™: {e}")

    def delete_document(self, file_name: str) -> None:
        """
        åˆ é™¤æ–‡æ¡£ï¼šç›´æ¥ä»æ•°æ®åº“ä¸­ç‰©ç†åˆ é™¤è¯¥æ–‡æ¡£çš„æ‰€æœ‰åˆ‡ç‰‡ã€‚
        """
        self.vectorstore.delete(where={"doc_id": file_name})
        print(f"ğŸ”¥ å·²ç‰©ç†åˆ é™¤æ–‡æ¡£: {file_name}")

    def search(
        self,
        query: str,
        k: int = DEFAULT_SEARCH_K,
        t: float = SIMILARITY_THRESHOLD,
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥è¯¢ï¼šè¿”å›åŒ…å«å†…å®¹ã€æ¥æºIDå’Œæ·»åŠ æ—¶é—´çš„å­—å…¸åˆ—è¡¨ã€‚
        """
        # ç›´æ¥æœç´¢ï¼ŒChroma å†…éƒ¨ä¼šå¤„ç†ç©ºåº“æƒ…å†µ
        docs_and_scores = self.vectorstore.similarity_search_with_score(query, k=k)

        formatted_results = []
        for doc, score in docs_and_scores:
            # åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼Œåœ¨ Cosine Distance ä¸‹ï¼Œscore è¶Šå°ä»£è¡¨è¶Šç›¸å…³
            if score <= t:
                formatted_results.append(
                    {
                        "content": doc.page_content,
                        "doc_id": doc.metadata.get("doc_id"),
                        "add_time": doc.metadata.get("add_time"),
                        "score": round(float(score), 4),
                    }
                )
        return formatted_results

    def get_overview(self) -> Dict[str, Any]:
        """
        æ¦‚è§ˆï¼šæ˜¾ç¤ºå½“å‰å‘é‡åº“çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬æ–‡æ¡£åˆ—è¡¨å’Œæ›´æ–°ç»Ÿè®¡ã€‚
        """
        # ä»…è·å–å…ƒæ•°æ®ï¼Œé¿å…åœ¨å¤§è§„æ¨¡åº“ä¸­åŠ è½½æ‰€æœ‰æ–‡æœ¬å¯¼è‡´ OOM
        all_data = self.vectorstore.get(include=["metadatas"])
        metadatas = all_data.get("metadatas", [])

        # è·å–ç›®å½•åˆ›å»ºæ—¶é—´ä½œä¸ºâ€œåº“åˆ›å»ºæ—¶é—´â€
        if os.path.exists(self.persist_directory):
            ctime = os.path.getctime(self.persist_directory)
            create_time_str = datetime.fromtimestamp(ctime, BEIJING_TZ).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
        else:
            create_time_str = "Unknown"

        doc_stats = {}
        for meta in metadatas:
            did = meta.get("doc_id")
            atime = meta.get("add_time")
            if did:
                # ä¿ç•™è¯¥æ–‡æ¡£æœ€æ–°çš„æ—¶é—´è®°å½•
                if did not in doc_stats or atime > doc_stats[did]:
                    doc_stats[did] = atime

        sorted_docs = sorted(doc_stats.items(), key=lambda x: x[1], reverse=True)
        latest_update = sorted_docs[0][1] if sorted_docs else "N/A"

        print("\n" + "=" * 25 + " å‘é‡åº“å®æ—¶æ¦‚è§ˆ " + "=" * 25)
        print(f"ğŸ“ è·¯å¾„: {self.persist_directory} | ğŸ“… åˆ›å»º: {create_time_str}")
        print(f"ğŸ•’ æ›´æ–°: {latest_update}")
        print(f"ğŸ“Š è§„æ¨¡: {len(metadatas)} åˆ‡ç‰‡ | {len(doc_stats)} æ–‡æ¡£")

        if sorted_docs:
            print("ğŸ“œ æ–‡æ¡£æ¸…å•:")
            for name, time in sorted_docs:
                # å°† iso æ ¼å¼è½¬å›æ˜“è¯»æ ¼å¼
                display_time = datetime.fromisoformat(time).strftime(
                    "%Y-%m-%d %H:%M:%S"
                )
                print(f"  - {name:<20} | å¯¼å…¥æ—¶é—´: {display_time}")
        else:
            print("ğŸ“œ æ–‡æ¡£æ¸…å•: (ç©º)")
        print("=" * 66 + "\n")

        return {"total_chunks": len(metadatas)}

    def reset_index(self) -> None:
        """
        ä¸€é”®åˆå§‹åŒ–/é‡ç½®å‘é‡åº“ï¼š
        å½»åº•åˆ é™¤ç£ç›˜ä¸Šçš„ç´¢å¼•æ–‡ä»¶å¹¶æ¢å¤åˆ°åˆå§‹ç©ºåº“çŠ¶æ€ã€‚
        """
        self._load_or_create(is_reset=True)
        print("âœ¨ å‘é‡åº“å·²å®Œæˆä¸€é”®é‡ç½®ã€‚")

    def as_retriever(self, search_kwargs: dict = None):
        """
        è¿”å›ä¸€ä¸ªå…¼å®¹ LangChain çš„ Retriever å¯¹è±¡ã€‚
        """
        
        from langchain_core.documents import Document
        from langchain_core.retrievers import BaseRetriever

        class ChromaRetriever(BaseRetriever):
            def __init__(self, kb_manager, k=DEFAULT_SEARCH_K, t=SIMILARITY_THRESHOLD):
                self.kb_manager = kb_manager
                self.k = k
                self.t = t

            def _get_relevant_documents(self, query: str):
                results = self.kb_manager.search(query, k=self.k, t=self.t)
                docs = [
                    Document(
                        page_content=r["content"],
                        metadata={"doc_id": r["doc_id"], "score": r["score"]},
                    )
                    for r in results
                ]
                return docs

        return ChromaRetriever(self, **(search_kwargs or {}))


if __name__ == "__main__":
    # --- æµ‹è¯•æµç¨‹ ---
    manager = VectorKBManager()

    # 1. åˆ›å»ºå¤šä¸ªæµ‹è¯•æ–‡æ¡£
    files_to_test = {
        "test_f1.txt": "åä¸ºæ˜¯å…¨çƒé¢†å…ˆçš„ ICTï¼ˆä¿¡æ¯ä¸é€šä¿¡ï¼‰åŸºç¡€è®¾æ–½å’Œæ™ºèƒ½ç»ˆç«¯æä¾›å•†ã€‚",
        "test_f2.md": "# Pythonç®€ä»‹\nPython æ˜¯ä¸€ç§å¹¿æ³›è¿ç”¨äºäººå·¥æ™ºèƒ½å¼€å‘çš„é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚",
    }

    print("--- å¼€å§‹æµ‹è¯•ï¼šæ·»åŠ æ–‡æ¡£ ---")
    for filename, content in files_to_test.items():
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        manager.add_document(filename)

    # 2. æµ‹è¯•è¦†å†™é€»è¾‘ï¼ˆå†æ¬¡æ·»åŠ åŒåæ–‡ä»¶ï¼‰
    print("\n--- å¼€å§‹æµ‹è¯•ï¼šè¦†å†™æ–‡æ¡£ ---")
    manager.add_document("test_f1.txt")

    # 3. æœç´¢å±•ç¤ºï¼ˆæµ‹è¯•æœ‰æ•ˆæœç´¢å’Œæ— æ•ˆæœç´¢ï¼‰
    print("\n--- å¼€å§‹æµ‹è¯•ï¼šæœç´¢åŠŸèƒ½ ---")
    test_queries = ["åä¸º", "äººå·¥æ™ºèƒ½", "è¥¿ç“œ"]
    for q in test_queries:
        print(f">>> æœç´¢å…³é”®è¯: [{q}]")
        res = manager.search(q)
        if not res:
            print("    (æ— ç»“æœ)")
        for r in res:
            print(
                f"    å†…å®¹: {r['content']} | è¯„åˆ†: {r['score']} | æ¥æº: {r['doc_id']}"
            )

    # 4. æŸ¥çœ‹æ¦‚è§ˆ
    manager.get_overview()

    # 5. åˆ é™¤æµ‹è¯•
    print("--- å¼€å§‹æµ‹è¯•ï¼šåˆ é™¤æ–‡æ¡£ ---")
    manager.delete_document("test_f1.txt")
    manager.get_overview()

    # 6. é‡ç½®æµ‹è¯•
    print("--- å¼€å§‹æµ‹è¯•ï¼šé‡ç½®å‘é‡åº“ ---")
    manager.reset_index()
    manager.get_overview()

    # æ¸…ç†æµ‹è¯•äº§ç”Ÿçš„æœ¬åœ°æ–‡ä»¶
    for filename in files_to_test.keys():
        if os.path.exists(filename):
            os.remove(filename)
    # å¦‚æœå¸Œæœ›æµ‹è¯•å®Œå½»åº•åˆ é™¤æ•°æ®åº“ç›®å½•ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢æ³¨é‡Š
    # if os.path.exists(CHROMA_PATH): shutil.rmtree(CHROMA_PATH)
    print("âœ… æµ‹è¯•æµç¨‹ç»“æŸï¼Œä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")
